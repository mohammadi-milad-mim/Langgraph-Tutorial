{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (with LanceDB and LlamaParse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-index-core\n",
      "  Downloading llama_index_core-0.12.10.post1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting llama-parse\n",
      "  Downloading llama_parse-0.5.19-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting llama-index-readers-file\n",
      "  Downloading llama_index_readers_file-0.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: python-dotenv in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (1.0.1)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (3.11.11)\n",
      "Requirement already satisfied: dataclasses-json in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (1.2.15)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (2024.12.0)\n",
      "Requirement already satisfied: httpx in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (0.28.1)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (3.4.2)\n",
      "Collecting nltk>3.8.1 (from llama-index-core)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: numpy in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (2.10.1)\n",
      "Requirement already satisfied: requests>=2.31.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (9.0.0)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (0.8.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-core) (1.17.0)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.7 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-parse) (8.1.8)\n",
      "Collecting beautifulsoup4<5.0.0,>=4.12.3 (from llama-index-readers-file)\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pandas in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from llama-index-readers-file) (2.2.3)\n",
      "Collecting pypdf<6.0.0,>=5.1.0 (from llama-index-readers-file)\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file)\n",
      "  Downloading striprtf-0.0.26-py3-none-any.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core) (1.18.3)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: joblib in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from nltk>3.8.1->llama-index-core) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from pydantic>=2.8.0->llama-index-core) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from requests>=2.31.0->llama-index-core) (2024.12.14)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from typing-inspect>=0.8.0->llama-index-core) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from dataclasses-json->llama-index-core) (3.23.2)\n",
      "Requirement already satisfied: anyio in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from httpx->llama-index-core) (4.7.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from httpx->llama-index-core) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from httpcore==1.*->httpx->llama-index-core) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from pandas->llama-index-readers-file) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from pandas->llama-index-readers-file) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from pandas->llama-index-readers-file) (2024.2)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core) (23.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->llama-index-readers-file) (1.17.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from anyio->httpx->llama-index-core) (1.3.1)\n",
      "Downloading llama_index_core-0.12.10.post1-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_parse-0.5.19-py3-none-any.whl (15 kB)\n",
      "Downloading llama_index_readers_file-0.4.2-py3-none-any.whl (38 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: striprtf, dirtyjson, soupsieve, pypdf, nltk, beautifulsoup4, llama-index-core, llama-parse, llama-index-readers-file\n",
      "Successfully installed beautifulsoup4-4.12.3 dirtyjson-1.0.8 llama-index-core-0.12.10.post1 llama-index-readers-file-0.4.2 llama-parse-0.5.19 nltk-3.9.1 pypdf-5.1.0 soupsieve-2.6 striprtf-0.0.26\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-index-core llama-parse llama-index-readers-file python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_files = [\"ChatLLM_Network.pdf\", \"Cognitive_Architectures_for_Language_Agents.pdf\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Parse PDF file using LlamaParse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llama_index.core.readers.file.base.SimpleDirectoryReader at 0x1768fecd0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_parse import LlamaParse\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "# set up parser\n",
    "parser = LlamaParse(result_type=\"text\")\n",
    "\n",
    "file_extractor = {\".pdf\": parser}\n",
    "\n",
    "data_for_parse = SimpleDirectoryReader(input_files=pdf_files, file_extractor=file_extractor)\n",
    "data_for_parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started parsing the file under job_id 058676da-8f9b-430c-bc36-b1e1da8c6139\n",
      "Started parsing the file under job_id 8b14c8cb-a6cc-4217-8de1-23e2c94001d0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(id_='d8e107bc-267a-4918-9831-64028f5ff03f', embedding=None, metadata={'file_path': 'ChatLLM_Network.pdf', 'file_name': 'ChatLLM_Network.pdf', 'file_type': 'application/pdf', 'file_size': 925390, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ChatLLM Network: More brains, More intelligence\\n                              Rui Hao†\\n                  School of Computer Science\\n\\n    Beijing University of Posts and Telecommunications\\n                      haorui@bupt.edu.cn\\n\\n           Linmei Hu ∗ †\\n\\n  School of Computer Science\\nBeijing Institute of Technology\\n     hulinmei@bit.edu.cn\\n\\n   arXiv:2304.12998v1  [cs.AI]  24 Apr 2023Qingliu WuDepartment of AutomationWeijian Qi          School of Computer Science\\n                                         Xi’an Jiaotong University                 Beijing University of Posts and Telecommunications\\n                                      qiweijian@stu.xjtu.edu.cn                                       wuql@bupt.edu.cn\\n\\n                                                                                    Yirui Zhang\\n                                                                  School of Information and Communication\\n                                                            Beijing University of Posts and Telecommunications\\n                                                                           zhangyirui@bupt.edu.cn\\n\\n                                                                                    Liqiang Nie\\n                                                                 School of Computer Science and Technology\\n                                                                  Harbin Institute of Technology (Shenzhen)\\n                                                                             nieliqiang@gmail.com\\n\\n                                                                                     Abstract\\n                                           Dialogue-based language models mark a huge milestone in the field of artificial\\n                                           intelligence, by their impressive ability to interact with users, as well as a series of\\n                                           challenging tasks prompted by customized instructions. However, the prevalent\\n                                           large-scale dialogue-based language models like ChatGPT still have room for\\n                                           improvement, such as unstable responses to questions and the inability to think\\n                                           cooperatively like humans. Considering the ability of dialogue-based language\\n                                           models in conversation and their inherent randomness in thinking, we propose\\n                                           ChatLLM network that allows multiple dialogue-based language models to inter-\\n                                           act, provide feedback, and think together. We design the network of ChatLLMs\\n                                           based on ChatGPT. Specifically, individual instances of ChatGPT may possess\\n                                           distinct perspectives towards the same problem, and by consolidating these diverse\\n                                           viewpoints via a separate ChatGPT, the ChatLLM network system can conduct\\n                                           decision-making more objectively and comprehensively. In addition, a language-\\n                                           based feedback mechanism comparable to backpropagation is devised to update the\\n                                           ChatGPTs within the network. Experiments on two datasets demonstrate that our\\n                                           network attains significant improvements in problem-solving, leading to observable\\n                                           progress amongst each member.\\n                                    ∗Corresponding authors.\\n\\n\\n                                    †These authors contributed equally to this work.\\n                                Preprint. Under review.', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8ddafeb4-70dd-4de6-ab43-e403a053d1cb', embedding=None, metadata={'file_path': 'ChatLLM_Network.pdf', 'file_name': 'ChatLLM_Network.pdf', 'file_type': 'application/pdf', 'file_size': 925390, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=\"1    Introduction\\nLarge language models have attracted widespread attention in the field of artificial intelligence\\nbecause of their impressive ability to solve natural language processing tasks. Dialogue-based large\\nlanguage models, such as ChatGPT, in particular, have exerted a significant impact on the development\\nof society and have become an exemplar of artificial intelligence applied to daily life, attracting\\nextensive attention from both academia and industry. Among their wide range of intelligence, their\\nexceptional level of insight and helpfulness during conversations is so profound that distinguishing\\nthem from humans solely based on their speech style and content becomes incredibly challenging.\\nDespite their impressive capabilities in interacting with humans and handling various natural language\\nprocessing tasks, dialogue-based large language models like GPT-3 may still provide unsatisfactory\\nresponses in certain conversational scenarios. This is because these models are based on generative\\nmodels that rely on statistical patterns in the data they were trained on, rather than on specific\\nknowledge or reasoning. We observe two distinct aspects of these unsatisfactory responses. The\\nfirst aspect is instability, in which the answers can significantly vary despite the same context and\\nprompt being provided, as shown in Figure 1. In some scenarios, the model may provide a response\\nthat is grammatically correct but factually incorrect [Wei et al., 2022]. The second is incomprehen-\\nsiveness, as a single instance of model may easily provide one-sided answers, failing to engage in\\ncollaborative thinking, which brings more open-ended answers with a wider range of perspectives.\\n\\nIn this work, to address the potential issues with\\na single dialogue-based language model, such as\\nunstable responses and limited comprehensive-                        Which number is the largest in 76,28,36?\\nness, we propose a ChatLLM network model to\\naggregate the viewpoints of other models. First,\\nwe devise a forward aggregation mechanism that                       The number 36 is the largest.\\nenables dialogue-based large language models\\nto converge on optimal outcomes by consider-\\ning and synthesizing the highlights of input and                                     Think about it again.\\noutput from each instance model. Subsequently,\\na language-based backpropagation mechanism                           I’m very sorry, my  last answer was\\nis employed to learn from their mistakes and                         wrong. The number 76 is the largest.Which number is the largest in 76,28,362\\nimprove their performance over time by incorpo-\\nrating feedback and updating their thinking pro-                                     Think about it again.\\ncesses. Moreover, the dropout mechanism is in-\\ntroduced to manage the input for each dialogue-                      I‘m sorry, my first two answers were\\nbased language model, thus preventing infor-                         both wrong. The correct answer is 28.\\nmation overload. After repeated iterations of\\nforward and backpropagation, all models in the\\nnetwork collaborate as leaders or employees and\\nthus enhancing overall performance. In addition,\\nwe do not necessarily require the models to use\\nChatGPT in this network, as the entire network            Figure 1: An example of instability of ChatGPTThe number 36 is the largest:\\nmay become stronger with the enhancement of\\ndialogue-based language models or with the use\\nof different types of dialogue-based language models. The main contributions of this article can be\\nsummarized as follows:\\n1. We propose a novel ChatLLM network model that allows multiple dialogue-based language models\\nto interact, provide feedback, and think together, in order to enhance their problem-solving abilities.\\nThis network model can be applied to different types of dialogue-based language models, thus having\\ncertain universality and scalability.\\n2. We utilize a forward aggregation mechanism to consolidate the outputs of multiple dialogue-based\\nwe propose a language based backpropagation method to update the reflections of dialogue-based\\n                                                                                              large language models, leveraging on the unique strengths of each individual model. Moreover,\\nlanguage models, in order to further improve the performance of the network.                                                                                                Think about it again:\\n                                                         2\\n                                                                                                       Im                 very   sOrry,                                   my              last            answer      was\\n                                                                                                        wrong The number 76 is the largest\\n                                                                                                                                                                             Think about it again:\\n                                                                                                       I'm               sorry,                    my             first two                         answers       were\\n                                                                                                        both wrong: The correct answer is 28.\", path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='36d88cb8-c99f-4040-9a0e-0e519bfeb3ff', embedding=None, metadata={'file_path': 'ChatLLM_Network.pdf', 'file_name': 'ChatLLM_Network.pdf', 'file_type': 'application/pdf', 'file_size': 925390, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3. We conduct experiments on two tasks, digital mode classification which represents a customized\\ntask, and sentiment reversal, a traditional NLP task. The results show significant enhancements\\nin problem-solving, compared to the vanilla model. As a fundamental research, our study can\\nprovide valuable insights and inspirations for synthesizing multiple models in future work on multiple\\ndialogue-based language models.\\n\\n2\\n      Related work\\n2.1    Large Language Models\\nThe introduction of the transformer model [Ashish et al., 2017] has made it possible to train large-\\nscale unsupervised text data. In the past few years, encoder-based models such as BERT [Jacob\\net al., 2019] have demonstrated impressive capabilities in various natural language processing (NLP)\\ntasks. More recently, decoder-based models such as GPT-1 [Radford et al., 2018], GPT-2 [Radford\\net al., 2019], and T5 [Colin et al., 2020] have made even greater strides. As the number of model\\nparameters has increased, models like GPT-3 [Brown et al., 2020], often referred to as large language\\nmodels, have gradually acquired zero-shot learning abilities, which have the capacity to generate\\nresponses based on instructions without requiring any examples.\\n\\n2.2    ChatGPT\\nChatGPT, also known as InstructGPT, is an advanced version of the GPT-3 model, enhanced by\\ninstruction tuning [Jason et al., 2022], and reinforcement learning from human feedback (RLHF)\\n[Knox and Stone, 2008] [Long et al., 2022]. Unlike the original GPT-3 models, which are not\\nspecifically designed to follow user instructions, the InstructGPT models demonstrate a considerably\\nenhanced capability to generate more alinged and helpful outputs in response to user instructions.\\nChatGPT has been extensively applied in various artificial intelligence scenarios, including search-\\nbased QA, basic NLP tasks, and human-scene tool connections.\\nThe launch of ChatGPT also has a significant impact on AI research, paving the way for Artificial\\nGeneral Intelligence (AGI) systems. Yongliang et al. [2023] proposed HuggingGPT, a cooperative\\nsystem designed to connect various AI models within the HuggingFace community, leveraging\\nChatGPT as a controller to accomplish multimodal complex tasks. Toran [2023] further introduced an\\nopen-source application, AutoGPT, driven by GPT-4, which can autonomously achieve user-defined\\ngoals. Additionally, Joon et al. [2023] presented generative agents, an architecture that extends large\\nlanguage models to simulate believable human behavior. Guohao et al. [2023] introduced a novel\\nLLM agent communication network, CAMEL, showcasing the potential for autonomous cooperation\\namong communication. Both CAMEL and our work are inspired by the intuition that the involvement\\nof multiple ChatGPTs can enhance performance. While CAMEL emphasizes the decomposition\\nof a complex task into sub-tasks and allocation of responsibilities to ChatGPTs, representing a\\nbreadth-wise extension, our study aims to facilitate a deep-level understanding of a challenging task\\nby encouraging each ChatGPT to reflect and contribute towards a common goal.\\n\\n2.3    Improving Language Models via Feedback\\nRecently large language models (LLMs) have shown great potential in improving their performance\\nand generating high-quality text by incorporating iterative feedback mechanisms. Madaan et al.\\n[2023] proposed SELF-REFINE, a network that leverages iterative feedback and refinement to\\nimprove initial outputs from LLMs. The approach allows a single LLM to generate an output,\\nprovide multi-aspect feedback on its own output, and refine it based on the feedback, leading to better\\nresults across a range of tasks. Press et al. [2022] investigated the compositionality gap in GPT-3\\nmodels and presented the self-ask method to enhance compositional reasoning. Additionally, Fu et al.\\n[2023], Peng et al. [2023], Yang et al. [2022] have explored various ways of incorporating feedback\\nmechanisms to improve LLM performance and reliability.\\n\\n3\\n      ChatLLM Network\\nIn this section, we first introduce our model architecture (Section 3.1). Then we decribe the feedfor-\\nward process (Section 3.2), followed by the language based backpropagation mechanism (Section\\n\\n                                                               3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='2a733401-2e75-47d3-8a08-6a8e01241c55', embedding=None, metadata={'file_path': 'ChatLLM_Network.pdf', 'file_name': 'ChatLLM_Network.pdf', 'file_type': 'application/pdf', 'file_size': 925390, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='3.3). Lastly, we explain the drop-out mechanism as well as the network optimization (Section 3.4 and\\nSection 3.5).\\n\\n3.1    Network Architecture\\n\\nThe ChatLLM network is a multi-layered dialogue-based language model consisting of n − 1 fully\\nconnected layers and 1 final aggregation layer, as depicted in Figure 2. The models at layer i are\\ndenoted as mi,1, mi,2, . . . , mi,li , where li represents the number of models at layer i. Adjacent\\nlayers of models communicate with each other through a leader-employee relationship, where the\\nmodels at layer i + 1 serves as the leaders for the models at layer i. A dropout and concatenation\\nmechanism is applied after each fully connected layer. The last layer, namely the aggregation layer, is\\ncomprised of one leader model mn, takes the aggregated input from all previous layers and generates\\nthe final output of our network.\\n\\n            Layers  1                        Layers\\n                                                     2           Layers\\n                                                                         n-1                Aggregation  Layer\\n                               Dropout+\\n                               Concat\\n                               Dropout                                             Dropout + Concat\\n                                 +\\n                               Concat\\n\\n                               Dropout\\n                                 +                                                            …                    output\\n                               Concat\\n\\n                               Dropout\\n                                 +\\n                               Concat\\n\\n                                    Question\\n                       Figure 2: ChatLLM Network Architecture and Forward Process\\n\\n3.2    Forward-aggregation Mechanism\\nIn this section, we describe the forward-aggregation mechanism of the ChatLLM network. Unlike\\none standalone LLM, models in our network not only receive the question information itself, but are\\nalso given answers generated by previous layers as references. This enables the subsequent layers of\\nmodels to identify the key highlights from the previous answers, resulting in more comprehensive\\nand precise responses. Such benefits are highly applicable to many tasks such as dialogue generation.\\nOn the other hand, the instability of large models can also be improved, as the integration of outputs\\nfrom multiple members can effectively offset deviations.\\nWe can imagine a real scenario when a leader and many employees need to solve a problem.\\nEach employee may have a unique perspective on the problem, notice different angles and express\\nindividual ideas, but ultimately a leader will consider these ideas and make the final decision on the\\nsolution to the problem. Without considering the opinions of others, the decision would be arbitrary\\nand imperfect. On the other hand, with the suggestions from multiple models, the leader can better\\nevaluate the situation and make a more correct decision. Dialogue-based language models, such as\\nChatGPT, are inherently random because they are based on generative models. This randomness\\nsometimes leads to unexpected outcomes. With a leader evaluating the ideas generated by the\\nemployees and providing guidance, a more optimal outcome can be achieved.\\nWe define mi as a dialogue-based language model, mi as the input of m, mout as the output ofin    i\\nm, and Q represents the description of a question to be solved. Generally, we use mi\\n   i                                                                                                         inand miout   to\\nrepresent the input and the output of mi. ⊕ means concatenation operation.\\n\\n                                                                4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='f7825781-6d26-440b-b049-26db04512ce6', embedding=None, metadata={'file_path': 'ChatLLM_Network.pdf', 'file_name': 'ChatLLM_Network.pdf', 'file_type': 'application/pdf', 'file_size': 925390, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='We use an example of a leader and i employees to illustrate the forward-aggregation mechanism. Let\\nthe leader be denoted as mi+1 and the employees as m1, m2, . . . , mi. Then we have the following\\nrepresentations:\\n                                        m1 in, m2 in, . . . , mi in= Q                                                                    (1)\\n                                        mi+1 = Q ⊕ m1 in        out⊕ m2 out⊕ · · · ⊕ miout\\nFrom equation 1, we can see that the input of each leader is composed of the question and the output\\nof his or her employees. In this way, the leader can not only think independently but also take into\\naccount the opinions of employees.\\nFigure 3 shows a detailed example of the forward-aggregation mechanism on digital mode classifica-\\ntion task.\\n\\n                        Employees                                           Leader\\n                   input：48, 68, 49                                       3Instruction: In this task, you will be given a\\n                                                                           -dimension vector, and you need to guess which\\n                   output：                                                category it belongs to.\\n                   Category: 1\\n                   Rationale: I guess vector (48, 68, 49)\\n                   belongs to category 1 because …                        input:\\n                                                                          Task Definition: [Instruction], here are 3\\n                                                                          responses for your reference:\\n                                                                          Vector: [Input digital vector]\\n                   input：48, 68, 49                                       Reference:\\n                   output：                                                    input De finition:\\n                   Category: 2\\n                   Rationale: I guess vector (48, 68, 49)\\n                   belongs to category 2 because …                        output:\\n                                                                          Category: 1Task   ,   while   the   second   …                           [Instruction]                                    here\\n                   input：48, 68, 49                                       Rationale: I guess vector (48, 68, 49) belongs\\n                                                                          to category 1 because because the first tends                                                                                               are\\n                                                                          to classify it into …\\n                   output：                                                                                              for                                reference\\n                   Category: 3\\n                   Rationale: I guess vector (48, 68, 49)\\n                   belongs to category 3 because …                            responses                                                 your\\n      Figure 3: Illustration of the forward-aggregation mechanism on digital mode classification.Vector:        [Input digital                                             vector]\\n                                                                              Re ference\\n3.3 Language Based Backpropagation Mechanism\\n\\n                        Layers 1                            Layers 2\\n                                                                                   Layers n-1\\n                                                                                                                       AggregationLayer\\n                                           Dropout\\n                      Feedback               +\\n                                           Concat\\n                                           Dropout                                     mn-1,1\\n                      Feedback               +                                output\\n                                           Concat\\n                      Feedback             Dropout\\n                                           Concat+                           Category kFeedbac\\n                                                                              Rationale                                              guess                vector                       (48     68           49)  belongs\\n                                           Dropout                            to         category\\n                                                                                         classify                                       because                        because              the first                      tends\\n                     Feedback                +                                to                                                 it          into                               while       the           second\\n                                           Concat\\n                                  Answer ⊕ Prompt\\n                                                                                       mn-1,2\\n                                              Figure 4: Backpropagation Process\\nTraditional backpropagation mechanism calculates the gradients of the loss function with respect\\nto the weights of a neural network, and utilizes these gradients to update the weights using an\\noptimization algorithm such as gradient descent. Inspired by that, we design a novel language based\\nbackpropagation mechanism to allow the ChatLLM network to learn from the incorrect samples\\n\\n                                                                      5\\n                                                                                        mn-1,3]                                                                   Feedback\\n                                                                                     mn-l,ln _', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3262a86a-eeb0-4d9f-bcba-1ebd300b05df', embedding=None, metadata={'file_path': 'ChatLLM_Network.pdf', 'file_name': 'ChatLLM_Network.pdf', 'file_type': 'application/pdf', 'file_size': 925390, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=' and improve its performance over time, ensuring that incorrect suggestions are corrected and that\\n individual models in the network with correct reasoning remain stable.\\n Similar to real-life scenarios, a leader is able to verify the correct answer with their own earlier than\\n their employees, in contrast to the forward-aggregation process. If a leader is incorrect, he or she\\nwill give orders to each employee of him or her to modify their own ideas. Taking into account these\\n feedbacks, employees will enhance their corresponding responses more effectively.\\n\\nAlgorithm 1 Language Based Backpropagation Mechanism\\n Input: {mi+1,j }: dialogue-based large language models at layer i + 1;mi,∗: an employee model at layer i;\\n            Answer; P rompt.\\n    1: for j = 1 to li+1 do\\n    2:              mini+1,j in← Answer ⊕ P rompt\\n    3:              input mi+1,j to mi+1,j ;\\n    4:              output mi+1out,j from mi+1,j ;\\n    5: end for\\n    6: if notmatch(mi,∗ out, Answer) then\\n    7:\\n    8:\\n    9:              mi, j in∗\\n                    for\\n                    end for← mi,mi,in∗ = 1 to li+1 ⊕ mi+1,j← Answer ⊕ P romptin∗ do out\\n 10:\\n 11: else\\n 13: end ifin∗ ← Answer ⊕ P rompt\\n 12:                mi,\\n 14: input mi,outfrom mi,∗;in∗ to mi,∗;\\n 15: output mi,∗\\n 16: return\\n\\n                                             Employee\\n                                                                                                                                                                    Leader\\n                                         Wrong CategoryWrong Category   The   answer   is   1. wrong.   Please   yourAnswer:\\n                                        Answer: The answer is 1.                                                                                                   Prompt: You guessed it\\n                                                                                                                                                                   speculate a possible reason and update\\n                                        Prompt: You guessed it wrong. Please speculate a possible                                                                  classification criteria.                                                                                                                                                                                              Answer\\n                                        reason and update your classification criteria. Here is one                                                                                                                                                                                                                                                                                                                        The                      answer                             1s\\n                                        person\\'s thinking for your reference:\\n                                        Reference:                                                                                                                 output:\\n                                                                                                                                                                   Category: 1\\n                                                                                                                                                                   Reflection: I guess … belongs to                                                                                                                                                                                        Pr ompt                        You guessed                                                          it                                             Please\\n                                                                                                                                                                   category 1 because …                                                                                                                                                                                                                                                                                                                         wrong\\n                                        output:\\n                                                Category: 1Answer :                                                          The                           answer   is   2. is                                                                                                                                                                                                            speculate                                      possible                                                                                  and             update\\n                                                Reflection: Based on my speculation … belongs                                                                      Answer: The answer                                                                                                                                                                                                                                                                                                        reason                                                                   your\\n                                        to category … because …Fr ompt                                                      You guessed  the   next  input.Prompt: You guessed it right, rememberit                                                                              Please                               speculate                              possible\\n                                                                                                                                                                   your reasoning and wait for                                                 wrong\\n                                        Prompt: You guessed it right, remember your update   … your\\n                                                                                                   Right Category                                                  output:Thank you very much for providingclassification                                                                                           criteria.                                          is                classification                                                       criteria\\n                                        Answer: The answer is 2.reason                                    and                                                      your feedback. My classification\\n                                                                                                                                                                   criteria is                                                                                                                                                                    Here                     one\\n                                        reasoning and wait for the next input.\\n                                        output:person                                                                   thinkingoutput:                                                          for                          your                             reference                                                                                                                outrut_\\n                                        Thank you very much for providing your feedback.\\n                                        My classification criteria is …Reference                                                                                                                                                                                                                                                                                                lcategory:\\n     Figure 5: A feedback example of the backpropagation process in digital mode classification task\\nWe illustrate the language based backpropagation mechanism in Figure 4. Specifically, after the                                                                                                                                                                                                                                                                                          Reflection:                                                                    guess                                         belongs                              to\\n forward process is finished, the final output of the model will be compared with the ground-truth                                                                                                                                                                                                                                                                                      category\\n Answer. If the output is correct, the model will get the prompt (P rompt) like \"You guessed it right,\\n remember your reasoning...\" and thus maintain its original thinking. If it is incorrect, the model                                                                                                                                                                                                                                                                                                                                             because\\nwill get the prompt (P rompt) like \"You guessed it wrong. Please speculate a possible reason why\\n the answer is this and update your thinking.\" and try to improve its thinking process. Feedback is\\n applied throughout all layers in the network, encouraging the correct model to maintain its state, while\\n encouraging the incorrect model to approach the correct answer. One detailed feedback example ofoutput\\n the language based backpropagation process is shown in Figure 5. Let the employee be denoted as\\n                                                                                    Category:6                                                                                                                                                                                         speculation                                                  belongs                                 Answer                         The                        answer                             is 2\\n                                                                                     Reflection                                                                                                  Based                                                on                  my                                                                                                                Prompt                          You guessed                                                         it                right                         remember\\n                                                to                          category                                                                                             because                                                                                                                                                                                                   your                reasoning                                         and             wait                           for                 the             next       input\\n                                                                                                                                                                                                                                                                                                                                                                                          cutrut\\n                                                           Right Category                                                                                                                                                                                                                                                                                                               yourThankfeedback.you                                                            veryMy             classificationmuch                 for providing\\n                                                  Answer                                                                   The                                 answer                                                           13                                                                                                                                                         criteria                                 is\\n                                                 Prompt                                                                   You guessed                                                                                                 it                  right,                                    remember                                your\\n                                                 reasoning                                                                                     and                               wait                                           for                        the                 next                           input                                                                     output:\\n                                                  output\\n                                                Thank                                                   you                                  very                                         much                                       for providing                                                                           your feedback\\n                                                My                          classification                                                                                                                             criteria                                               is', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='6ac8efde-dfdc-40ba-9992-00bf8834e274', embedding=None, metadata={'file_path': 'ChatLLM_Network.pdf', 'file_name': 'ChatLLM_Network.pdf', 'file_type': 'application/pdf', 'file_size': 925390, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='mi and the leaders as mi+1, mi+2, . . . , mi+n. Formally, if the model mi outputs a wrong answer,\\nit will get the following feedback as input (as shown in the upper left in Figure 5):\\n                               mi = Answer ⊕ P rompt ⊕ mi+1⊕ mi+2⊕ · · · ⊕ mn\\n                                                                             in       out           out   out  (2)\\nOtherwise, the model will get input (shown in bottom left in Figure 5):\\n                                                         mi = Answer ⊕ P romptin                               (3)\\nThe detailed algorithm is described in Algorithm 1.\\n\\n3.4      Dropout Mechanism\\nThe capacity of an individual dialogue-based language model is inherently constrained. By restricting\\nthe input to an appropriate range, we can prevent these models from becoming inundated with an\\nexcessive amount of information. Additionally, the implementation of a dropout mechanism in neural\\nnetworks, as described in [Srivastava et al., 2014], has been shown to effectively reduce overfitting\\nand enhance generalization performance. Therefore we devise a dropout mechanism, as describle in\\nequation 4.\\nAnalogously, if a leader has too many employees, it may be difficult for them to handle all of his\\nor her employees’ ideas. Similarly, if an employee has too many leaders, it may be challenging\\nfor them to satisfy all of leaders. Therefore, based on the structure of the entire network, we allow\\neach dialogue-based language model to randomly receive messages from only a limited number of\\nother models, thus ensuring that the overall input is controlled within a certain range. Formally, to\\nimplement it, we calculate a random variable r whose value is 0 or 1:\\n                                                                 r ∼ Bernoulli(ρ)                              (4)\\n\\nwhere ρ is the rate of the number of selected models. Then the model mi+1 receives selected\\nmessages from the sender models:\\n                                      mi+1 = r1 · m1in          out⊕ r2 · m2out⊕ · · · ⊕ ri · miout            (5)\\nwhere if ri = 1, ri · mioutequals miout; otherwise, ri · mioutis a null string.\\n\\n3.5      Network Optimization\\n\\nDuring the training process, individual training examples are inputted sequentially. Inspired by\\nthe Stochastic Gradient Descendent algorithm, we update the network with the language-based\\nbackpropagation mechanism for each training sample accordingly. To prevent overfitting, we employ\\nthe early stopping technique. The stopping criteria can be met by either of the two conditions: reaching\\na predetermined number of iterations, or the performance ceasing to show further improvement.\\n\\n4\\n       Experiments\\nChatGPT is currently one of the most widely used conversational language models. However, due\\nto the limitations of ChatGPT4, we choose to use ChatGPT3.5 as the basic member of our overall\\nnetwork. A collection of ChatGPT3.5 is supposed to learn from and refer to each other when solving\\nthe prompted question.\\nWe conduct two experiments to test the network: the digital mode classification experiment and the\\nsentiment reversal experiment. The former aims to separate the model’s learning abilities from the\\nexisting implicit knowledge in a large language model, since the model is unaware of the predefined\\nrules amongst the digits. The latter is designed to demonstrate that our proposed network can\\nsignificantly improve the performance in traditional NLP tasks.\\nIn terms of the ChatLLM network structure, we design a two-layer structure. The first layer consists\\nof three ChatGPTs, and the second layer consists of one ChatGPT. Detailed experimental details are\\nas follows.\\n\\n                                                                               7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='02e7b740-c258-4c63-8293-1b221d3e6911', embedding=None, metadata={'file_path': 'ChatLLM_Network.pdf', 'file_name': 'ChatLLM_Network.pdf', 'file_type': 'application/pdf', 'file_size': 925390, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='4.1     Digital Mode Classification\\nThe experiment aims to test ChatGPT’s learning ability from scratch. In the digital mode classification\\ntask, we generate a dataset consisting of different categories of digital vectors. Particularly, we\\ncategorize a three-dimensional vector (a, b, c) based on the position of the largest dimension in the\\nvector. For example, (1, 2, 4) belongs to category 3 because the largest number 4 is located in the third\\ndimension. Since ChatGPT has no pre-existing knowledge of the task, this provides an opportunity to\\nevaluate its inductive learning capability.\\nWe expect ChatGPT to output the category of a digital vector input, as well as the rationales which\\nshould also align with the predefined rules.\\nWe conduct eight observations at intermediate stages of the training and feedback process, wherein\\nthree vectors are prompted to every ChatGPT model between stages, and potential feedback is\\nprovided in accordance with each setting’s configuration. The testing set comprises of 30 challenging\\nsamples that have been manually designed, which are collectively fed into the model. The outputs are\\nthe corresponding label for each vector. No feedback is involved in the testing process.\\nFor evaluation, we report the accuracy by checking if the categories are consistent with the pre-defined\\nrules. We compare our ChatLLM network model with the following baselines:\\n         • ChatGPT-w/o FB: a vanilla ChatGPT takes the instruction and training input vectors with\\n            categories as input without further feedback\\n         • ChatGPT-refine: a vanilla ChatGPT takes the same input as ChatGPT-w/o FB, and if the\\n            answer is incorrect, we request it to refine the answer with the instruction \"refine your\\n            answer\"\\n         • ChatGPT-ensemble uses simple voting and selects the most frequent answer amongst three\\n            individual ChatGPTs as the consensus output.\\n\\n                                   1             2            3            4            5            6           7          8\\n      ChatGPT-w/o FB             0.295         0.306        0.328        0.328        0.345        0.333       0.361     0.400\\n                               (±24.10%)    (±18.77%)    (±19.76%)    (±21.74%)     (±11.73%)    (±12.68%)  (±10.81%)  (±14.90%)\\n    ChatGPT-refine(mean)         0.300         0.339        0.374        0.389        0.383        0.367       0.334     0.378\\n                               (±23.27%)    (±26.72%)    (±16.42%)    (±11.74%)     (±16.30%)    (±29.22%)  (±31.55%)  (±18.22%)\\n     ChatGPT-ensemble            0.317         0.317        0.389        0.361        0.367        0.361       0.317     0.361\\n                               (±12.90%)    (±12.83%)     (±6.93%)     (±7.00%)     (±15.18%)    (±21.40%)  (±19.68%)  (±10.85%)\\n    ChatGPT-mem (mean)           0.406         0.472        0.428        0.456        0.439        0.439       0.344     0.406\\n                               (±28.63%)    (±22.32%)    (±26.68%)    (±17.03%)     (±24.19%)    (±25.12%)  (±20.95%)   (±6.12%)\\n  ChatGPT Network (leader)       0.478         0.528        0.417        0.461        0.483        0.433       0.400     0.383\\n                               (±19.03%)    (±13.48%)    (±16.55%)     (±9.63%)     (±16.74%)    (±17.54%)   (±7.38%)  (±14.28%)\\n\\n                    Table 1: Model accuracy of different models at 8 intermediate stages.\\nTable 1 reports the average results of six times for all the models. From the table, we can observe that:\\n1) In terms of accuracy, the members of the ChatGPT network have a significantly higher accuracy\\nrate than the baselines, which fully demonstrates the enormous advantages of the ChatGPT network\\nin terms of mutual communication and feedback. Among them, the output of the ChatGPT Network\\n(learder), which is also the final output of the ChatGPT network, is significantly higher than that\\nof the other members. This also indicates the advantage of the entire network in obtaining output\\nthrough the forward and feedback processes. 2) Regarding the level of variance, the accuracy rate of\\nChatGPT Network (learder) is generally similar to that of ChatGPT-ensemble and lower than that of\\nChatGPT in other baselines. This indicates that the output of the ChatGPT network is relatively stable.\\nTaking both aspects into account, we conclude that the ChatGPT network is capable of delivering\\nmore stable and accurate output results than the baselines.\\nFigure 6 illustrates the comparison of accuracy of all experiment settings. In terms of accuracy,\\nthe members of the ChatLLM network achieved better results. From the entire figure, most curves\\nreached their highest value relatively early, indicating that early stopping during the training process\\nis necessary to achieve the best results. On the other hand, almost all ChatGPT curves showed some\\ninstability after reaching the peak, possibly due to input overload causing forgetting or unexpected\\nchanges. During the experiment, we also observed that the ChatLLM network gradually increased\\nthe complexity of the classification criteria after reaching the peak, exhibiting a phenomenon similar\\nto overfitting.\\n\\n                                                                   8', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='97012870-0f32-4d99-b439-7b80c84d9a9c', embedding=None, metadata={'file_path': 'ChatLLM_Network.pdf', 'file_name': 'ChatLLM_Network.pdf', 'file_type': 'application/pdf', 'file_size': 925390, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='             0.550\\n\\n             0.500\\n\\n             0.450\\n\\n             0.400\\n\\n             0.350\\n\\n             0.300\\n\\n             0.250\\n\\n             0.200\\n                        1           2           3           4           5           6           7           8\\n\\n                                  ChatGPT-w/o FB          ChatGPT-refine(mean)   ChatGPT-m(mean)\\n                                  ChatGPT-leader          ChatGPT-ensemble\\n\\n          Figure 6: Comparison of different models on accuracy along the intermediate stages.\\n\\n4.2     Sentiment Reversal Experiment\\n\\nSentiment reversal is a typical NLP task that rewrites a given sentence by reversing its current\\nsentiment (positive or negative) [Madaan et al., 2023].\\n\\nDataset Generation             We generate a dataset comprising 60 emotionally biased sentences, along\\nwith their corresponding sentiments, using ChatGPT.\\n\\nExperimental Setting             Our objective is to examine the efficacy of the network by backpropagating\\nfeedback from leaders to employees. To accomplish this, we have structured the experiments into\\ntwo groups.\\n\\n         • w/o Feedback: All instances of ChatGPTs are assigned with a sentiment reversal task. For\\n            the network group, the leader ChatGPT consolidates the outputs generated by its employees.\\n         • w/ Feedback: We direct both the baseline model ChatGPT and the ChatGPT network to\\n            augment the emotional intensity of the former output sentences, with the instruction of\\n            \"Make the emotionally reversed sentences more emotionally intense\". In this stage, the\\n            baseline model ChatGPT employs self-feedback, whereas the ChatGPT network utilizes\\n            backpropagation for feedback provision.\\n\\nEvaluation         Similar to [Madaan et al., 2023], the evaluation process incorporates a separate Chat-\\nGPT as a judge, which is responsible for determining which group produced sentences with more\\nintense emotions. The scores assigned by the ChatGPT judge are reported in Table 4.2 and Figure 7.\\nEach superior sentence earns one point for the generating model. To ensure fairness and eliminate\\npotential biases in the ChatGPT judge’s scoring, we request the provision of a rationale for each\\ndecision as illustrated in Table 3.\\nThe results in Table 4.2 and Figure 7 reveal that without feedback, the ChatGPT network displays an\\nmarginally enhanced performance compared to an isolated ChatGPT (baseline model), attributable to\\nits ability to summarize information. However, when feedback is employed, our ChatGPT network\\nsignificantly outperforms the standalone baseline ChatGPT, showing the immense improvement of\\nthe feedback on the ChatGPT network’s performance.\\n\\n                                                                        Win       Loss      Tie      total\\n                     w/o Feedback                ChatGPT                  22        26       12       60\\n                                           ChatGPT network                26        22       12       60\\n                     w/ Feedback                 ChatGPT                   7        53        0       60\\n                                           ChatGPT network                53         7        0       60\\n                    Table 2: The results of sentiment reversal without and with feedback\\n\\n                                                                9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='afa72eaf-b4f5-40cb-86fb-3c41c2b419cd', embedding=None, metadata={'file_path': 'ChatLLM_Network.pdf', 'file_name': 'ChatLLM_Network.pdf', 'file_type': 'application/pdf', 'file_size': 925390, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='                                                                                                                                     Win Times\\n                                                                                    w/ Feedback\\n\\n                                                                                  w/o Feedback\\n\\n                                                                                                        0        10         20        30         40     50  60  70\\n                                                                                                              ChatGPT             ChatGPT Network\\n\\n                                                                 Figure 7: Comparison of win times between ChatGPT and ChatGPT network\\n\\n                 ChatGPT                             ChatGPT network                          Result\\n\\n  Example1       This movie is incredibly dull.      This movie is excruciatingly dull.       ChatGPT network wins\\n\\n\\n\\n  Example2       This journey is completely          This journey is soul-crushingly          ChatGPT network wins\\n                 unfulfilling.                       and utterly unfulfilling\\n\\nTable 3: Sentiment reversal examples. The original inputs are as follows. Example 1: This movie is\\ninteresting. Example 2: This journey is satisfying.\\nReason\\nBoth sentences express negative emotions\\ntowards the movie, but \"excruciatingly\\ndull\" implies a stronger degree of negative\\nfeeling compared to \"incredibly dull.\"\\nThe addition of \"soul-crushingly and\\nutterly\" intensifies the negative\\nemotion of the sentence,making it feel\\nmore impactful and powerful.\\n\\n\\n\\n                                              In table 3, two examples are provided to illustrate the ChatGPT network’s results. As we can observe\\n                                              that compared to ChatGPT, our proposed ChatGPT network can generate a sentence with reversal\\n                                              sentiment in higher emotional intensity of adjectives and in richer vocabulary.\\n\\n                                              5\\n                                                      Limitations\\n                                              Due to the limited capabilities of current dialogue-based language models, especially in processing\\n                                              large-scale numerical inputs, our network may not demonstrate absolute superiority in certain\\n                                              scenarios. Furthermore, the absence of an efficacious communication mechanism among different\\n                                              dialogue language models precludes larger scales of collaboration, restricting our present research to\\n                                              few members of the network.\\n\\n                                              6\\n                                                      Conclusion\\n                                              In this work, we propose a novel ChatLLM network that allows multiple dialogue-based language\\n                                              models to interact, provide feedback, and think together. Specifically, individual instances of\\n                                              ChatLLM in the network may possess distinct perspectives towards the same problem, and by\\n                                              consolidating these diverse viewpoints via a separate ChatLLM, the ChatLLM network system can\\n                                              conduct decision-making more objectively and comprehensively. The optimization of the network is\\n                                              carried out based on a novel language-based backpropagation mechanism. We evaluate the network’s\\n                                              performance through experiments on two tasks, demonstrating the effectiveness and superiority of\\n                                              the ChatLLM network.\\n                                              While acknowledging the existence of limitations, such as the lack of a unified mechanism for\\n                                              communication between the models, we believe that our research will serve as a foundational work to\\n                                              provide valuable insights to guide future endeavor in the field. As part of our ongoing efforts, we plan\\n                                              to develop and implement a global strategy for assigning distinct identities to each model, thereby\\n\\n                                                                                                                                  10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8b70a8ea-890b-405c-83a4-0d7d94f62108', embedding=None, metadata={'file_path': 'ChatLLM_Network.pdf', 'file_name': 'ChatLLM_Network.pdf', 'file_type': 'application/pdf', 'file_size': 925390, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='ensuring that each model performs its exclusive task and enhancing the traceability of inter-model\\ncommunication.\\n\\nReferences\\n [1] Jason Wei, Xuezhi Wang, Dale Schuurmans, and et al. Chain of thought prompting elicits\\n      reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022.\\n [2] Vaswani Ashish, Shazeer Noam, Parmar Niki, and et al. Attention is all you need. Advances in\\n      neural information processing systems, 30, 2017.\\n [3] Devlin Jacob, Chang Ming-Wei, Lee Kenton, and et al. Bert: Pre-training of deep bidirectional\\n      transformers for language understanding. ArXiv, abs/1810.04805, 2019.\\n [4] Alec Radford, Karthik Narasimhan, Tim Salimans, and et al. Improving language understanding\\n      by generative pre-training. 2018.\\n [5] Alec Radford, Jeff Wu, Rewon Child, and et al. Language models are unsupervised multitask\\n      learners. OpenAI blog, 1(8), 2019.\\n [6] Raffel Colin, Shazeer Noam, Roberts Adam, and et al. Exploring the limits of transfer learning\\n      with a unified text-to-text transformer. The Journal of Machine Learning Research, 21, 2020.\\n [7] Tom B. Brown, Benjamin Mann, Nick Ryder, and et al. Language models are few-shot learners.\\n      Advances in Neural Information Processing Systems, 33, 2020.\\n [8] Wei Jason, Bosma Maarten, Y. Zhao Vincent, and et al. Finetuned language models are zero-shot\\n      learners. ArXiv, abs/2109.01652, 2022.\\n [9] W. Bradley Knox and P. Stone. Tamer: Training an agent manually via evaluative reinforcement.\\n      7th IEEE International Conference on Development and Learning, 2008.\\n[10] Ouyang Long, Wu Jeff, Jiang Xu, and et al. Training language models to follow instructions\\n      with human feedback. Advances in Neural Information Processing Systems, 35, 2022.\\n[11] Shen Yongliang, Song Kaitao, Tan Xu, and et al. Hugginggpt: Solving ai tasks with chatgpt and\\n      its friends in huggingface. ArXiv, abs/2303.17580, 2023.\\n[12] Bruce Richards Toran. Auto-gpt: An autonomous gpt-4 experiment. https://github.com/\\n      Significant-Gravitas/Auto-GPT, 2023.\\n[13] Sung Park Joon, C. O’Brien Joseph, J. Cai Carrie, and et al. Generative agents: Interactive\\n      simulacra of human behavior. ArXiv, abs/2304.03442, 2023.\\n[14] Li Guohao, Abed Al Kader Hammoud Hasan, Itani Hani, and et al. Camel: Communicative\\n      agents for \"mind\" exploration of large scale language model society. ArXiv, abs/2303.17760,\\n      2023.\\n[15] Aman Madaan, Niket Tandon, Prakhar Gupta, and et al. Self-refine: Iterative refinement with\\n      self-feedback. ArXiv, abs/2303.17651, 2023.\\n[16] Ofir Press, Muru Zhang, Sewon Min, and et al. Measuring and narrowing the compositionality\\n      gap in language models. ArXiv, abs/2210.03350, 2022.\\n[17] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and et al. Gptscore: Evaluate as you desire. ArXiv,\\n      abs/2302.04166, 2023.\\n[18] Baolin Peng, Michel Galley, Pengcheng He, and et al. Check your facts and try again: Im-\\n      proving large language models with external knowledge and automated feedback. ArXiv,\\n      abs/2302.12813, 2023.\\n[19] Kevin Yang, Nanyun Peng, Yuandong Tian, and et al. Re3: Generating longer stories with\\n      recursive reprompting and revision. Conference on Empirical Methods in Natural Language\\n      Processing, 2022.\\n[20] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and et al. Dropout: A\\n      simple way to prevent neural networks from overfitting. Journal of Machine Learning Research,\\n      15(56):1929–1958, 2014. URL http://jmlr.org/papers/v15/srivastava14a.html.\\n\\n                                                             11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1493c661-440b-4e23-91e9-ad150ff78ba6', embedding=None, metadata={'file_path': 'ChatLLM_Network.pdf', 'file_name': 'ChatLLM_Network.pdf', 'file_type': 'application/pdf', 'file_size': 925390, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A    DMC Task Dialogue Example\\nA.1 Forward-aggregation\\n\\n ChatGPT1-input: 48, 68, 49\\n ChatGPT1-output: Based on my guess, this data point may belong to the first\\n category, as the value of the first element is the largest. However, I still need\\n more input data and category definitions to give a more accurate answer.\\n ChatGPT2-input: 48, 68, 49\\n ChatGPT2-output: Based on the previous speculation, the first feature value of\\n this data point is 48, which belongs to the first category. However, according\\n to the explanation of the second person, they only focused on the second feature\\n value of the data point, which is 68, and classified it as the second category.\\n Therefore, based on the previous speculation and the second person’s explanation,\\n I would speculate that this data point belongs to the second category. However,\\n this is still a subjective judgment and assumption and cannot guarantee accuracy.\\n ChatGPT3-input: 48, 68, 49\\n ChatGPT3-output: Based on my initial guess and the second person’s explanation,\\n I would lean towards classifying this data point as belonging to the second class\\n because of its large second feature value. However, considering the previous three\\n data points and different people’s classification results, I cannot completely\\n rule out the possibility of it belonging to another class. This is still a\\n subjective judgment and hypothesis and cannot guarantee accuracy. If you can\\n provide more data points and class definitions, I can use machine learning\\n algorithms to classify these data points more accurately.\\n ChatGPT-leader-input: You need to guess (48, 68, 49), and here are three responses\\n for your reference:\\n The first person: Based on previous assumptions and the classification of the\\n previous data, I guess this data point may belong to the first category because\\n the first value is relatively large, the second value is relatively small, and the\\n third value is also relatively small, which may match the feature distribution\\n of the first category.\\n The second person: According to my guess, this data point may belong to the first\\n category because the first element has the largest value.\\n The third person: Based on previous assumptions, this data point’s first feature\\n value is 48, which belongs to the first category. According to the second person’s\\n explanation, he only paid attention to the second feature value of the data point,\\n which is 68, and classified it as the second category. Therefore, combining the\\n previous assumptions and the second person’s explanation, I would guess that this\\n data point belongs to the second category.\\n ChatGPT-leader-output: Based on the responses of the three individuals, they have\\n different classifications for (48, 68, 49). The first and third individuals tend\\n to classify it into the first category because the first feature value is large.\\n The second individual tends to classify it into the second category because the\\n second feature value is large. Taking into account all the responses, I would lean\\n towards classifying it into the second category because in previous assumptions\\n and the second individual’s explanation, the second feature value is considered\\n an important factor affecting classification and is more important than the third\\n feature value.\\n\\n                                                   12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='935cd4e5-3073-4bdf-8e64-5f4be76116df', embedding=None, metadata={'file_path': 'ChatLLM_Network.pdf', 'file_name': 'ChatLLM_Network.pdf', 'file_type': 'application/pdf', 'file_size': 925390, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='A.2 Backpropagation\\n\\n  ChatGPT-leader-input: You guessed it right, remember your reasoning and wait for\\n  the next input.\\n  ChatGPT-leader-output: Sure, thank you for the feedback. I will keep in mind my\\n  guesses and reasons, and wait for the next input.\\n  ChatGPT1-input: You guessed wrong. Please speculate a possible reason and update\\n  your classification criteria.Here is one person’s thinking for your reference:\\n  According to the answers of the three people,they have different classifications\\n  for (48, 68, 49).\\n  The first and third person tend to classify it into the first category because\\n  the first feature value is large.The second person tends to classify it into the\\n  second category because the second feature value is large.Considering all the\\n  opinions, I would lean towards classifying it into the second category because\\n  in previous hypotheses and the explanations of the second person, the second\\n  feature value is considered as an important factor for classification and it is\\n  more important than the third feature value.\\n  ChatGPT1-output: Based on my speculation, this data point may belong to the second\\n  category, as the value of the second element is the highest and, according to\\n  some hypotheses and interpretations, the second feature value is considered an\\n  important factor affecting the classification. However, this is still based on\\n  subjective judgment and assumption, and more input data and category definitions\\n  are needed to provide a more accurate classification.\\n  ChatGPT2-input: You are right!\\n  ChatGPT2-output: Thank you very much for the feedback you provided. It helps me\\n  better understand the relationship between the data and the categories.\\n  ChatGPT3-input: You are right!\\n  ChatGPT3-output: Thank you very much for providing your feedback.\\n\\n                                                    13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='01e39c7d-ef31-4680-b144-30531f88337c', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\n                  Cognitive Architectures for Language Agents\\n                 Theodore R. Sumers∗                 Shunyu Yao∗           Karthik Narasimhan                 Thomas L. Griffiths\\n                 Princeton University\\n                 {sumers, shunyuy, karthikn, tomg}@princeton.edu\\n\\n                  Reviewed on OpenReview: https: // openreview. net/ forum? id= 1i6ZCvflQJ\\n\\narXiv:2309.02427v3  [cs.AI]  15 Mar 2024Abstract\\n                            Recent efforts have augmented large language models (LLMs) with external resources (e.g.,\\n                            the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding\\n                            or reasoning, leading to a new class of language agents. While these agents have achieved\\n                            substantial empirical success, we lack a framework to organize existing agents and plan future\\n                            developments. In this paper, we draw on the rich history of cognitive science and symbolic\\n                            artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA).\\n                            CoALA describes a language agent with modular memory components, a structured action\\n                            space to interact with internal memory and external environments, and a generalized decision-\\n                            making process to choose actions. We use CoALA to retrospectively survey and organize\\n                            a large body of recent work, and prospectively identify actionable directions towards more\\n                            capable agents. Taken together, CoALA contextualizes today’s language agents within the\\n                            broader history of AI and outlines a path towards language-based general intelligence.\\n\\n                  1\\n                       Introduction\\n                 Language agents (Weng, 2023; Wang et al., 2023b; Xi et al., 2023; Yao and Narasimhan, 2023) are an emerging\\n                  class of artifical intelligence (AI) systems that use large language models (LLMs; Vaswani et al., 2017; Brown\\n                  et al., 2020; Devlin et al., 2019; OpenAI, 2023a) to interact with the world. They apply the latest advances\\n                  in LLMs to the existing field of agent design (Russell and Norvig, 2013). Intriguingly, this synthesis offers\\n                  benefits for both fields. On one hand, LLMs possess limited knowledge and reasoning capabilities. Language\\n                  agents mitigate these issues by connecting LLMs to internal memory and environments, grounding them to\\n                  existing knowledge or external observations. On the other hand, traditional agents often require handcrafted\\n                  rules (Wilkins, 2014) or reinforcement learning (Sutton and Barto, 2018), making generalization to new\\n                  environments challenging (Lake et al., 2016). Language agents leverage commonsense priors present in LLMs\\n                  to adapt to novel tasks, reducing the dependence on human annotation or trial-and-error learning.\\n                 While the earliest agents used LLMs to directly select or generate actions (Figure 1B; Ahn et al., 2022;\\n                  Huang et al., 2022b), more recent agents additionally use them to reason (Yao et al., 2022b), plan (Hao et al.,\\n                 2023; Yao et al., 2023), and manage long-term memory (Park et al., 2023; Wang et al., 2023a) to improve\\n                  decision-making. This latest generation of cognitive language agents use remarkably sophisticated internal\\n                  processes (Figure 1C). Today, however, individual works use custom terminology to describe these processes\\n                 (such as ‘tool use’, ‘grounding’, ‘actions’), making it difficult to compare different agents, understand how\\n                  they are evolving over time, or build new agents with clean and consistent abstractions.\\n                  In order to establish a conceptual framework organizing these efforts, we draw parallels with two ideas\\n                  from the history of computing and artificial intelligence (AI): production systems and cognitive architectures.\\n                  Production systems generate a set of outcomes by iteratively applying rules (Newell and Simon, 1972).\\n                 They originated as string manipulation systems – an analog of the problem that LLMs solve – and were\\n                  subsequently adopted by the AI community to define systems capable of complex, hierarchically structured\\n                     ∗Equal contribution, order decided by coin flip. Each person reserves the right to list their name first. A CoALA-based repo\\n                  of recent work on language agents: https://github.com/ysymyth/awesome-language-agents.\\n\\n                                                                                      1', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='cde19242-dfb3-47b6-9d8d-d1af40fa4961', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n                                                                                       Cognitive Language Agent\\n                                                                                        Memory         Reasoning\\n\\n                         Input            LLM          Output                      Retrieval Learning\\n\\n                                   Language Agent\\n                Observations                                Actions\\n                                                                             Observations                        Actions     Memory  Reasoning\\n                                    Environment                                               Environment\\n\\nFigure 1: Different uses of large language models (LLMs). A: In natural language processing (NLP), an LLMRetrieval Learning\\ntakes text as input and outputs text. B: Language agents (Ahn et al., 2022; Huang et al., 2022c) place the\\nLLM in a direct feedback loop with the external environment by transforming observations into text and\\nusing the LLM to choose actions. C: Cognitive language agents (Yao et al., 2022b; Shinn et al., 2023; Wang\\net al., 2023a) additionally use the LLM to manage the agent’s internal state via processes such as learning\\nand reasoning. In this work, we propose a blueprint to structure such agents.\\n\\nbehaviors (Newell et al., 1989). To do so, they were incorporated into cognitive architectures that specified\\ncontrol flow for selecting, applying, and even generating new productions (Laird et al., 1987; Laird, 2022;\\nKotseruba and Tsotsos, 2020). We suggest a meaningful analogy between production systems and LLMs: just\\nas productions indicate possible ways to modify strings, LLMs define a distribution over changes or additions\\nto text. This further suggests that controls from cognitive architectures used with production systems might\\nbe equally applicable to transform LLMs into language agents.\\nThus, we propose Cognitive Architectures for Language Agents (CoALA), a conceptual framework to\\ncharacterize and design general purpose language agents. CoALA organizes agents along three key dimensions:\\ntheir information storage (divided into working and long-term memories); their action space (divided into\\ninternal and external actions); and their decision-making procedure (which is structured as an interactive\\nloop with planning and execution). Through these three concepts (memory, action, and decision-making),\\nwe show CoALA can neatly express a large body of existing agents and identify underexplored directions\\nto develop new ones. Notably, while several recent papers propose conceptual architectures for general\\nintelligence (LeCun, 2022; McClelland et al., 2019) or empirically survey language models and agents (Mialon\\net al., 2023; Weng, 2023; Wang et al., 2023b), this paper combines elements of both: we propose a theoretical\\nframework and use it to organize diverse empirical work. This grounds our theory to existing practices and\\nallows us to identify both short-term and long-term directions for future work.\\nThe plan for the rest of the paper is as follows. We first introduce production systems and cognitive\\narchitectures (Section 2) and show how these recent developments in LLMs and language agents recapitulate\\nthese historical ideas (Section 3). Motivated by these parallels, Section 4 introduces the CoALA framework\\nand uses it to survey existing language agents. Section 5 provides a deeper case study of several prominent                                     Actions\\nagents. Section 6 suggests actionable steps to construct future language agents, while Section 7 highlights\\nopen questions in the broader arc of cognitive science and AI. Finally, Section 8 concludes. Readers interested\\nin applied agent design may prioritize Sections 4-6.                             Observations\\n\\n                                                                   2', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ec0967cd-09e2-4e64-846b-b6f4bbf90781', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\n2\\n      Background: From Strings to Symbolic AGI\\nWe first introduce production systems and cognitive architectures, providing a historical perspective on\\ncognitive science and artificial intelligence: beginning with theories of logic and computation (Post, 1943),\\nand ending with attempts to build symbolic artificial general intelligence (Newell et al., 1989). We then\\nbriefly introduce language models and language agents. Section 3 will connect these ideas, drawing parallels\\nbetween production systems and language models.\\n\\n2.1    Production systems for string manipulation\\n\\nIn the first half of the twentieth century, a significant line of intellectual work led to the reduction of\\nmathematics (Whitehead and Russell, 1997) and computation (Church, 1932; Turing et al., 1936) to symbolic\\nmanipulation. Production systems are one such formalism. Intuitively, production systems consist of a set\\nof rules, each specifying a precondition and an action. When the precondition is met, the action can be\\ntaken. The idea originates in efforts to characterize the limits of computation. Post (1943) proposed thinking\\nabout arbitrary logical systems in these terms, where formulas are expressed as strings and the conclusions\\nthey license are identified by production rules (as one string “produces” another). This formulation was\\nsubsequently shown to be equivalent to a simpler string rewriting system. In such a system, we specify rules\\nof the form\\n                                                           X Y Z → X W Z\\n\\nindicating that the string XY Z can be rewritten to the string XW Z. String rewriting plays a significant\\nrole in the theory of formal languages, in the form of Chomsky’s phrase structure grammar (Chomsky, 1956).\\n\\n2.2    Control flow: From strings to algorithms\\n\\nBy itself, a production system simply characterizes the set of strings that can be generated from a starting point.\\nHowever, they can be used to specify algorithms if we impose control flow to determine which productions are\\nexecuted. For example, Markov algorithms are production systems with a priority ordering (Markov, 1954).\\nThe following algorithm implements division-with-remainder by converting a number written as strokes | into\\nthe form Q ∗ R, where Q is the quotient of division by 5 and R is the remainder:\\n\\n                                                             ∗|||||   →    | ∗\\n                                                                 ∗    − ∗\\n                                                                         →•\\n                                                                      →    ∗\\n\\nwhere the priority order runs from top to bottom, productions are applied to the first substring matching\\ntheir preconditions when moving from left to right (including the empty substring, in the last production),\\nand − indicates the algorithm halts after executing the rule. The first rule effectively “subtracts” five if\\n      →•\\npossible; the second handles the termination condition when no more subtraction is possible; and the third\\nhandles the empty substring input case. For example, given the input 11, this would yield the sequence of\\nproductions ∗||||||||||| → | ∗ |||||| → || ∗ | −→ || ∗ | which is interpreted as 2 remainder 1. Simple productions can\\n                                                                                                                      •\\nresult in complex behavior – Markov algorithms can be shown to be Turing complete.\\n\\n2.3    Cognitive architectures: From algorithms to agents\\n\\nProduction systems were popularized in the AI community by Allen Newell, who was looking for a formalism\\nto capture human problem solving (Newell, 1967; Newell and Simon, 1972). Productions were generalized\\nbeyond string rewriting to logical operations: preconditions that could be checked against the agent’s goals\\nand world state, and actions that should be taken if the preconditions were satisfied. In their landmark book\\nHuman Problem Solving (Newell and Simon, 1972), Allen Newell and Herbert Simon gave the example of a\\n\\n                                                                     3', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='e881df07-ed2d-4b5f-8056-ee3695ad134f', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='                     Published in Transactions on Machine Learning Research (02/2024)\\n                                                         Symbolic Long_Term Memories_                                                    Input\\n                                              Procedural                 Semantic                   Episodic\\n                                                                                                                                     Proposal and\\n                                                                                                                                      Evalutation\\n                                               RL           Chunking           Semantic                Episodic\\n                                                                              Learning                 Learning\\n                                     8yl                                                                                                Action\\n                                                    ]             Symbolic Working Memory                                              Selection\\n                                                                                                                                      Application\\n                                                     Spatial-Visual System    Perceptual LT Memory\\n                                     Other Perception  Visual Perception                                 Motor                          Output\\n                                                                        Embodiment\\n                     Figure 2: Cognitive architectures augment a production system with sensory groundings, long-term memory,\\n                     and a decision procedure for selecting actions. A: The Soar architecture, reproduced with permission from\\n                     Laird (2022). B: Soar’s decision procedure uses productions to select and implement actions. These actions\\n                     may be internal (such as modifying the agent’s memory) or external (such as a motor command).\\n\\nsimple production system implementing a thermostat agent:\\n             (temperature > 70◦) ∧ (temperature < 72◦)\\n                                             temperature < 32◦\\n                       (temperature < 70◦) ∧ (furnace off)\\n                       (temperature > 72◦) ∧ (furnace on)\\n\\n→     stop\\n→     call for repairs; turn on electric heater\\n→     turn on furnace\\n→     turn off furnace\\n\\n                     Following this work, production systems were adopted by the AI community. The resulting agents con-\\n                     tained large production systems connected to external sensors, actuators, and knowledge bases – requiring\\n                     correspondingly sophisticated control flow. AI researchers defined “cognitive architectures” that mimicked\\n                     human cognition – explicitly instantiating processes such as perception, memory, and planning (Adams et al.,\\n                     2012) to achieve flexible, rational, real-time behaviors (Sun, 2004; Newell, 1980; 1992; Anderson and Lebiere,\\n                     2003). This led to applications from psychological modeling to robotics, with hundreds of architectures and\\n                     thousands of publications (see Kotseruba and Tsotsos (2020) for a recent survey).\\n                     A canonical example is the Soar architecture (Fig. 2A). Soar stores productions in long-term memory and\\n                     executes them based on how well their preconditions match working memory (Fig. 2B). These productions\\n                     specify actions that modify the contents of working and long-term memory. We next provide a brief overview\\n                     of Soar and refer readers to Laird (2022; 2019) for deeper introductions.\\n                     Memory. Building on psychological theories, Soar uses several types of memory to track the agent’s\\n                     state (Atkinson and Shiffrin, 1968). Working memory (Baddeley and Hitch, 1974) reflects the agent’s current\\n                     circumstances: it stores the agent’s recent perceptual input, goals, and results from intermediate, internal\\n                     reasoning. Long term memory is divided into three distinct types. Procedural memory stores the production\\n                     system itself: the set of rules that can be applied to working memory to determine the agent’s behavior.\\n                     Semantic memory stores facts about the world (Lindes and Laird, 2016), while episodic memory stores\\n                     sequences of the agent’s past behaviors (Nuxoll and Laird, 2007).\\n                     Grounding. Soar can be instantiated in simulations (Tambe et al., 1995; Jones et al., 1999) or real-world\\n                     robotic systems (Laird et al., 2012). In embodied contexts, a variety of sensors stream perceptual input into\\n\\n                                                                                           4', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='b86b711b-f9dc-42f2-b217-9bc021d9eb08', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nworking memory, where it is available for decision-making. Soar agents can also be equipped with actuators,\\nallowing for physical actions and interactive learning via language (Mohan et al., 2012; Mohan and Laird,\\n2014; Kirk and Laird, 2014).\\nDecision making. Soar implements a decision loop that evaluates productions and applies the one that\\nmatches best (Fig. 2B). Productions are stored in long-term procedural memory. During each decision cycle,\\ntheir preconditions are checked against the agent’s working memory. In the proposal and evaluation phase,\\na set of productions is used to generate and rank a candidate set of possible actions.∗ The best action is\\nthen chosen.† Another set of productions is then used to implement the action – for example, modifying the\\ncontents of working memory or issuing a motor command.\\nLearning. Soar supports multiple modes of learning. First, new information can be stored directly in\\nlong-term memory: facts can be written to semantic memory, while experiences can be written to episodic\\nmemory (Derbinsky et al., 2012). This information can later be retrieved back into working memory when\\nneeded for decision-making. Second, behaviors can be modified. Reinforcement learning (Sutton and Barto,\\n2018) can be used to up-weight productions that have yielded good outcomes, allowing the agent to learn\\nfrom experience (Nason and Laird, 2005). Most remarkably, Soar is also capable of writing new productions\\ninto its procedural memory (Laird et al., 1986) – effectively updating its source code.\\nCognitive architectures were used broadly across psychology and computer science, with applications including\\nrobotics (Laird et al., 2012), military simulations (Jones et al., 1999; Tambe et al., 1995), and intelligent\\ntutoring (Koedinger et al., 1997). Yet they have become less popular in the AI community over the last few\\ndecades. This decrease in popularity reflects two of the challenges involved in such systems: they are limited\\nto domains that can be described by logical predicates and require many pre-specified rules to function.\\nIntriguingly, LLMs appear well-posed to meet these challenges. First, they operate over arbitrary text, making\\nthem more flexible than logic-based systems. Second, rather than requiring the user to specify productions,\\nthey learn a distribution over productions via pre-training on an internet corpus. Recognizing this, researchers\\nhave begun to use LLMs within cognitive architectures, leveraging their implicit world knowledge (Wray\\net al., 2021) to augment traditional symbolic approaches (Kirk et al., 2023; Romero et al., 2023). Here, we\\ninstead import principles from cognitive architecture to guide the design of LLM-based agents.\\n\\n2.4     Language models and agents\\n\\nLanguage modeling is a decades-old endeavor in the NLP and AI communities, aiming to develop systems\\nthat can generate text given some context (Jurafsky, 2000). Formally, language models learn a distribution\\nP (wi|w<i), where each w is an individual token (word). This model can then generate text by sampling from\\nthe distribution, one token at a time. At its core, a language model is a probabilistic input-output system,\\nsince there are inherently several ways to continue a text (e.g., “I went to the” → “market” | “beach” | ...).\\nWhile earlier attempts at modeling language (e.g., n-grams) faced challenges in generalization and scaling,\\nthere has been a recent resurgence of the area due to the rise of Transformer-based (Vaswani et al., 2017)\\nLLMs with a large number (billions) of parameters (e.g., GPT-4; OpenAI, 2023a) and smart tokenization\\nschemes. Modern LLMs are trained on enormous amounts of data, which helps them accumulate knowledge\\nfrom a large number of input-output combinations and successfully generate human-like text (Andreas, 2022).\\nUnexpectedly, training these models on internet-scale text also made them useful for many tasks beyond\\ngenerating text, such as writing code (Li et al., 2022b; Rozière et al., 2023; Li et al., 2023c), modeling\\nproteins (Meier et al., 2021), and acting in interactive environments (Yao et al., 2022b; Nakano et al., 2021).\\nThe latter has led to the rise of “language agents” – systems that use LLMs as a core computation unit to\\nreason, plan, and act – with applications in areas such as robotics (Ahn et al., 2022), manufacturing (Xia\\net al., 2023), web manipulation (Yao et al., 2022a; Deng et al., 2023), puzzle solving (Yao et al., 2023; Hao\\net al., 2023) and interactive code generation (Yang et al., 2023). The combination of language understanding\\n    ∗In more detail, Soar divides productions into two types: “operators,” which we refer to as actions, and “rules” which are\\nused to propose, evaluate, and execute operators.†If no actions are valid, or multiple actions tie, then an impasse occurs. Soar creates a subgoal to resolve the impasse,\\nresulting in hierarchical task decomposition. We refer the reader to Laird (2022) for a more detailed discussion.\\n\\n                                                                       5', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bf9032d7-61e0-4a84-aea4-f0128784aa82', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nand decision-making capabilities is an exciting and emerging direction that promises to bring these agents\\ncloser to human-like intelligence.\\n\\n3\\n      Connections between Language Models and Production Systems\\nBased on their common origins in processing strings, there is a natural analogy between production systems\\nand language models. We develop this analogy, then show that prompting methods recapitulate the algorithms\\nand agents based on production systems. The correspondence between production systems and language\\nmodels motivates our use of cognitive architectures to build language agents, which we introduce in Section 4.\\n\\n3.1     Language models as probabilistic production systems\\n\\nIn their original instantiation, production systems specified the set of strings that could be generated from a\\nstarting point, breaking this process down into a series of string rewriting operations. Language models also\\ndefine a possible set of expansions or modifications of a string – the prompt provided to the model.‡\\nFor example, we can formulate the problem of completing a piece of text as a production. If X is the prompt\\nand Y the continuation, then we can write this as the production X → X Y .§ We might want to allow\\nmultiple possible continuations, in which case we have X → X Yi for some set of Y. LLMs assign a probabilityi\\nto each of these completions. Viewed from this perspective, the LLM defines a probability distribution\\nover which productions to select when presented with input X, yielding a distribution P (Yi|X) over possible\\ncompletions (Dohan et al., 2022). LLMs can thus be viewed as probabilistic production systems that sample\\na possible completion each time they are called, e.g., X ∼∼▸ X Y .\\nThis probabilistic form offers both advantages and disadvantages compared to traditional production systems.\\nThe primary disadvantage of LLMs is their inherent opaqueness: while production systems are defined by\\ndiscrete and human-legible rules, LLMs consist of billions of uninterpretable parameters. This opaqueness –\\ncoupled with inherent randomness from their probabilistic formulation – makes it challenging to analyze or\\ncontrol their behaviors (Romero et al., 2023; Valmeekam et al., 2022). Nonetheless, their scale and pre-training\\nprovide massive advantages over traditional production systems. LLMs pre-trained on large-scale internet\\ndata learn a remarkably effective prior over string completions, allowing them to solve a wide range of tasks\\nout of the box (Huang et al., 2022b).\\n\\n3.2     Prompt engineering as control flow\\n\\nThe weights of an LLM define a prioritization over output strings (completions), conditioned by the input string\\n(the prompt). The resulting distribution can be interpreted as a task-specific prioritization of productions –\\nin other words, a simple control flow. Tasks such as question answering can be formulated directly as an\\ninput string (the question), yielding conditional distributions over completions (possible answers).\\nEarly work on few-shot learning (Brown et al., 2020) and prompt engineering (Wei et al., 2022b; Kojima\\net al., 2022; Xu et al., 2023c) found that the LLM could be further biased towards high-quality productions\\nby pre-processing the input string. These simple manipulations – typically concatenating additional text\\nto the input – can themselves be seen as productions, meaning that these methods define a sequence of\\nproductions (Table 1). Later work extended these approaches to dynamic, context-sensitive prompts: for\\nexample, selecting few-shot examples that are maximally relevant to the input (Liu et al., 2021) or populating\\na template with external observations from video (Zeng et al., 2022) or databases (Lewis et al., 2020). For a\\nsurvey of such prompting techniques, see Liu et al. (2023d).\\nSubsequent work used the LLM itself as a pre-processing step, eliciting targeted reasoning to foreground a\\nparticular aspect of the problem (Bai et al., 2022; Jin et al., 2022; Ganguli et al., 2023; Madaan et al., 2023;\\nSaunders et al., 2022; Kim et al., 2023; Kirk et al., 2023) or generate intermediate reasoning steps (Tafjord\\n    ‡In this work, we focus on autoregressive LLMs which are typically used for language agents. However, bidirectional LLMs\\nsuch as BERT (Devlin et al., 2019) can be seen in a similar light: they define a distribution over in-filling productions.\\n    §Alternatively, we can treat the prompt as input and take the output of the LLM as the next state, represented by the\\nproduction X → Y – a more literal form of rewriting.\\n\\n                                                                       6', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1bfbc227-3898-48f1-971d-9e9fac6cb229', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=' Published in Transactions on Machine Learning Research (02/2024)\\n\\n                                                    Prompting Method                                                                                                                    Production Sequence\\n                                                    Zero-shot                                                                                                                           QLLM▸Q A ∼∼∼∼\\n                                                    Few-shot                                                                                                                            Q −→ Q1 A1 Q2 A2 Q ∼∼∼∼LLM▸Q1 A1 Q2 A2 Q A\\n                                                    Retrieval Augmented Generation                                                                                                      Q −−        Wiki Q O ∼∼∼∼▸Q O A\\n                                                                                                                                                                                                     −→                                  LLM\\n                                                    Socratic Models                                                                                                                     QVLM▸Q O ∼∼∼∼ ∼∼∼∼                          LLM▸Q O A\\n                                                    Self-Critique                                                                                                                       QLLM▸Q A ∼∼∼∼▸Q A C ∼∼∼∼ ∼∼∼∼               LLM                      LLM▸Q A C A\\nTable 1: Conceptual diagram illustrating how prompting methods manipulate the input string before\\n generating completions. Q = question, A = answer, O = observation, C = critique, and ∼∼∼▸ denotes sampling\\n from a stochastic production. These pre-processing manipulations – which can employ other models such as\\nvision-language models (VLMs), or even the LLM itself – can be seen as productions. Prompting methods\\n thus define a sequence of productions.\\n                                                                                                                                                 Question                                                                                        Environment                   Human\\n                                                              Prompt\\n                                                        construction\\n\\n                                                                                                                   Answer                          Critique                   Reﬁnement                                 Answer                    VLM                     Act\\n                                                                 LLM                                                                                                                                                                                                                     Input\\n                                                                                                                                              Self-Critique                                                                                          Inner Monologue\\n                                                                                                                                                                                                                                                                                      LLM calls\\n                                                                                                                           Context                       Question                                                                                         Environment                 Chain /\\n                                                      String parsing                                                                                                                                                                                                                    Agent\\n                                                           Execution                                                       Selection                               Inference                          Answer                                   Reason                     Act\\n\\n                                                                                                                                      Selection-Inference                                                                                                          ReAct\\n\\n Figure 3: From language models to language agents. A: Basic structure of an LLM call. Prompt construction\\n selects a template and populates it with variables from working memory. After calling the LLM, the string\\n output is parsed into an action space and executed. An LLM call may result in one or more actions – for\\n example, returning an answer, calling a function, or issuing motor commands. B: Prompt chaining techniques\\n such as Self-Critique (Wang et al., 2022b) or Selection-Inference (Creswell et al., 2023) use a pre-defined\\n sequence of LLM calls to generate an output. C: Language agents such as Inner Monologue (Huang et al.,\\n2022c) and ReAct (Yao et al., 2022b) instead use an interactive feedback loop with the external environment.\\nVision-language models (VLMs) can be used to translate perceptual data into text for the LLM to process.\\n\\n et al., 2021; Creswell et al., 2023; Yao et al., 2023) before returning an answer. Chaining multiple calls to an\\n LLM (Wu et al., 2022a;b; Dohan et al., 2022) allows for increasingly complicated algorithms (Fig. 3).\\n\\n 3.3                   Towards cognitive language agents\\n\\nLanguage agents move beyond pre-defined prompt chains and instead place the LLM in a feedback loop with\\n the external environment (Fig. 1B). These approaches first transform multimodal input into text and pass it\\n to the LLM. The LLM’s output is then parsed and used to determine an external action (Fig. 3C). Early\\n agents interfaced the LLM directly with the external environment, using it to produce high-level instructions\\n based on the agent’s state (Ahn et al., 2022; Huang et al., 2022c; Dasgupta et al., 2022). Later work developed\\n more sophisticated language agents that use the LLM to perform intermediate reasoning before selecting\\n an action (Yao et al., 2022b). The most recent agents incorporate sophisticated learning strategies such as\\n reflecting on episodic memory to generate new semantic inferences (Shinn et al., 2023) or modifying their\\n\\n                                                                                                                                                                                                          7', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='979f6257-77cd-45cc-9863-d5a3d4a4128f', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=' Published in Transactions on Machine Learning Research (02/2024)\\n\\n                                  Procedural Memory                           Semantic Memory             Episodic Memory                                        Observation\\n                                 LLM              Agent Code                    Procedural MemoryN                 3                                     PlanningProposal                           Semantic Memory                             Episodic Memory\\n                        Prompt     Parse    Retrieval         Learning        Retrieval Learning         Retrieval    Learning\\n\\n                                                                                                                                                                  Evaluation\\n                                                                                                     Reasoning\\n\\n                          Decision Procedure                                                                Working Memory\\n                                                                                                                                                                   Selection\\n                                                               Actions       Observations\\n                                                  Dialogue            Physical LLMDigital                                                          Agent CodeExecution\\n Figure 4: Cognitive architectures for language agents (CoALA). A: CoALA defines a set of interacting\\n modules and processes. The decision procedure executes the agent’s source code. This source code consists\\n of procedures to interact with the LLM (prompt templates and parsers), internal memories (retrieval and\\n learning), and the external environment (grounding). B: Temporally, the agent’s decision procedure executes\\n a decision cycle in a loop with the external environment. During each cycle, the agent uses retrieval andPrompt       Retrieval                                              Learning              Retrieval        Learning                Retrieval                 Learning\\n reasoning to plan by proposing and evaluating candidate\\n is then selected and executed. An observation may be made, and the cycle begins again.Parse   learning   or   grounding   actions. The best action\\n\\n program code to generate procedural knowledge (Wang et al., 2023a), using their previous experience to\\n adapt their future behaviors.\\nThese cognitive language agents employ nontrivial LLM-based reasoning and learning (Fig. 1C). Just as\\n cognitive architectures were used to structure production systems’ interactions with agents’ internal state and\\n external environments, we suggest that they can help design LLM-based cognitive agents. In the remainder\\n of the paper, we use this perspective to organize existing approaches and highlight promising extensions.\\n\\n 4\\n        Cognitive Architectures for Language Agents (CoALA): A Conceptual Framework\\nWe present Cognitive Architectures for Language Agents (CoALA) as a framework to organize existing                                                                                                                                Reasoning\\n language agents and guide the development of new ones. CoALA positions the LLM as the core component\\n of a larger cognitive architecture (Figure 4). Under CoALA, a language agent stores information in memory\\n modules (Section 4.1), and acts in an action space structured into external and internal parts (Figure 5):\\n\\n          • External actions interact with external environments (e.g., control a robot, communicate with a\\n               human, navigate a website) through grounding (Section 4.2).\\n          • Internal actions interact with internal memories. Depending on which memory gets accessed and\\n               whether the access is read or write, internal actions can be further decomposed into three kinds:\\n               retrieval (read from long-term memory; Section 4.3), reasoning (update the short-term working ActionsDecision Procedure                                                                                                                 Working Memory\\n               memory with LLM; Section 4.4), and learning (write to long-term memory; Section 4.5).                                                                                              Observations\\n Language agents choose actions via decision-making, which follows a repeated cycle (Section 4.6, Figure 4B).\\n In each cycle, the agent can use reasoning and retrieval actions to plan. This planning subprocess selects a\\n grounding or learning action, which is executed to affect the outside world or the agent’s long-term memory.\\n CoALA’s decision cycle is analogous to a program’s “main” procedure (a method without return values, as\\n\\n                                                                                                 8\\n                                                                                                                                              Dialogue                                  Physical                         Digital', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='0c38e3ba-d24b-442f-814b-660cd0e35802', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=' Published in Transactions on Machine Learning Research (02/2024)\\n\\n                                                        Internal                          External\\n\\n                                       Reasoning         Retrieval      Learning        Grounding\\n\\n                                                Planning\\n\\n Figure 5: Agents’ action spaces can be divided into internal memory accesses and external interactions\\n with the world. Reasoning and retrieval actions are used to support planning.\\n\\n opposed to functions) that runs in loops continuously, accepting new perceptual input and calling various\\n action procedures in response.\\n CoALA (Figure 4) is inspired by the decades of research in cognitive architectures (Section 2.3), leveraging key\\n concepts such as memory, grounding, learning, and decision-making. Yet the incorporation of an LLM leads\\n to the addition of “reasoning” actions, which can flexibly produce new knowledge and heuristics for various\\n purposes – replacing hand-written rules in traditional cognitive architectures. It also makes text the de facto\\n internal representation, streamlining agents’ memory modules. Finally, recent advances in vision-language\\n models (VLMs; Alayrac et al., 2022) can simplify grounding by providing a straightforward translation of\\n perceptual data into text (Zeng et al., 2022).\\n The rest of this section details key concepts in CoALA: memory, actions (grounding, reasoning, retrieval,\\n and learning), and decision-making. For each concept, we use existing language agents (or relevant NLP/RL\\n methods) as examples – or note gaps in the literature for future directions.\\n\\n 4.1    Memory\\n\\n Language models are stateless: they do not persist information across calls. In contrast, language agents\\n may store and maintain information internally for multi-step interaction with the world. Under the CoALA\\n framework, language agents explicitly organize information (mainly textural, but other modalities also allowed)\\n into multiple memory modules, each containing a different form of information. These include short-term\\n working memory and several long-term memories: episodic, semantic, and procedural.\\n Working memory. Working memory maintains active and readily available information as symbolic variables\\n for the current decision cycle (Section 4.6). This includes perceptual inputs, active knowledge (generated by\\n reasoning or retrieved from long-term memory), and other core information carried over from the previous\\n decision cycle (e.g., agent’s active goals). Previous methods encourage the LLM to generate intermediate\\n reasoning (Wei et al., 2022b; Nye et al., 2021), using the LLM’s own context as a form of working memory.\\n CoALA’s notion of working memory is more general: it is a data structure that persists across LLM calls.\\n On each LLM call, the LLM input is synthesized from a subset of working memory (e.g., a prompt template\\n and relevant variables). The LLM output is then parsed back into other variables (e.g., an action name\\n and arguments) which are stored back in working memory and used to execute the corresponding action\\n(Figure 3A). Besides the LLM, the working memory also interacts with long-term memories and grounding\\n interfaces. It thus serves as the central hub connecting different components of a language agent.\\n Episodic memory. Episodic memory stores experience from earlier decision cycles. This can consist of\\n training input-output pairs (Rubin et al., 2021), history event flows (Weston et al., 2014; Park et al., 2023),\\n game trajectories from previous episodes (Yao et al., 2020; Tuyls et al., 2022), or other representations of\\n the agent’s experiences. During the planning stage of a decision cycle, these episodes may be retrieved into\\n working memory to support reasoning. An agent can also write new experiences from working to episodic\\n memory as a form of learning (Section 4.5).\\n Semantic memory. Semantic memory stores an agent’s knowledge about the world and itself. Traditional\\n NLP or RL approaches that leverage retrieval for reasoning or decision-making initialize semantic memory\\n from an external database for knowledge support. For example, retrieval-augmented methods in NLP (Lewis\\n et al., 2020; Borgeaud et al., 2022; Chen et al., 2017) can be viewed as retrieving from a semantic memory of\\n\\n                                                                      9', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='bbdd4752-2f0b-4cf9-a940-cefb85857f18', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nunstructured text (e.g., Wikipedia). In RL, “reading to learn” approaches (Branavan et al., 2012; Narasimhan\\net al., 2018; Hanjie et al., 2021; Zhong et al., 2021) leverage game manuals and facts as a semantic memory\\nto affect the policy. While these examples essentially employ a fixed, read-only semantic memory, language\\nagents may also write new knowledge obtained from LLM reasoning into semantic memory as a form of\\nlearning (Section 4.5) to incrementally build up world knowledge from experience.\\nProcedural memory. Language agents contain two forms of procedural memory: implicit knowledge stored\\nin the LLM weights, and explicit knowledge written in the agent’s code. The agent’s code can be further\\ndivided into two types: procedures that implement actions (reasoning, retrieval, grounding, and learning\\nprocedures), and procedures that implement decision-making itself (Section 4.6). During a decision cycle, the\\nLLM can be accessed via reasoning actions, and various code-based procedures can be retrieved and executed.\\nUnlike episodic or semantic memory that may be initially empty or even absent, procedural memory must be\\ninitialized by the designer with proper code to bootstrap the agent. Finally, while learning new actions by\\nwriting to procedural memory is possible (Section 4.5), it is significantly riskier than writing to episodic or\\nsemantic memory, as it can easily introduce bugs or allow an agent to subvert its designers’ intentions.\\n\\n4.2     Grounding actions\\n\\nGrounding procedures execute external actions and process environmental feedback into working memory as\\ntext. This effectively simplifies the agent’s interaction with the outside world as a “text game” with textual\\nobservations and actions. We categorize three kinds of external environments:\\nPhysical environments. Physical embodiment is the oldest instantiation envisioned for AI agents (Nilsson,\\n1984). It involves processing perceptual inputs (visual, audio, tactile) into textual observations (e.g., via\\npre-trained captioning models), and affecting the physical environments via robotic planners that take\\nlanguage-based commands. Recent advances in LLMs have led to numerous robotic projects (Ahn et al., 2022;\\nLiang et al., 2023a; Singh et al., 2023; Palo et al., 2023; Ren et al., 2023) that leverage LLMs as a “brain”\\nfor robots to generate actions or plans in the physical world. For perceptual input, vision-language models\\nare typically used to convert images to text (Alayrac et al., 2022; Sumers et al., 2023) providing additional\\ncontext for the LLM (Driess et al., 2023; Huang et al., 2023; Brohan et al., 2022; 2023).\\nDialogue with humans or other agents. Classic linguistic interactions allow the agent to accept\\ninstructions (Winograd, 1972; Tellex et al., 2011; Chen and Mooney, 2011; Bisk et al., 2016) or learn from\\npeople (Nguyen et al., 2021; Sumers et al., 2022; 2021; Wang et al., 2016). Agents capable of generating\\nlanguage may ask for help (Ren et al., 2023; Nguyen et al., 2022b; 2019; Nguyen and Daumé III, 2019) or\\nclarification (Biyik and Palan, 2019; Sadigh et al., 2017; Padmakumar et al., 2022; Thomason et al., 2020;\\nNarayan-Chen et al., 2019) – or entertain or emotionally help people (Zhang et al., 2020; Zhou et al., 2018;\\nPataranutaporn et al., 2021; Hasan et al., 2023; Ma et al., 2023). Recent work also investigates interaction\\namong multiple language agents for social simulation (Park et al., 2023; Jinxin et al., 2023; Gao et al., 2023),\\ndebate (Chan et al., 2023; Liang et al., 2023b; Du et al., 2023), improved safety (Irving et al., 2018), or\\ncollabrative task solving (Qian et al., 2023; Wu et al., 2023; Hong et al., 2023a; Dong et al., 2023).\\nDigital environments. This includes interacting with games (Hausknecht et al., 2020; Côté et al., 2019;\\nShridhar et al., 2020; Wang et al., 2022a; Liu et al., 2023e), APIs (Schick et al., 2023; Yao et al., 2022b; Parisi\\net al., 2022; Tang et al., 2023b), and websites (Shi et al., 2017; Nakano et al., 2021; Yao et al., 2022a; Zhou\\net al., 2023b; Gur et al., 2023; Deng et al., 2023) as well as general code execution (Yang et al., 2023; Le\\net al., 2022; Ni et al., 2023). Such digital grounding is cheaper and faster than physical or human interaction.\\nIt is thus a convenient testbed for language agents and has been studied with increasing intensity in recent\\nyears. In particular, for NLP tasks that require augmentation of external knowledge or computation, stateless\\ndigital APIs (e.g., search, calculator, translator) are often packaged as “tools” (Parisi et al., 2022; Schick\\net al., 2023; Xu et al., 2023a; Tang et al., 2023b; Qin et al., 2023), which can be viewed as special “single-use”\\ndigital environments.\\n\\n                                                                      10', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='a97b9657-b1e7-4d8a-b83a-2859169e84c0', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=' Published in Transactions on Machine Learning Research (02/2024)\\n\\n 4.3    Retrieval actions\\n\\n In CoALA, a retrieval procedure (Li et al., 2022a; Gu et al., 2018) reads information from long-term memories\\n into working memory. Depending on the information and memory type, it could be implemented in various\\n ways, e.g., rule-based, sparse, or dense retrieval. For example, Voyager (Wang et al., 2023a) loads code-based\\n skills from a skill library via dense retrieval to interact with the Minecraft world – effectively retrieving\\n grounding procedures from a procedural memory. Generative Agents (Park et al., 2023) retrieves relevant\\n events from episodic memory via a combination of recency (rule-based), importance (reasoning-based),\\n and relevance (embedding-based) scores. DocPrompting (Zhou et al., 2022a) proposes to leverage library\\n documents to assist code generation, which can be seen as retrieving knowledge from semantic memory.\\n While retrieval plays a key role in human decision-making (Zhou et al., 2023a; Zhao et al., 2022), adaptive\\n and context-specific recall remains understudied in language agents. In Section 6, we suggest a principled\\n integration of decision-making and retrieval as an important future direction.\\n\\n 4.4    Reasoning actions\\n\\n Reasoning allows language agents to process the contents of working memory to generate new information.\\n Unlike retrieval (which reads from long-term memory into working memory), reasoning reads from and\\n writes to working memory. This allows the agent to summarize and distill insights about the most recent\\n observation (Yao et al., 2022b; Peng et al., 2023), the most recent trajectory (Shinn et al., 2023), or\\n information retrieved from long-term memory (Park et al., 2023). Reasoning can be used to support learning\\n(by writing the results into long-term memory) or decision-making (by using the results as additional context\\n for subsequent LLM calls).\\n\\n 4.5    Learning actions\\n\\n Learning occurs by writing information to long-term memory, which includes a spectrum of diverse procedures.\\n Updating episodic memory with experience. It is common practice for RL agents to store episodic\\n trajectories to update a parametric policy (Blundell et al., 2016; Pritzel et al., 2017) or establish a non-\\n parametric policy (Ecoffet et al., 2019; Tuyls et al., 2022). For language agents, added experiences in episodic\\n memory may be retrieved later as examples and bases for reasoning or decision-making (Weston et al., 2014;\\n Rubin et al., 2021; Park et al., 2023).\\n Updating semantic memory with knowledge. Recent work (Shinn et al., 2023; Park et al., 2023) has\\n applied LLMs to reason about raw experiences and store the resulting inferences in semantic memory. For\\n example, Reflexion (Shinn et al., 2023) uses an LLM to reflect on failed episodes and stores the results (e.g.,\\n“there is no dishwasher in kitchen”) as semantic knowledge to be attached to LLM context for solving later\\n episodes. Finally, work in robotics (Chen et al., 2023a) uses vision-language models to build a semantic map\\n of the environment, which can later be queried to execute instructions.\\n Updating LLM parameters (procedural memory). The LLM weights represent implicit procedural\\n knowledge. These can be adjusted to an agent’s domain by fine-tuning during the agent’s lifetime. Such fine-\\n tuning can be accomplished via supervised (Liu et al., 2023c; Zhang et al., 2023b) or imitation learning (Hussein\\n et al., 2017), reinforcement learning (RL) from environment feedback (Sutton and Barto, 2018), human\\n feedback (RLHF; Christiano et al., 2017; Ouyang et al., 2022; Nakano et al., 2021), or AI feedback (Bai et al.,\\n 2022; Liu et al., 2023f). Classic LLM self-improvement methods (Huang et al., 2022a; Zelikman et al., 2022)\\n use an external measure such as consistency Wang et al. (2022b) to select generations to fine-tune on. In\\n reinforcement learning settings, this can be extended to use environmental feedback instead: for example,\\n XTX (Tuyls et al., 2022) periodically fine-tunes a small language model on high-scoring trajectories stored in\\n episodic memory, which serves as a robust “exploitation” policy to reach exploration frontiers in the face of\\n stochasity. Fine-tuning the agent’s LLM is a costly form of learning; thus, present studies specify learning\\n schedules. However, as training becomes more efficient – or if agents utilize smaller subtask-specific LLMs – it\\n may be possible to allow language agents to autonomously determine when and how to fine-tune their LLMs.\\n\\n                                                                     11', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='1d13315e-9369-4f5b-8875-3df3a0ebae19', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nUpdating agent code (procedural memory). CoALA allows agents to update their source code, thus\\nmodifying the implementation of various procedures. These can be broken down as follows:\\n\\n       • Updating reasoning (e.g., prompt templates; Gao et al., 2020; Zhou et al., 2022b). For example,\\n           APE (Zhou et al., 2022b) infers prompt instructions from input-output examples, then uses these\\n           instructions as part of the LLM prompt to assist task solving. Such a prompt update can be seen as\\n           a form of learning to reason.\\n       • Updating grounding (e.g., code-based skills; Liang et al., 2023a; Ellis et al., 2021; Wang et al.,\\n           2023a). For example, Voyager (Wang et al., 2023a) maintains a curriculum library. Notably, current\\n           methods are limited to creating new code skills to interact with external environments.\\n       • Updating retrieval. To our knowledge, these learning options are not studied in recent language\\n           agents. Retrieval is usually considered a basic action designed with some fixed implementation\\n          (e.g., BM25 or dense retrieval), but research in query/document expansion (Nogueira et al., 2019;\\n           Wang et al., 2023c; Tang et al., 2023a) or retrieval distillion (Izacard et al., 2021) may be helpful for\\n           language agents to learn better retrieval procedures.\\n       • Updating learning or decision-making. Finally, it is theoretically possible for CoALA agents\\n           to learn new procedures for learning or decision-making, thus providing significant adaptability.\\n           In general, however, updates to these procedures are risky both for the agent’s functionality and\\n           alignment. At present, we are not aware of any language agents that implement this form of learning;\\n           we discuss such possibilities more in Section 6.\\n\\nWhile RL agents usually fix one way of learning (e.g., Q-learning, PPO, or A3C) and learn by updating model\\nparameters, language agents can select from a diversity of learning procedures. This allows them to learn\\nrapidly by storing task-relevant language (cheaper and quicker than parameter updates), and leverage multiple\\nforms of learning to compound their self-improvement (e.g., Generative Agents discussed in Section 5).\\nFinally, while our discussion has mostly focused on adding to memory, modifying and deleting (a case of\\n“unlearning”) are understudied in recent language agents. We address these areas more in Section 6.\\n\\n4.6    Decision making\\n\\nWith various actions (grounding, learning, reasoning, retrieval) in the action space, how should a language\\nagent choose which action to apply? This is handled by the decision-making procedure, which is effectively the\\ntop-level or “main” agent program. CoALA structures this top-level program into decision cycles (Figure 4B)\\nwhich yield an external grounding action (Section 4.2) or internal learning action (Section 4.5). In each\\ncycle, program code defines a sequence of reasoning and retrieval actions to propose and evaluate alternatives\\n(planning stage), then executes the selected action (execution stage) – then the cycle loops again.\\nPlanning stage. During planning, reasoning and retrieval can be flexibly applied to propose, evaluate, and\\nselect actions, and these sub-stages could interleave or iterate to build up multi-step simulations (Tamari\\net al., 2020) before taking an external action (Yao et al., 2023; Hao et al., 2023). It also enables agents to\\niteratively improve candidate solutions – for example, by using the LLM to simulate them, identifying defects,\\nand proposing modifications that address those defects (Kirk et al., 2023; Shinn et al., 2023).\\n\\n       • Proposal. The proposal sub-stage generates one or more action candidates. The usual approach\\n           is to use reasoning (and optionally retrieval) to sample one (Huang et al., 2022c) or more (Chen\\n           et al., 2021; Wang et al., 2022b) external grounding actions from the LLM. For simple domains with\\n           limited actions, the proposal stage might simply include all actions (e.g., SayCan in Section 5). More\\n           sophisticated agents use if-else or while-if code structures (Wang et al., 2023a; Park et al., 2023);\\n           while agents deployed in well-defined domains may utilize structured simulators (Haslum et al., 2019)\\n           to generate plausible rollouts (Liu et al., 2023a; Dagan et al., 2023).\\n       • Evaluation. If multiple actions are proposed, the evaluation sub-stage assigns a value to each.\\n           This may use heuristic rules, LLM (perplexity) values (Ahn et al., 2022), learned values (Yao et al.,\\n\\n                                                                     12', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='de6e2720-09fa-4881-993b-e0a82b2cc672', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n                                            Long-term               External          Internal\\n                                            Memory¶                 Grounding         Actions\\n SayCan (Ahn et al., 2022)                  -                       physical          -\\n ReAct (Yao et al., 2022b)                  -                       digital           reason\\n Voyager (Wang et al., 2023a)               procedural              digital           reason/retrieve/learn\\n Generative Agents (Park et al., 2023)      episodic/semantic       digital/agent     reason/retrieve/learn\\n Tree of Thoughts (Yao et al., 2023)        -                       digital‖          reason\\n\\nDecision\\nMaking\\nevaluate\\npropose\\npropose\\npropose\\npropose, evaluate, select\\n\\n                                              Table 2: Some recent language agents cast into the CoALA framework.\\n                                2020), LLM reasoning (Yao et al., 2023; Hao et al., 2023), or some combination. Particularly, LLM\\n                                reasoning can help evaluate actions by internally simulating their grounding feedback from the\\n                                external world (Hao et al., 2023; Yang et al., 2023).\\n                             • Selection. Given a set of actions and their values, the selection step either selects one to execute or\\n                                rejects them and loops back to the proposal step. Depending on the form of action values, selection\\n                                may occur via argmax, softmax, or an alternative such as majority vote (Wang et al., 2022b).\\n\\n                      Execution. The selected action is applied by executing the relevant procedures from the agent’s source\\n                      code. Depending on the agent implementation, this might be an external grounding action (e.g., an API call;\\n                      Section 4.2) or an internal learning action (e.g., a write to episodic memory; Section 4.5). An observation can\\n                      be made from the environment, providing feedback from the agent’s action, and the cycle loops again.\\n                      Empirically, many early language agents simply use LLMs to propose an action (Schick et al., 2023), a\\n                      sequence of actions (Huang et al., 2022b), or evaluate a fixed set of actions (Ahn et al., 2022) without\\n                      intermediate reasoning or retrieval. Followup work (Yao et al., 2022b; Shinn et al., 2023; Xu et al., 2023b;\\n                      Lin et al., 2023; Wang et al., 2023a; Park et al., 2023) has exploited intermediate reasoning and retrieval to\\n                      analyze the situation, make and maintain action plans, refine the previous action given the environmental\\n                      feedback, and leveraged a more complex procedure to propose a single action. Most recently, research has\\n                      started to investigate more complex decision-making employing iterative proposal and evaluation to consider\\n                      multiple actions. These procedures are modeled after classical planning algorithms: for example, Tree of\\n                     Thoughts (Yao et al., 2023) and RAP (Hao et al., 2023) use LLMs to implement BFS/DFS and Monte Carlo\\n                     Tree Search (MCTS; Browne et al., 2012) respectively. LLMs are used to generate proposals (i.e., to simulate\\n                      rollouts conditioned on an action) and evaluate them (i.e., to value the outcome of the proposed action).\\n\\n                      5\\n                           Case Studies\\n                     With variations and ablations of the memory modules, action space, and decision-making procedures, CoALA\\n                      can express a wide spectrum of language agents. Table 2 lists some popular recent methods across diverse\\n                      domains — from Minecraft to robotics, from pure reasoning to social simulacra. CoALA helps characterize\\n                      their internal mechanisms and reveal their similarities and differences in a simple and structured way.\\n                      SayCan (Ahn et al., 2022) grounds a language model to robotic interactions in a kitchen to satisfy user\\n                      commands (e.g., “I just worked out, can you bring me a drink and a snack to recover?”). Its long-term\\n                      memory is procedural only (an LLM and a learned value function). The action space is external only – a fixed\\n                      set of 551 grounding skills (e.g., “find the apple”, “go to the table”), with no internal actions of reasoning,\\n                      retrieval, or learning. During decision-making, SayCan evaluates each action using a combination of LLM\\n                      and learned values, which balance a skill’s usefulness and groundedness. SayCan therefore employs the LLM\\n                     (in conjunction with the learned value function) as a single-step planner.\\n                      ReAct (Yao et al., 2022b) is a language agent grounded to various digital environments (e.g., Wikipedia API,\\n                      text game, website). Like SayCan, it lacks semantic or episodic memory and therefore has no retrieval or\\n                      learning actions. Its action space consists of (internal) reasoning and (external) grounding. Its decision cycle\\n                      is fixed to use a single reasoning action to analyze the situation and (re)make action plans, then generates a\\n                         ¶All agents contain some procedural memory (agent code and LLM weights), so here we only list writable procedural memory.\\n                         ‖Special digital grounding with the only external action being submitting a final answer.\\n\\n                                                                                            13', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c5aae101-6e30-4795-a480-fc45013c3e79', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\ngrounding action without evaluation or selection stages. ReAct can be considered the simplest language agent\\nthat leverages both internal and external actions, and is the initial work that demonstrates their synergizing\\neffects: reasoning helps guide acting, while acting provides environmental feedback to support reasoning.\\nVoyager (Wang et al., 2023a) is a language agent grounded to the Minecraft API. Unlike SayCan, which\\ngrounds to perception via the learned value function, Voyager’s grounding is text-only. It has a long-term\\nprocedural memory that stores a library of code-based grounding procedures a.k.a. skills (e.g., “combatZombie”,\\n“craftStoneSword”). This library is hierarchical: complex skills can use simpler skills as sub-procedures\\n(e.g., “combatZombie” may call “craftStoneSword” if no sword is in inventory). Most impressively, its action\\nspace has all four kinds of actions: grounding, reasoning, retrieval, and learning (by adding new grounding\\nprocedures). During a decision cycle, Voyager first reasons to propose a new task objective if it is missing\\nin the working memory, then reasons to propose a code-based grounding procedure to solve the task. In\\nthe next decision cycle, Voyager reasons over the environmental feedback to determine task completion. If\\nsuccessful, Voyager selects a learning action adding the grounding procedure to procedural memory; otherwise,\\nit uses reasoning to refine the code and re-executes it. The importance of long-term memory and procedural\\nlearning is empirically verified by comparing to baselines like ReAct and AutoGPT and ablations without the\\nprocedural memory. Voyager is shown to better explore areas, master the tech tree, and zero-shot generalize\\nto unseen tasks.\\nGenerative Agents (Park et al., 2023) are language agents grounded to a sandbox game affording interaction\\nwith the environment and other agents. Its action space also has all four kinds of actions: grounding, reasoning,\\nretrieval, and learning. Each agent has a long-term episodic memory that stores events in a list. These agents\\nuse retrieval and reasoning to generate reflections on their episodic memory (e.g., “I like to ski now.”) which\\nare then written to long-term semantic memory. During decision-making, it retrieves relevant reflections from\\nsemantic memory, then reasons to make a high-level plan of the day. While executing the plan, the agent\\nreceives a stream of grounding observations; it can reason over these to maintain or adjust the plan.\\nTree of Thoughts (ToT) (Yao et al., 2023) can be seen as a special kind of language agent with only one\\nexternal action: submitting a final solution to a reasoning problem (game of 24, creative writing, crosswords\\npuzzle). It has no long-term memory, and only reasoning in its internal action space, but differs from all\\nprevious agents in its deliberate decision-making. During planning, ToT iteratively proposes, evaluates,\\nand selects “thoughts” (reasoning actions) based on LLM reasoning, and maintains them via a tree search\\nalgorithm to enable global exploration as well as local backtrack and foresight.\\n\\n6\\n      Actionable Insights\\nCompared to some recent empirical surveys around language agents (Mialon et al., 2023; Weng, 2023; Wang\\net al., 2023b), CoALA offers a theoretical framework grounded in the well-established research of cognitive\\narchitectures. This leads to a unique and complementary set of actionable insights.\\nModular agents: thinking beyond monoliths. Perhaps our most important suggestion is that agents\\nshould be structured and modular. Practically, just as standardized software is used across robotics plat-\\nforms (Quigley, 2009; Macenski et al., 2022), a framework for language agents would consolidate technical\\ninvestment and improve compatibility.\\n\\n       • In academic research, standardized terms allow conceptual comparisons across works (Table 2), and\\n           open-source implementations would further facilitate modular plug-and-play and re-use. For example,\\n           the theoretical framework of Markov Decision Processes (Puterman, 2014) provides a standardized set\\n           of concepts and terminology (e.g., state, action, reward, transition) for reinforcement learning (Sutton\\n           and Barto, 2018). Correspondingly, empirical frameworks like OpenAI Gym (Brockman et al., 2016)\\n           provided standardized abstractions (e.g., obs, reward, done, info = env.step(action)) that\\n           facilitate empirical RL work. Thus, it would be timely and impactful to also implement useful\\n           abstractions (e.g., Memory, Action, Agent classes) for language agents, and cast simpler agents into\\n           such an empirical CoALA framework as examples for building more complex agents.\\n\\n                                                                     14', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='01c3a8ea-95d4-4342-9c35-20f56b39bf79', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\n       • In industry applications, maintaining a single company-wide “language agent library” would reduce\\n           technical debt (Sculley et al., 2014; Lwakatare et al., 2020) by facilitating testing and component\\n           re-use across individual agent deployments. It could also standardize the customer experience: rather\\n           than interacting with a hodgepodge of language agents developed by individual teams, end users\\n           would experience a context-specific instantiation of the same base agent.\\n       • LLMs vs. code in agent design. CoALA agents possess two forms of procedural memory: agent\\n           code (deterministic rules) and LLM parameters (a large, stochastic production system). Agent code is\\n           interpretable and extensible, but often brittle in face of stochasticity and limited to address situations\\n           the designer anticipates. In contrast, LLM parameters are hard to interpret, but offer significant\\n           zero-shot flexibility in new contexts (Huang et al., 2022b). CoALA thus suggests using code sparingly\\n           to implement generic algorithms that complement LLM limitations, e.g., implementing tree search to\\n           mitigate myopia induced by autoregressive generation (Yao et al., 2023; Hao et al., 2023).\\n\\nAgent design: thinking beyond simple reasoning. CoALA defines agents over three distinct concepts: (i)\\ninternal memory, (ii) a set of possible internal and external actions, and (iii) a decision making procedure over\\nthose actions. Using CoALA to develop an application-specific agent consists of specifying implementations\\nfor each of these components in turn. We assume that the agent’s environment and external action space are\\ngiven, and show how CoALA can be used to determine an appropriate high-level architecture. For example,\\nwe can imagine designing a personalized retail assistant (Yao et al., 2022a) that helps users find relevant items\\nbased on their queries and purchasing history. In this case, the external actions would consist of dialogue or\\nreturning search results to the user.\\n\\n       • Determine what memory modules are necessary. In our retail assistant example, it would be\\n           helpful for the agent to have semantic memory containing the set of items for sale, as well as episodic\\n           memory about each customer’s previous purchases and interactions. It will need procedural memory\\n           defining functions to query these datastores, as well as working memory to track the dialogue state.\\n       • Define the agent’s internal action space. This consists primarily of defining read and write\\n           access to each of the agent’s memory modules. In our example, the agent should have read and write\\n           access to episodic memory (so it can store new interactions with customers), but read-only access to\\n           semantic and procedural memory (since it should not update the inventory or its own code).\\n       • Define the decision-making procedure. This step specifies how reasoning and retrieval actions\\n           are taken in order to choose an external or learning action. In general, this requires a tradeoff between\\n           performance and generalization: more complex procedures can better fit to a particular problem\\n          (e.g., Voyager (Wang et al., 2023a) for Minecraft) while simpler ones are more domain-agnostic and\\n           generalizable (e.g., ReAct (Yao et al., 2022b)). For our retail assistant, we may want to encourage\\n           retrieval of episodic memory of interactions with a user to provide a prior over their search intent,\\n           as well as an explicit evaluation step reasoning about whether a particular set of search results will\\n           satisfy that intent. We can simplify the decision procedure by deferring learning to the end of the\\n           interaction (Shinn et al., 2023; Park et al., 2023), summarizing the episode prior to storing it in\\n           episodic memory.\\n\\nStructured reasoning: thinking beyond prompt engineering. Early work on prompt engineering\\nmanipulated the LLM’s input and output via low-level string operations. CoALA suggests a more structured\\nreasoning procedure to update working memory variables.\\n\\n       • Prompting frameworks like LangChain (LangChain, 2022) and LlamaIndex (LlamaIndex, 2023)\\n           can be used to define higher-level sequences of reasoning steps, reducing the burden of reasoning per\\n           LLM call and the low-level prompt crafting efforts. Structural output parsing solutions such as\\n           Guidance (Guidance, 2023) and OpenAI function calling (OpenAI, 2023b) can help update working\\n           memory variables. Defining and building good working memory modules will also be an important\\n           direction of future research. Such modules may be especially important for industry solutions where\\n           LLM reasoning needs to seamlessly integrate with large-scale code infrastructure.\\n\\n                                                                    15', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='8d818b91-8a88-4c51-8e65-c648f0e37304', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\n       • Reasoning usecases in agents can inform and reshape LLM training in terms of the types\\n          (e.g., reasoning for self-evaluation, reflection, action generation, etc.) and formats (e.g., CoT (Wei\\n          et al., 2022b), ReAct (Yao et al., 2022b), Reflexion (Shinn et al., 2023)) of training instances. By\\n          default, existing LLMs are trained and optimized for NLP tasks, but agent applications have explored\\n          new modes of LLM reasoning (e.g., self-evaluation) that have proven broadly useful. LLMs trained\\n          or finetuned towards these capabilities will more likely be the backbones of future agents.\\n\\nLong-term memory: thinking beyond retrieval augmentation. While traditional retrieval-augmented\\nlanguage models (Guu et al., 2020; Lewis et al., 2020; Borgeaud et al., 2022) only read from human-written\\ncorpora, memory-augmented language agents can both read and write self-generated content autonomously.\\nThis opens up numerous possibilities for efficient lifelong learning.\\n\\n       • Combining existing human knowledge with new experience and skills can help agents\\n          bootstrap to learn efficiently. For example, a code-writing agent could be endowed with semantic\\n          programming knowledge in the form of manuals or textbooks. It could then generate its own episodic\\n          knowledge from experience; reflect on these experiences to generate new semantic knowledge; and\\n          gradually create procedural knowledge in the form of a code library storing useful methods.\\n\\n       • Integrating retrieval and reasoning can help to better ground planning. Recent computational\\n          psychological models implicate an integrated process of memory recall and decision-making (Zhou\\n          et al., 2023a; Zhao et al., 2022) – suggesting that adaptive mechanisms interleaving memory search\\n          and forward simulation will allow agents to make the most of their knowledge.\\n\\nLearning: thinking beyond in-context learning or finetuning. CoALA’s definition of “learning”\\nencompasses these methods, but extends further to storing new experience or knowledge, or writing new\\nagent code (Section 4.5). Important future directions include:\\n\\n       • Meta-learning by modifying agent code would allow agents to learn more effectively. For example,\\n          learning better retrieval procedures could enable agents to make better use of their experience. Recent\\n          expansion-based techniques (Nogueira et al., 2019; Wang et al., 2023c; Tang et al., 2023a) could\\n          allow agents to reason about when certain knowledge would be useful, and store this as metadata to\\n          facilitate later recall. These forms of meta-learning would enable agents to go beyond human-written\\n          code, yet are understudied due to their difficulty and risk.\\n       • New forms of learning (and unlearning) could include fine-tuning smaller models for specific\\n          reasoning sub-tasks (Zelikman et al., 2022; Huang et al., 2022a; Ahn et al., 2022), deleting unneeded\\n          memory items for “unlearning” (Nguyen et al., 2022c), and studying the interaction effects between\\n          multiple forms of learning (Tuyls et al., 2022; Park et al., 2023; Xie et al., 2023; Khattab et al., 2022).\\n\\nAction space: thinking beyond external tools or actions. Although “action space” is a standard term\\nin reinforcement learning, it has been used sparingly with language agents. CoALA argues for defining a clear\\nand task-suitable action space with both internal (reasoning, retrieval, learning) and external (grounding)\\nactions, which will help systematize and inform the agent design.\\n\\n       • Size of the action space. More capable agents (e.g., Voyager, Generative Agents) have larger\\n          action spaces – which in turn means they face a more complex decision-making problem. As a result,\\n          these agents rely on more customized or hand-crafted decision procedures. The tradeoff of the action\\n          space vs. decision-making complexities is a basic problem to be considered before agent development,\\n          and taking the minimal action space necessary to solve a given task might be preferred.\\n       • Safety of the action space. Some parts of the action space are inherently riskier. “Learning”\\n          actions (especially procedural deletion and modification) could cause internal harm, while “grounding”\\n          actions (e.g., “rm” in bash terminal, harmful speech in human dialog, holding a knife in physical\\n          environments) could cause external harm. Today, safety measures are typically task-specific heuristics\\n\\n                                                                    16', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='c5789ac5-1f13-495f-a3b0-79b6b757a69a', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\n          (e.g., remove “os” operations in Python (Chen et al., 2021), filter keywords in dialog (Chowdhery\\n           et al., 2022; Driess et al., 2023), limit robots to controlled environments (Ahn et al., 2022)). However,\\n           as agents are grounded to more complex environments with richer internal mechanisms, it may\\n           be necessary to specify and ablate the agent’s action space for worst-case scenario prediction and\\n           prevention (Yao and Narasimhan, 2023).\\n\\nDecision making: thinking beyond action generation. We believe one of the most exciting future\\ndirections for language agents is decision-making: as detailed in Section 4.6, most works are still confined to\\nproposing (or directly generating) a single action. Present agents have just scratched the surface of more\\ndeliberate, propose-evaluate-select decision-making procedures.\\n\\n       • Mixing language-based reasoning and code-based planning may offer the best of both worlds.\\n           Existing approaches either plan directly in natural language (Huang et al., 2022c; Ahn et al., 2022)\\n           or use LLMs to translate from natural language to structured world models (Wong et al., 2023; Liu\\n           et al., 2023a; Zhang et al., 2023a; Li et al., 2023a; Guan et al., 2023; Silver et al., 2022; 2023). Future\\n           work could integrate these: just as Soar incorporates a simulator for physical reasoning (Laird, 2022),\\n           agents may write and execute simulation code on the fly to evaluate the consequences of plans. See\\n           Section 7 for more discussion.\\n\\n       • Extending deliberative reasoning to real-world settings. Initial works have implemented\\n           classical planning and tree search (Yao et al., 2023; Hao et al., 2023; Liu et al., 2023a; Dagan et al.,\\n           2023), using toy tasks such as game of 24 or block building. Extending these schemes to more\\n           complicated tasks with grounding (Qin et al., 2023) and long-term memory is an exciting direction.\\n\\n       • Metareasoning to improve efficiency. LLM calls are both slow and computationally intensive.\\n           Using LLMs for decision-making entails a balance between their computational cost and the utility of\\n           the resulting improved plan. Most LLM reasoning methods fix a search budget by specifying a depth\\n           of reasoning (Yao et al., 2023), but humans appear to adaptively allocate computation (Russek et al.,\\n           2022; Lieder and Griffiths, 2020; Callaway et al., 2022; Gershman et al., 2015). Future work should\\n           develop mechanisms to estimate the utility of planning (Laidlaw et al., 2023) and modify the decision\\n           procedure accordingly, either via amortization (fine-tuning the LLM based on the results of previous\\n           actions, e.g. Nguyen, 2023; Hamrick et al., 2019), routing among several decision sub-procedures\\n          (e.g., ReAct (Yao et al., 2022b) investigated backing off to CoT (Wei et al., 2022b) and vice versa),\\n           or updates to the decision-making procedure.\\n\\n       • Calibration and alignment. More complex decision-making is currently bottlenecked by issues\\n           such as over-confidence and miscalibration (Jiang et al., 2021; Braverman et al., 2020; Chen et al.,\\n           2022), misalignment with human values or bias (Liang et al., 2021; Feng et al., 2023), hallucinations\\n           in self-evaluation (Shinn et al., 2023), and lack of human-in-the-loop mechanisms in face of uncer-\\n           tainties (Nguyen et al., 2022a; Ren et al., 2023). Solving these issues will significantly improve LLMs’\\n           utilities as agent backbones.\\n\\n7\\n      Discussion\\nIn addition to the practical insights presented above, CoALA raises a number of open conceptual questions.\\nWe briefly highlight the most interesting as important directions for future research and debate.\\nLLMs vs VLMs: should reasoning be language-only or multimodal? Most language agents use\\nlanguage-only models for decision-making (Yao et al., 2022b; Wang et al., 2023a; Yao et al., 2023), employing\\na separate captioning model to convert environment observations to text when necessary (Ahn et al., 2022;\\nZeng et al., 2022). However, the latest generation of language models are multimodal, allowing interleaved\\nimage and text input (OpenAI, 2023a; Alayrac et al., 2022; Team et al., 2023; Li et al., 2023b). Language\\nagents built on such multimodal models natively reason over both image and text input (Bavishi et al., 2023;\\nElsen et al., 2023; Liu et al., 2023b; Hong et al., 2023b; Driess et al., 2023), allowing them to ingest perceptual\\n\\n                                                                     17', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='36a7d51c-eca7-4901-9a58-6d5b6b7b1530', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\ndata and directly produce actions. This bypasses the lossy image-to-text conversion; however, it also tightly\\ncouples the reasoning and planning process to the model’s input modalities.\\nAt a high level, the two approaches can be seen as different tokenization schemes to convert non-linguistic\\nmodalities into the core reasoning model’s language domain. The modular approach uses a separate image-to-\\ntext model to convert perceptual data into language (Ahn et al., 2022; Zeng et al., 2022), while the integrated\\napproach projects images directly into the language model’s representation space (Bavishi et al., 2023; Elsen\\net al., 2023; Liu et al., 2023b). Integrated, multimodal reasoning may allow for more human-like behaviors: a\\nVLM-based agent could “see” a webpage, whereas a LLM-based agent would more likely be given raw HTML.\\nHowever, coupling the agent’s perception and reasoning systems makes the agent more domain-specific and\\ndifficult to update. In either case, the basic architectural principles described by CoALA — internal memories,\\na structured action space, and generalized decision-making — can be used to guide agent design.\\nInternal vs. external: what is the boundary between an agent and its environment? While\\nhumans or robots are clearly distinct from their embodied environment, digital language agents have less\\nclear boundaries. For example, is a Wikipedia database an internal semantic memory or an external digital\\nenvironment (Yao et al., 2022b)? If an agent iteratively executes and improves code before submitting\\nan answer (Shinn et al., 2023; Yang et al., 2023), is the code execution internal or external? If a method\\nconsists of proposal and evaluation prompts (Yao et al., 2023), should it be considered a single agent or two\\ncollaborating simpler agents (proposer and evaluator)?\\nWe suggest the boundary question can be answered in terms of controllability and coupling. For example,\\nWikipedia is not controllable: it is an external environment that may be unexpectedly modified by other users.\\nHowever, an offline version that only the agent may write to is controllable, and thus can be considered an\\ninternal memory. Similarly, code execution on a internal virtual environment should be considered an internal\\nreasoning action, whereas code execution on an external machine (which may possess security vulnerabilities)\\nshould be considered an external grounding action. Lastly, if aspects of the agent – such as proposal and\\nevaluation prompts – are designed for and dependent on each other, then they are tightly coupled and best\\nconceptualized as components in an individual agent. In contrast, if the steps are independently useful, a\\nmulti-agent perspective may be more appropriate. While these dilemmas are primarily conceptual, such\\nunderstanding can support agent design and help the field align on shared terminology. Practioners may also\\njust choose their preferred framing, as long as it is consistent and useful for their own work.\\nPhysical vs. digital: what differences beget attention? While animals only live once in the physical\\nworld, digital environments (e.g., the Internet) often allow sequential (via resets) and parallel trials. This\\nmeans digital agents can more boldly explore (e.g., open a million webpages) and self-clone for parallel task\\nsolving (e.g., a million web agents try different web paths), which may result in decision-making procedures\\ndifferent from current ones inspired by human cognition (Griffiths, 2020).\\nLearning vs. acting: how should agents continuously and autonomously learn? In the CoALA\\nframework, learning is a result action of a decision-making cycle just like grounding: the agent deliberately\\nchooses to commit information to long-term memory. This is in contrast to most agents, which simply fix a\\nlearning schedule and only use decison making for external actions. Biological agents, however, do not have\\nthis luxury: they must balance learning against external actions in their lifetime, choosing when and what to\\nlearn (Mattar and Daw, 2018). More flexible language agents (Wang et al., 2023a; Park et al., 2023) would\\nfollow a similar design and treat learning on par with external actions. Learning could be proposed as a\\npossible action during regular decision-making, allowing the agent to “defer” it until the appropriate time.\\nGPT-4 vs GPT-N: how would agent design change with more powerful LLMs? Agent design\\nis a moving target as new LLM capabilities emerge with scale (Wei et al., 2022a). For example, earlier\\nlanguage models such as GPT-2 (Radford et al., 2019) would not support LLM agents — indeed, work at\\nthat time needed to combine GPT-2 with reinforcement learning for action generation (Yao et al., 2020);\\nGPT-3 (Brown et al., 2020) unlocked flexible few-shot and zero-shot reasoning for NLP tasks; while only\\nGPT-4 (OpenAI, 2023a) starts to afford more reliable self-evaluation (Saunders et al., 2022; Shinn et al.,\\n2023; Yao et al., 2023) and self-refinement (Madaan et al., 2023; Chen et al., 2023b). Will future LLMs\\nfurther reduce the need for coded rules and extra-learned models? Will this necessitate changes to the CoALA\\nframework? As a thought experiment, imagine GPT-N could “simulate” memory, grounding, learning, and\\n\\n                                                                    18', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='fcb54f8d-0685-43cf-95d9-54c09cb4d8cf', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=' Published in Transactions on Machine Learning Research (02/2024)\\n\\n decision-making in context: list all the possible actions, simulate and evaluate each one, and maintain its\\n entire long-term memory explicitly in a very long context. Or even more boldly: perhaps GPT-N+1 succeeds\\n at generating the next action by simulating these implicitly in neurons, without any intermediate reasoning\\n in context. While these extreme cases seem unlikely in the immediate future, incremental improvements may\\n alter the importance of different CoALA components. For example, a longer context window could reduce\\n the importance of long-term memory, while more powerful reasoning for internal evaluation and simulation\\n could allow longer-horizon planning. In general, LLMs are not subject to biological limitations (Griffiths,\\n2020), and their emergent properties have been difficult to predict. Nonetheless, CoALA – and cognitive\\n science more generally – may still help organize tasks where language agents succeed or fail, and suggest\\n code-based procedures to complement a given LLM on a given task. Even in the most extreme case, where\\n GPT implements all of CoALA’s mechanisms in neurons, it may be helpful to leverage CoALA as a conceptual\\n guide to discover and interpret those implicit circuits. Of course, as discussed in Section 6, agent usecases\\nwill also help discover, define and shape LLM capabilities. Similar to how chips and computer architectures\\n have co-evolved, language model and agent design should also develop a reciprocal path forward.\\n\\n 8\\n      Conclusion\\nWe proposed Cognitive Architectures for Language Agents (CoALA), a conceptual framework to describe\\n and build language agents. Our framework draws inspiration from the rich history of symbolic artificial\\n intelligence and cognitive science, connecting decades-old insights to frontier research on large language\\n models. We believe this approach provides a path towards developing more general and more human-like\\n artificial intelligence.\\n\\n Acknowledgements\\nWe thank Harrison Chase, Baian Chen, Khanh Nguyen, Ofir Press, Noah Shinn, Jens Tuyls for proofreading\\n and valuable feedback, and members from the Princeton NLP Group and Princeton Computational Cognitive\\n Science Lab for helpful discussions. Finally, we thank our anonymous reviewers for insightful comments and\\n suggestions. SY and KN acknowledge support from an Oracle Collaborative Research award and the National\\n Science Foundation under Grant No. 2239363. Any opinions, findings, conclusions, or recommendations\\n expressed in this material are those of the author(s) and do not necessarily reflect the views of the National\\n Science Foundation. SY is also supported by the Harold W. Dodds Fellowship from Princeton. TS is\\n supported by the National Defense Science and Engineering (NDSEG) Graduate Fellowship Program.\\n\\n References\\n S. Adams, I. Arel, J. Bach, R. Coop, R. Furlan, B. Goertzel, J. S. Hall, A. Samsonovich, M. Scheutz,\\n   M. Schlesinger, et al. Mapping the landscape of human-level artificial general intelligence. AI magazine, 33\\n   (1):25–42, 2012.\\n M. Ahn, A. Brohan, N. Brown, Y. Chebotar, O. Cortes, B. David, C. Finn, C. Fu, K. Gopalakrishnan,\\n   K. Hausman, et al. Do as I can, not as I say: Grounding language in robotic affordances. arXiv preprint\\n   arXiv:2204.01691, 2022.\\n J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds,\\n   et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing\\n   Systems, 35:23716–23736, 2022.\\n J. R. Anderson and C. Lebiere. The Newell test for a theory of cognition. Behavioral and Brain Sciences, 26\\n   (5):587–601, 2003.\\n J. Andreas. Language models as agent models. In Findings of the Association for Computational Linguistics:\\n   EMNLP 2022, pages 5769–5779, 2022.\\n R. C. Atkinson and R. M. Shiffrin. Human memory: A proposed system and its control processes. In\\n   Psychology of Learning and Motivation, volume 2, pages 89–195. Elsevier, 1968.\\n\\n                                                                    19', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='56f483cd-3479-4f38-9dc2-4eca87323e28', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nA. D. Baddeley and G. Hitch. Working memory. In Psychology of Learning and Motivation, volume 8, pages\\n   47–89. Elsevier, 1974.\\n\\nY. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini,\\n   C. McKinnon, et al. Constitutional AI: Harmlessness from AI feedback. arXiv preprint arXiv:2212.08073,\\n   2022.\\n\\nR. Bavishi, E. Elsen, C. Hawthorne, M. Nye, A. Odena, A. Somani, and S. Taşırlar. Introducing our\\n   multimodal models, 2023. URL https://www.adept.ai/blog/fuyu-8b.\\n\\nY. Bisk, D. Marcu, and W. Wong. Towards a dataset for human computer communication via grounded\\n   language acquisition. In Workshops at the Thirtieth AAAI Conference on Artificial Intelligence, 2016.\\n\\nE. Biyik and M. Palan. Asking easy questions: A user-friendly approach to active reward learning. In\\n   Proceedings of the 3rd Conference on Robot Learning, 2019.\\n\\nC. Blundell, B. Uria, A. Pritzel, Y. Li, A. Ruderman, J. Z. Leibo, J. Rae, D. Wierstra, and D. Hassabis.\\n   Model-free episodic control. arXiv preprint arXiv:1606.04460, 2016.\\n\\nS. Borgeaud, A. Mensch, J. Hoffmann, T. Cai, E. Rutherford, K. Millican, G. B. Van Den Driessche, J.-B.\\n   Lespiau, B. Damoc, A. Clark, et al. Improving language models by retrieving from trillions of tokens. In\\n   International Conference on Machine Learning, pages 2206–2240, 2022.\\n\\nS. Branavan, D. Silver, and R. Barzilay. Learning to win by reading manuals in a Monte-Carlo framework.\\n   Journal of Artificial Intelligence Research, 43:661–704, 2012.\\n\\nM. Braverman, X. Chen, S. Kakade, K. Narasimhan, C. Zhang, and Y. Zhang. Calibration, entropy rates,\\n   and memory in language models. In International Conference on Machine Learning, pages 1089–1099,\\n   2020.\\n\\nG. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym,\\n   2016.\\n\\nA. Brohan, N. Brown, J. Carbajal, Y. Chebotar, J. Dabis, C. Finn, K. Gopalakrishnan, K. Hausman,\\n   A. Herzog, J. Hsu, et al. RT-1: Robotics transformer for real-world control at scale. arXiv preprint\\n   arXiv:2212.06817, 2022.\\n\\nA. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey,\\n   C. Finn, et al. RT-2: Vision-language-action models transfer web knowledge to robotic control. arXiv\\n   preprint arXiv:2307.15818, 2023.\\n\\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\\n   A. Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing\\n   Systems, 33:1877–1901, 2020.\\n\\nC. B. Browne, E. Powley, D. Whitehouse, S. M. Lucas, P. I. Cowling, P. Rohlfshagen, S. Tavener, D. Perez,\\n   S. Samothrakis, and S. Colton. A survey of Monte Carlo tree search methods. IEEE Transactions on\\n   Computational Intelligence and AI in games, 4(1):1–43, 2012.\\n\\nF. Callaway, B. van Opheusden, S. Gul, P. Das, P. M. Krueger, T. L. Griffiths, and F. Lieder. Rational use of\\n   cognitive resources in human planning. Nature Human Behaviour, 6(8):1112–1125, 2022.\\n\\nC.-M. Chan, W. Chen, Y. Su, J. Yu, W. Xue, S. Zhang, J. Fu, and Z. Liu. Chateval: Towards better llm-based\\n   evaluators through multi-agent debate. arXiv preprint arXiv:2308.07201, 2023.\\n\\nB. Chen, F. Xia, B. Ichter, K. Rao, K. Gopalakrishnan, M. S. Ryoo, A. Stone, and D. Kappler. Open-\\n   vocabulary queryable scene representations for real world planning. In 2023 IEEE International Conference\\n   on Robotics and Automation (ICRA), pages 11509–11522, 2023a.\\n\\n                                                                   20', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='4f3c50f0-b849-42a2-9bef-9cb7707bd3f1', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nD. Chen and R. Mooney. Learning to interpret natural language navigation instructions from observations.\\n  In Proceedings of the AAAI Conference on Artificial Intelligence, volume 25, pages 859–865, 2011.\\nD. Chen, A. Fisch, J. Weston, and A. Bordes. Reading Wikipedia to answer open-domain questions. arXiv\\n  preprint arXiv:1704.00051, 2017.\\n\\nM. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda, N. Joseph,\\n  G. Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374,\\n  2021.\\n\\nX. Chen, M. Lin, N. Schärli, and D. Zhou. Teaching large language models to self-debug. arXiv preprint\\n  arXiv:2304.05128, 2023b.\\nY. Chen, L. Yuan, G. Cui, Z. Liu, and H. Ji. A close look into the calibration of pre-trained language models.\\n  arXiv preprint arXiv:2211.00151, 2022.\\nN. Chomsky. Three models for the description of language. IRE Transactions on information theory, 2(3):\\n  113–124, 1956.\\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W. Chung, C. Sutton,\\n  S. Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311,\\n  2022.\\nP. F. Christiano, J. Leike, T. Brown, M. Martic, S. Legg, and D. Amodei. Deep reinforcement learning from\\n  human preferences. Advances in neural information processing systems, 30, 2017.\\nA. Church. A set of postulates for the foundation of logic. Annals of mathematics, pages 346–366, 1932.\\nM.-A. Côté, A. Kádár, X. Yuan, B. Kybartas, T. Barnes, E. Fine, J. Moore, M. Hausknecht, L. El Asri,\\n  M. Adada, et al. Textworld: A learning environment for text-based games. In Computer Games: 7th\\n  Workshop, CGW 2018, pages 41–75. Springer, 2019.\\nA. Creswell, M. Shanahan, and I. Higgins. Selection-inference: Exploiting large language models for\\n  interpretable logical reasoning. In The Eleventh International Conference on Learning Representations,\\n  2023.\\n\\nG. Dagan, F. Keller, and A. Lascarides. Dynamic Planning with a LLM. arXiv preprint arXiv:2308.06391,\\n  2023.\\nI. Dasgupta, C. Kaeser-Chen, K. Marino, A. Ahuja, S. Babayan, F. Hill, and R. Fergus. Collaborating with\\n  language models for embodied reasoning. In Second Workshop on Language and Reinforcement Learning,\\n  2022.\\nX. Deng, Y. Gu, B. Zheng, S. Chen, S. Stevens, B. Wang, H. Sun, and Y. Su. Mind2Web: Towards a\\n  generalist agent for the web. arXiv preprint arXiv:2306.06070, 2023.\\nN. Derbinsky, J. Li, and J. Laird. A multi-domain evaluation of scaling in a general episodic memory. In\\n  Proceedings of the AAAI Conference on Artificial Intelligence, volume 26, pages 193–199, 2012.\\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of Deep Bidirectional Transformers\\n  for Language Understanding. In NAACL-HLT (1), 2019.\\nD. Dohan, W. Xu, A. Lewkowycz, J. Austin, D. Bieber, R. G. Lopes, Y. Wu, H. Michalewski, R. A. Saurous,\\n  J. Sohl-Dickstein, et al. Language model cascades. arXiv preprint arXiv:2207.10342, 2022.\\nY. Dong, X. Jiang, Z. Jin, and G. Li. Self-collaboration code generation via chatgpt. arXiv preprint\\n  arXiv:2304.07590, 2023.\\nD. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson, Q. Vuong, T. Yu,\\n  et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023.\\n\\n                                                                  21', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='71ff2164-42fb-46a0-84cd-4ca6f1009d2a', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nY. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch. Improving factuality and reasoning in language\\n  models through multiagent debate. arXiv preprint arXiv:2305.14325, 2023.\\nA. Ecoffet, J. Huizinga, J. Lehman, K. O. Stanley, and J. Clune. Go-explore: a new approach for hard-\\n  exploration problems. arXiv preprint arXiv:1901.10995, 2019.\\n\\nK. Ellis, C. Wong, M. Nye, M. Sablé-Meyer, L. Morales, L. Hewitt, L. Cary, A. Solar-Lezama, and J. B.\\n  Tenenbaum. Dreamcoder: Bootstrapping inductive program synthesis with wake-sleep library learning. In\\n  Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and\\n  Implementation, pages 835–850, 2021.\\n\\nE. Elsen, A. Odena, M. Nye, S. Taşırlar, T. Dao, C. Hawthorne, D. Moparthi, and A. Somani. Releasing\\n  Persimmon-8B, 2023. URL https://www.adept.ai/blog/persimmon-8b.\\nS. Feng, C. Y. Park, Y. Liu, and Y. Tsvetkov. From pretraining data to language models to downstream tasks:\\n  Tracking the trails of political biases leading to unfair nlp models. arXiv preprint arXiv:2305.08283, 2023.\\nD. Ganguli, A. Askell, N. Schiefer, T. Liao, K. Lukoši¯ut˙e, A. Chen, A. Goldie, A. Mirhoseini, C. Olsson,\\n  D. Hernandez, et al. The capacity for moral self-correction in large language models. arXiv preprint\\n  arXiv:2302.07459, 2023.\\nC. Gao, X. Lan, Z. Lu, J. Mao, J. Piao, H. Wang, D. Jin, and Y. Li. S3: Social-network simulation system\\n  with large language model-empowered agents. arXiv preprint arXiv:2307.14984, 2023.\\nT. Gao, A. Fisch, and D. Chen. Making pre-trained language models better few-shot learners. arXiv preprint\\n  arXiv:2012.15723, 2020.\\nS. J. Gershman, E. J. Horvitz, and J. B. Tenenbaum. Computational rationality: A converging paradigm for\\n  intelligence in brains, minds, and machines. Science, 349(6245):273–278, 2015.\\nT. L. Griffiths. Understanding human intelligence through human limitations. Trends in Cognitive Sciences,\\n  24(11):873–883, 2020.\\nJ. Gu, Y. Wang, K. Cho, and V. O. Li. Search engine guided neural machine translation. In Proceedings of\\n  the AAAI Conference on Artificial Intelligence, volume 32, 2018.\\n\\nL. Guan, K. Valmeekam, S. Sreedharan, and S. Kambhampati. Leveraging pre-trained large language models\\n  to construct and utilize world models for model-based task planning. arXiv preprint arXiv:2305.14909,\\n  2023.\\n\\nGuidance. Guidance, 2023. URL https://github.com/guidance-ai/guidance.\\nI. Gur, H. Furuta, A. Huang, M. Safdari, Y. Matsuo, D. Eck, and A. Faust. A real-world webagent with\\n  planning, long context understanding, and program synthesis. arXiv preprint arXiv:2307.12856, 2023.\\nK. Guu, K. Lee, Z. Tung, P. Pasupat, and M. Chang. Retrieval augmented language model pre-training. In\\n  International conference on machine learning, pages 3929–3938, 2020.\\nJ. B. Hamrick, V. Bapst, A. Sanchez-Gonzalez, T. Pfaff, T. Weber, L. Buesing, and P. W. Battaglia.\\n  Combining q-learning and search with amortized value estimates. In International Conference on Learning\\n  Representations, 2019.\\nA. W. Hanjie, V. Zhong, and K. Narasimhan. Grounding language to entities and dynamics for generalization\\n  in reinforcement learning. In International Conference on Machine Learning (ICML), 2021.\\nS. Hao, Y. Gu, H. Ma, J. J. Hong, Z. Wang, D. Z. Wang, and Z. Hu. Reasoning with language model is\\n  planning with world model. arXiv preprint arXiv:2305.14992, 2023.\\nM. Hasan, C. Ozel, S. Potter, and E. Hoque. Sapien: Affective virtual agents powered by large language\\n  models. arXiv preprint arXiv:2308.03022, 2023.\\n\\n                                                                  22', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='14bafe16-61f3-4fb7-8f32-f2c17a2752e0', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text=' Published in Transactions on Machine Learning Research (02/2024)\\n\\n P. Haslum, N. Lipovetzky, D. Magazzeni, C. Muise, R. Brachman, F. Rossi, and P. Stone. An introduction to\\n   the planning domain definition language, volume 13. Springer, 2019.\\n M. Hausknecht, P. Ammanabrolu, M.-A. Côté, and X. Yuan. Interactive fiction games: A colossal adventure.\\n   In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 7903–7910, 2020.\\n S. Hong, X. Zheng, J. Chen, Y. Cheng, C. Zhang, Z. Wang, S. K. S. Yau, Z. Lin, L. Zhou, C. Ran, et al.\\n   Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352,\\n   2023a.\\n W. Hong, W. Wang, Q. Lv, J. Xu, W. Yu, J. Ji, Y. Wang, Z. Wang, Y. Dong, M. Ding, et al. Cogagent: A\\n   visual language model for gui agents. arXiv preprint arXiv:2312.08914, 2023b.\\n J. Huang, S. S. Gu, L. Hou, Y. Wu, X. Wang, H. Yu, and J. Han. Large language models can self-improve.\\n   arXiv preprint arXiv:2210.11610, 2022a.\\n S. Huang, Z. Jiang, H. Dong, Y. Qiao, P. Gao, and H. Li. Instruct2Act: Mapping Multi-modality Instructions\\n   to Robotic Actions with Large Language Model. arXiv preprint arXiv:2305.11176, 2023.\\n W. Huang, P. Abbeel, D. Pathak, and I. Mordatch. Language models as zero-shot planners: Extracting\\n   actionable knowledge for embodied agents. In International Conference on Machine Learning, pages\\n   9118–9147, 2022b.\\n W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng, J. Tompson, I. Mordatch, Y. Chebotar,\\n   et al. Inner monologue: Embodied reasoning through planning with language models. arXiv preprint\\n   arXiv:2207.05608, 2022c.\\n A. Hussein, M. M. Gaber, E. Elyan, and C. Jayne. Imitation learning: A survey of learning methods. ACM\\n   Computing Surveys (CSUR), 50(2):1–35, 2017.\\n G. Irving, P. Christiano, and D. Amodei. AI safety via debate. arXiv preprint arXiv:1805.00899, 2018.\\n G. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin, and E. Grave. Unsupervised dense\\n   information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021.\\n Z. Jiang, J. Araki, H. Ding, and G. Neubig. How can we know when language models know? on the calibration\\n   of language models for question answering. Transactions of the Association for Computational Linguistics,\\n   9:962–977, 2021.\\n Z. Jin, S. Levine, F. G. Adauto, O. Kamal, M. Sap, M. Sachan, R. Mihalcea, J. B. Tenenbaum, and\\n   B. Schölkopf. When to make exceptions: Exploring language models as accounts of human moral judgment.\\n   In A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, editors, Advances in Neural Information Processing\\n   Systems, 2022.\\n S. Jinxin, Z. Jiabao, W. Yilei, W. Xingjiao, L. Jiawen, and H. Liang. Cgmi: Configurable general multi-agent\\n   interaction framework. arXiv preprint arXiv:2308.12503, 2023.\\n R. M. Jones, J. E. Laird, P. E. Nielsen, K. J. Coulter, P. Kenny, and F. V. Koss. Automated intelligent pilots\\n   for combat flight simulation. AI magazine, 20(1):27–27, 1999.\\nD. Jurafsky. Speech & language processing. Pearson Education India, 2000.\\n O. Khattab, K. Santhanam, X. L. Li, D. Hall, P. Liang, C. Potts, and M. Zaharia. Demonstrate-search-predict:\\n   Composing retrieval and language models for knowledge-intensive NLP. arXiv preprint arXiv:2212.14024,\\n   2022. URL https://github.com/stanfordnlp/dspy.\\n G. Kim, P. Baldi, and S. McAleer. Language models can solve computer tasks. arXiv preprint arXiv:2303.17491,\\n   2023.\\n J. R. Kirk and J. E. Laird. Interactive task learning for simple games. Advances in Cognitive Systems, 3\\n   (13-30):5, 2014.\\n\\n                                                                    23', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='3cb213ad-0144-4ab1-970b-80130688cd72', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nJ. R. Kirk, W. Robert, P. Lindes, and J. E. Laird. Improving Knowledge Extraction from LLMs for Robotic\\n  Task Learning through Agent Analysis. arXiv preprint arXiv:2306.06770, 2023.\\nK. R. Koedinger, J. R. Anderson, W. H. Hadley, M. A. Mark, et al. Intelligent tutoring goes to school in the\\n  big city. International Journal of Artificial Intelligence in Education, 8(1):30–43, 1997.\\nT. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners.\\n  Advances in Neural Information Processing Systems, 35:22199–22213, 2022.\\nI. Kotseruba and J. K. Tsotsos. 40 years of cognitive architectures: core cognitive abilities and practical\\n  applications. Artificial Intelligence Review, 53(1):17–94, 2020.\\nC. Laidlaw, S. Russell, and A. Dragan. Bridging rl theory and practice with the effective horizon. arXiv\\n  preprint arXiv:2304.09853, 2023.\\nJ. E. Laird. The Soar cognitive architecture. MIT press, 2019.\\nJ. E. Laird. Introduction to Soar. arXiv preprint arXiv:2205.03854, 2022.\\nJ. E. Laird, P. S. Rosenbloom, and A. Newell. Chunking in Soar: The anatomy of a general learning\\n  mechanism. Machine Learning, 1:11–46, 1986.\\nJ. E. Laird, A. Newell, and P. S. Rosenbloom. Soar: An architecture for general intelligence. Artificial\\n  Intelligence, 33(1):1–64, 1987.\\nJ. E. Laird, K. R. Kinkade, S. Mohan, and J. Z. Xu. Cognitive robotics using the Soar cognitive architecture.\\n  In CogRob @ AAAI, 2012.\\nB. M. Lake, T. D. Ullman, J. B. Tenenbaum, and S. J. Gershman. Building machines that learn and think\\n  like people, 2016.\\nLangChain. LangChain, 2022. URL http://www.langchain.com.\\nH. Le, Y. Wang, A. D. Gotmare, S. Savarese, and S. C. H. Hoi. Coderl: Mastering code generation through\\n  pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems,\\n  35:21314–21328, 2022.\\nY. LeCun. A path towards autonomous machine intelligence version 0.9. 2, 2022-06-27. Open Review, 62,\\n  2022.\\nP. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih,\\n  T. Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive NLP tasks. Advances in\\n  Neural Information Processing Systems, 33:9459–9474, 2020.\\nB. Z. Li, W. Chen, P. Sharma, and J. Andreas. Lampp: Language models as probabilistic priors for perception\\n  and action. arXiv preprint arXiv:2302.02801, 2023a.\\nC. Li, Z. Gan, Z. Yang, J. Yang, L. Li, L. Wang, and J. Gao. Multimodal foundation models: From specialists\\n  to general-purpose assistants. arXiv preprint arXiv:2309.10020, 2023b.\\nH. Li, Y. Su, D. Cai, Y. Wang, and L. Liu. A survey on retrieval-augmented text generation. arXiv preprint\\n  arXiv:2202.01110, 2022a.\\nR. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li, J. Chim,\\n  Q. Liu, E. Zheltonozhskii, T. Y. Zhuo, T. Wang, O. Dehaene, M. Davaadorj, J. Lamy-Poirier, J. Monteiro,\\n  O. Shliazhko, N. Gontier, N. Meade, A. Zebaze, M.-H. Yee, L. K. Umapathi, J. Zhu, B. Lipkin, M. Oblokulov,\\n  Z. Wang, R. Murthy, J. Stillerman, S. S. Patel, D. Abulkhanov, M. Zocca, M. Dey, Z. Zhang, N. Fahmy,\\n  U. Bhattacharyya, W. Yu, S. Singh, S. Luccioni, P. Villegas, M. Kunakov, F. Zhdanov, M. Romero, T. Lee,\\n  N. Timor, J. Ding, C. Schlesinger, H. Schoelkopf, J. Ebert, T. Dao, M. Mishra, A. Gu, J. Robinson, C. J.\\n  Anderson, B. Dolan-Gavitt, D. Contractor, S. Reddy, D. Fried, D. Bahdanau, Y. Jernite, C. M. Ferrandis,\\n  S. M. Hughes, T. Wolf, A. Guha, L. von Werra, and H. de Vries. Starcoder: may the source be with you!\\n  ArXiv, abs/2305.06161, 2023c.\\n\\n                                                                   24', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='755107c3-290f-4285-840b-1b7b0223257c', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nY. Li, D. H. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, Tom, Eccles, J. Keeling, F. Gimeno,\\n  A. D. Lago, T. Hubert, P. Choy, C. de, M. d’Autume, I. Babuschkin, X. Chen, P.-S. Huang, J. Welbl,\\n  S. Gowal, Alexey, Cherepanov, J. Molloy, D. J. Mankowitz, E. S. Robson, P. Kohli, N. de, Freitas,\\n  K. Kavukcuoglu, and O. Vinyals. Competition-level code generation with alphacode. Science, 378:1092 –\\n  1097, 2022b.\\n\\nJ. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence, and A. Zeng. Code as policies:\\n  Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and\\n  Automation (ICRA), pages 9493–9500, 2023a.\\nP. P. Liang, C. Wu, L.-P. Morency, and R. Salakhutdinov. Towards understanding and mitigating social\\n  biases in language models. In International Conference on Machine Learning, pages 6565–6576, 2021.\\n\\nT. Liang, Z. He, W. Jiao, X. Wang, Y. Wang, R. Wang, Y. Yang, Z. Tu, and S. Shi. Encouraging divergent\\n  thinking in large language models through multi-agent debate. arXiv preprint arXiv:2305.19118, 2023b.\\n\\nF. Lieder and T. L. Griffiths. Resource-rational analysis: Understanding human cognition as the optimal use\\n  of limited computational resources. Behavioral and Brain Sciences, 43:e1, 2020.\\nB. Y. Lin, Y. Fu, K. Yang, P. Ammanabrolu, F. Brahman, S. Huang, C. Bhagavatula, Y. Choi, and X. Ren.\\n  Swiftsage: A generative agent with fast and slow thinking for complex interactive tasks. arXiv preprint\\n  arXiv:2305.17390, 2023.\\n\\nP. Lindes and J. E. Laird. Toward integrating cognitive linguistics and cognitive language processing. In\\n  Proceedings of the 14th International Conference on Cognitive Modeling (ICCM), 2016.\\nB. Liu, Y. Jiang, X. Zhang, Q. Liu, S. Zhang, J. Biswas, and P. Stone. LLM+P: Empowering large language\\n  models with optimal planning proficiency. arXiv preprint arXiv:2304.11477, 2023a.\\n\\nH. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In NeurIPS, 2023b.\\n\\nH. Liu, C. Sferrazza, and P. Abbeel. Languages are rewards: Hindsight finetuning using human feedback.\\n  arXiv preprint arXiv:2302.02676, 2023c.\\nJ. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen. What Makes Good In-Context Examples for\\n  GPT-3 ? arXiv preprint arXiv:2101.06804, 2021.\\n\\nP. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig. Pre-train, prompt, and predict: A systematic\\n  survey of prompting methods in natural language processing. ACM Computing Surveys, 55(9), 2023d.\\n  ISSN 0360-0300.\\nR. Liu, J. Wei, S. S. Gu, T.-Y. Wu, S. Vosoughi, C. Cui, D. Zhou, and A. M. Dai. Mind’s eye: Grounded\\n  language model reasoning through simulation. In The Eleventh International Conference on Learning\\n  Representations, 2023e.\\n\\nR. Liu, R. Yang, C. Jia, G. Zhang, D. Zhou, A. M. Dai, D. Yang, and S. Vosoughi. Training socially aligned\\n  language models in simulated human society. arXiv preprint arXiv:2305.16960, 2023f.\\n\\nLlamaIndex. LlamaIndex, 2023. URL http://www.llamaindex.ai.\\nL. E. Lwakatare, A. Raj, I. Crnkovic, J. Bosch, and H. H. Olsson. Large-scale machine learning systems in\\n  real-world industrial settings: A review of challenges and solutions. Information and software technology,\\n  127:106368, 2020.\\n\\nZ. Ma, Y. Mei, and Z. Su. Understanding the benefits and challenges of using large language model-based\\n  conversational agents for mental well-being support. arXiv preprint arXiv:2307.15810, 2023.\\nS. Macenski, T. Foote, B. Gerkey, C. Lalancette, and W. Woodall. Robot operating system 2: Design,\\n  architecture, and uses in the wild. Science Robotics, 7(66):eabm6074, 2022.\\n\\n                                                                  25', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='585d4ceb-79c4-4ead-b312-52bb11010825', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nA. Madaan, N. Tandon, P. Gupta, S. Hallinan, L. Gao, S. Wiegreffe, U. Alon, N. Dziri, S. Prabhumoye,\\n  Y. Yang, et al. Self-refine: Iterative refinement with self-feedback. arXiv preprint arXiv:2303.17651, 2023.\\n\\nA. A. Markov. The theory of algorithms. Trudy Matematicheskogo Instituta Imeni VA Steklova, 42:3–375,\\n  1954.\\nM. G. Mattar and N. D. Daw. Prioritized memory access explains planning and hippocampal replay. Nature\\n  Neuroscience, 21(11):1609–1617, 2018.\\nJ. L. McClelland, F. Hill, M. Rudolph, J. Baldridge, and H. Schütze. Extending machine language models\\n  toward human-level language understanding. arXiv preprint arXiv:1912.05877, 2019.\\n\\nJ. Meier, R. Rao, R. Verkuil, J. Liu, T. Sercu, and A. Rives. Language models enable zero-shot prediction of\\n  the effects of mutations on protein function. bioRxiv, 2021.\\nG. Mialon, R. Dessì, M. Lomeli, C. Nalmpantis, R. Pasunuru, R. Raileanu, B. Rozière, T. Schick, J. Dwivedi-\\n  Yu, A. Celikyilmaz, et al. Augmented language models: a survey. arXiv preprint arXiv:2302.07842,\\n  2023.\\nS. Mohan and J. Laird. Learning goal-oriented hierarchical tasks from situated interactive instruction. In\\n  Proceedings of the AAAI Conference on Artificial Intelligence, volume 28, 2014.\\n\\nS. Mohan, A. H. Mininger, J. R. Kirk, and J. E. Laird. Acquiring grounded representations of words with\\n  situated interactive instruction. Advances in Cognitive Systems, 2:113–130, 2012.\\n\\nR. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain, V. Kosaraju, W. Saunders, et al.\\n  WebGPT: Browser-Assisted Question-Answering with Human Feedback. arXiv preprint arXiv:2112.09332,\\n  2021.\\nK. Narasimhan, R. Barzilay, and T. Jaakkola. Deep transfer in reinforcement learning by language grounding.\\n  In Journal of Artificial Intelligence Research (JAIR), 2018.\\nA. Narayan-Chen, P. Jayannavar, and J. Hockenmaier. Collaborative dialogue in Minecraft. In Proceedings\\n  of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5405–5415. Association\\n  for Computational Linguistics, 2019.\\n\\nS. Nason and J. E. Laird. Soar-RL: Integrating reinforcement learning with Soar. Cognitive Systems Research,\\n  6(1):51–59, 2005.\\nA. Newell. Studies in problem solving: Subject 3 on the crypt-arithmetic task DONALD+ GERALD=\\n  ROBERT. Technical report, Carnegie Mellon University, 1967.\\nA. Newell. Physical symbol systems. Cognitive science, 4(2):135–183, 1980.\\n\\nA. Newell. Précis of unified theories of cognition. Behavioral and Brain Sciences, 15(3):425–437, 1992.\\nA. Newell and H. A. Simon. Human problem solving. Prentice-Hall, 1972.\\nA. Newell, P. S. Rosenbloom, and J. E. Laird. Symbolic architectures for cognition. Foundations of cognitive\\n  science, pages 93–131, 1989.\\n\\nK. Nguyen and H. Daumé III. Help, Anna! visual navigation with natural multimodal assistance via\\n  retrospective curiosity-encouraging imitation learning. arXiv preprint arXiv:1909.01871, 2019.\\nK. Nguyen, D. Dey, C. Brockett, and B. Dolan. Vision-based navigation with language-based assistance via\\n  imitation learning with indirect intervention. In Proceedings of the IEEE/CVF Conference on Computer\\n  Vision and Pattern Recognition, pages 12527–12537, 2019.\\nK. Nguyen, Y. Bisk, and H. Daumé III. A framework for learning to request rich and contextually useful\\n  information from humans. In ICML, July 2022a.\\n\\n                                                                   26', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='76972f14-e05f-45b1-a0a2-0506088a8fc2', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nK. X. Nguyen. Language models are bounded pragmatic speakers. In First Workshop on Theory of Mind in\\n   Communicating Agents, 2023.\\nK. X. Nguyen, D. Misra, R. Schapire, M. Dudík, and P. Shafto. Interactive learning from activity description.\\n   In International Conference on Machine Learning, pages 8096–8108, 2021.\\nK. X. Nguyen, Y. Bisk, and H. D. Iii. A framework for learning to request rich and contextually useful\\n   information from humans. In International Conference on Machine Learning, pages 16553–16568, 2022b.\\nT. T. Nguyen, T. T. Huynh, P. L. Nguyen, A. W.-C. Liew, H. Yin, and Q. V. H. Nguyen. A survey of\\n   machine unlearning. arXiv preprint arXiv:2209.02299, 2022c.\\nA. Ni, S. Iyer, D. Radev, V. Stoyanov, W.-t. Yih, S. Wang, and X. V. Lin. Lever: Learning to verify\\n   language-to-code generation with execution. In International Conference on Machine Learning, pages\\n   26106–26128, 2023.\\nN. J. Nilsson. Shakey the robot. Technical Note, 1984.\\nR. Nogueira, W. Yang, J. Lin, and K. Cho. Document expansion by query prediction, 2019.\\nA. M. Nuxoll and J. E. Laird. Extending cognitive architecture with episodic memory. In Proceedings of the\\n   AAAI Conference on Artificial Intelligence, pages 1560–1564, 2007.\\nM. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan, A. Lewkowycz,\\n   M. Bosma, D. Luan, et al. Show your work: Scratchpads for intermediate computation with language\\n   models. arXiv preprint arXiv:2112.00114, 2021.\\nOpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023a.\\nOpenAI.          Function calling and other API updates, 2023b.                             URL https://openai.com/blog/\\n   function-calling-and-other-api-updates.\\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama,\\n   A. Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural\\n   Information Processing Systems, 35:27730–27744, 2022.\\nA. Padmakumar, J. Thomason, A. Shrivastava, P. Lange, A. Narayan-Chen, S. Gella, R. Piramuthu, G. Tur,\\n   and D. Hakkani-Tur. Teach: Task-driven embodied agents that chat. In Proceedings of the AAAI Conference\\n   on Artificial Intelligence, volume 36, pages 2017–2025, 2022.\\nN. D. Palo, A. Byravan, L. Hasenclever, M. Wulfmeier, N. Heess, and M. Riedmiller. Towards a unified agent\\n   with foundation models. In Workshop on Reincarnating Reinforcement Learning at ICLR 2023, 2023.\\nA. Parisi, Y. Zhao, and N. Fiedel. Talm: Tool augmented language models. arXiv preprint arXiv:2205.12255,\\n   2022.\\nJ. S. Park, J. C. O’Brien, C. J. Cai, M. R. Morris, P. Liang, and M. S. Bernstein. Generative agents:\\n   Interactive simulacra of human behavior. arXiv preprint arXiv:2304.03442, 2023.\\nP. Pataranutaporn, V. Danry, J. Leong, P. Punpongsanon, D. Novy, P. Maes, and M. Sra. AI-generated\\n   characters for supporting personalized learning and well-being. Nature Machine Intelligence, 3(12):1013–\\n   1022, 2021.\\nA. Peng, I. Sucholutsky, B. Li, T. R. Sumers, T. L. Griffiths, J. Andreas, and J. A. Shah. Language guided\\n   state abstractions. In Workshop on Social Intelligence in Humans and Robots at RSS 2023, 2023.\\nE. L. Post. Formal reductions of the general combinatorial decision problem. American Journal of Mathematics,\\n   65(2):197–215, 1943.\\nA. Pritzel, B. Uria, S. Srinivasan, A. P. Badia, O. Vinyals, D. Hassabis, D. Wierstra, and C. Blundell. Neural\\n   episodic control. In International conference on machine learning, pages 2827–2836, 2017.\\n\\n                                                                   27', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='304855d0-8004-4d82-b31e-57f03c0df6da', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nM. L. Puterman. Markov decision processes: discrete stochastic dynamic programming. John Wiley & Sons,\\n  2014.\\n\\nC. Qian, X. Cong, C. Yang, W. Chen, Y. Su, J. Xu, Z. Liu, and M. Sun. Communicative agents for software\\n  development. arXiv preprint arXiv:2307.07924, 2023.\\n\\nY. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang, B. Qian, et al. Toolllm: Facilitating\\n  large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023.\\n\\nM. Quigley. Ros: an open-source robot operating system. In IEEE International Conference on Robotics and\\n  Automation, 2009. URL https://api.semanticscholar.org/CorpusID:6324125.\\n\\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised\\n  multitask learners. OpenAI blog, 1(8):9, 2019.\\n\\nA. Z. Ren, A. Dixit, A. Bodrova, S. Singh, S. Tu, N. Brown, P. Xu, L. Takayama, F. Xia, Z. Xu, et al. Robots\\n  that ask for help: Uncertainty alignment for large language model planners. In 7th Annual Conference on\\n  Robot Learning, 2023.\\n\\nO. J. Romero, J. Zimmerman, A. Steinfeld, and A. Tomasic. Synergistic integration of large language models\\n  and cognitive architectures for robust ai: An exploratory analysis. arXiv preprint arXiv:2308.09830, 2023.\\n\\nB. Rozière, J. Gehring, F. Gloeckle, S. Sootla, I. Gat, X. Tan, Y. Adi, J. Liu, T. Remez, J. Rapin,\\n  A. Kozhevnikov, I. Evtimov, J. Bitton, M. P. Bhatt, C. C. Ferrer, A. Grattafiori, W. Xiong, A. D’efossez,\\n  J. Copet, F. Azhar, H. Touvron, L. Martin, N. Usunier, T. Scialom, and G. Synnaeve. Code llama: Open\\n  foundation models for code. ArXiv, abs/2308.12950, 2023.\\n\\nO. Rubin, J. Herzig, and J. Berant. Learning to retrieve prompts for in-context learning. arXiv preprint\\n  arXiv:2112.08633, 2021.\\n\\nE. Russek, D. Acosta-Kane, B. van Opheusden, M. G. Mattar, and T. Griffiths. Time spent thinking in\\n  online chess reflects the value of computation. PsyArXiv, 2022.\\n\\nS. Russell and P. Norvig. Artificial Intelligence: A Modern Approach. Pearson Education Limited London,\\n  2013.\\n\\nD. Sadigh, A. D. Dragan, S. Sastry, and S. A. Seshia. Active preference-based learning of reward functions.\\n  In N. M. Amato, S. S. Srinivasa, N. Ayanian, and S. Kuindersma, editors, Robotics: Science and Systems\\n  XIII, 2017.\\n\\nW. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and J. Leike. Self-critiquing models for assisting\\n  human evaluators. arXiv preprint arXiv:2206.05802, 2022.\\n\\nT. Schick, J. Dwivedi-Yu, R. Dessì, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda, and T. Scialom.\\n  Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761, 2023.\\n\\nD. Sculley, G. Holt, D. Golovin, E. Davydov, T. Phillips, D. Ebner, V. Chaudhary, and M. Young. Machine\\n  Learning: The High Interest Credit Card of Technical Debt. In SE4ML: Software Engineering for Machine\\n  Learning (NIPS 2014 Workshop), 2014.\\n\\nT. Shi, A. Karpathy, L. Fan, J. Hernandez, and P. Liang. World of Bits: An Open-Domain platform for\\n  web-based agents. In International Conference on Machine Learning, pages 3135–3144, 2017.\\n\\nN. Shinn, F. Cassano, B. Labash, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language agents\\n  with verbal reinforcement learning. arXiv preprint arXiv:2303.11366, 2023.\\n\\nM. Shridhar, X. Yuan, M.-A. Côté, Y. Bisk, A. Trischler, and M. Hausknecht. Alfworld: Aligning text and\\n  embodied environments for interactive learning. arXiv preprint arXiv:2010.03768, 2020.\\n\\n                                                                   28', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ef0280d0-f900-4f93-bdb9-d73556fa55f9', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nT. Silver, V. Hariprasad, R. S. Shuttleworth, N. Kumar, T. Lozano-Pérez, and L. P. Kaelbling. Pddl planning\\n  with pretrained large language models. In NeurIPS 2022 Foundation Models for Decision Making Workshop,\\n  2022.\\nT. Silver, S. Dan, K. Srinivas, J. B. Tenenbaum, L. P. Kaelbling, and M. Katz. Generalized Planning in\\n  PDDL Domains with Pretrained Large Language Models. arXiv preprint arXiv:2305.11014, 2023.\\nI. Singh, V. Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox, J. Thomason, and A. Garg.\\n  Progprompt: Generating situated robot task plans using large language models. In 2023 IEEE International\\n  Conference on Robotics and Automation (ICRA), pages 11523–11530, 2023.\\nT. Sumers, R. Hawkins, M. K. Ho, T. Griffiths, and D. Hadfield-Menell. How to talk so AI will learn:\\n  Instructions, descriptions, and autonomy. Advances in Neural Information Processing Systems, 35:34762–\\n  34775, 2022.\\nT. Sumers, K. Marino, A. Ahuja, R. Fergus, and I. Dasgupta. Distilling internet-scale vision-language models\\n  into embodied agents. In Proceedings of the 40th International Conference on Machine Learning, pages\\n  32797–32818, 2023.\\nT. R. Sumers, M. K. Ho, R. D. Hawkins, K. Narasimhan, and T. L. Griffiths. Learning rewards from linguistic\\n  feedback. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 6002–6010,\\n  2021.\\nR. Sun. Desiderata for cognitive architectures. Philosophical Psychology, 17(3):341–373, 2004.\\nR. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. MIT press, 2018.\\nO. Tafjord, B. Dalvi, and P. Clark. Proofwriter: Generating implications, proofs, and abductive statements\\n  over natural language. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,\\n  pages 3621–3634, 2021.\\nR. Tamari, C. Shani, T. Hope, M. R. L. Petruck, O. Abend, and D. Shahaf. Language (re)modelling: Towards\\n  embodied language understanding. In Proceedings of the 58th Annual Meeting of the Association for\\n  Computational Linguistics, pages 6268–6281, Online, July 2020. Association for Computational Linguistics.\\n  doi: 10.18653/v1/2020.acl-main.559.\\nM. Tambe, W. L. Johnson, R. M. Jones, F. Koss, J. E. Laird, P. S. Rosenbloom, and K. Schwamb. Intelligent\\n  agents for interactive simulation environments. AI magazine, 16(1):15–15, 1995.\\nM. Tang, S. Yao, J. Yang, and K. Narasimhan. Referral augmentation for zero-shot information retrieval,\\n  2023a.\\nQ. Tang, Z. Deng, H. Lin, X. Han, Q. Liang, and L. Sun. ToolAlpaca: Generalized Tool Learning for Language\\n  Models with 3000 Simulated Cases. arXiv preprint arXiv:2306.05301, 2023b.\\nG. Team, R. Anil, S. Borgeaud, Y. Wu, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth,\\n  et al. Gemini: a family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\\nS. Tellex, T. Kollar, S. Dickerson, M. Walter, A. Banerjee, S. Teller, and N. Roy. Understanding natural\\n  language commands for robotic navigation and mobile manipulation. In Proceedings of the AAAI Conference\\n  on Artificial Intelligence, volume 25, pages 1507–1514, 2011.\\nJ. Thomason, M. Murray, M. Cakmak, and L. Zettlemoyer. Vision-and-dialog navigation. In Conference on\\n  Robot Learning, pages 394–406. PMLR, 2020.\\nA. M. Turing et al. On computable numbers, with an application to the entscheidungsproblem. J. of Math,\\n  58(345-363):5, 1936.\\nJ. Tuyls, S. Yao, S. Kakade, and K. Narasimhan. Multi-stage episodic control for strategic exploration in\\n  text games. arXiv preprint arXiv:2201.01251, 2022.\\n\\n                                                                   29', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='176180fa-1426-45c9-a913-ea6c39e46afa', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nK. Valmeekam, A. Olmo, S. Sreedharan, and S. Kambhampati. Large language models still can’t plan (a\\n   benchmark for llms on planning and reasoning about change). arXiv preprint arXiv:2206.10498, 2022.\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.\\n   Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017.\\nG. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, and A. Anandkumar. Voyager: An\\n   open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291, 2023a.\\nL. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, W. X. Zhao,\\n   Z. Wei, and J.-R. Wen. A survey on large language model based autonomous agents, 2023b.\\nL. Wang, N. Yang, and F. Wei. Query2doc: Query expansion with large language models. arXiv preprint\\n   arXiv:2303.07678, 2023c.\\nR. Wang, P. Jansen, M.-A. Côté, and P. Ammanabrolu. Scienceworld: Is your agent smarter than a 5th\\n   grader? arXiv preprint arXiv:2203.07540, 2022a.\\nS. I. Wang, P. Liang, and C. D. Manning. Learning language games through interaction. In Proceedings of\\n   the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages\\n   2368–2378, 2016.\\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, and D. Zhou. Self-consistency improves chain of thought\\n   reasoning in language models. arXiv preprint arXiv:2203.11171, 2022b.\\nJ. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph, S. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler,\\n   E. H. Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, and W. Fedus. Emergent abilities of large language\\n   models. Transactions on Machine Learning Research, 2022a. ISSN 2835-8856. Survey Certification.\\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, E. Chi, Q. Le, and D. Zhou. Chain of thought prompting elicits\\n   reasoning in large language models. arXiv preprint arXiv:2201.11903, 2022b.\\nL. Weng. Llm-powered autonomous agents. lilianweng.github.io, Jun 2023. URL https://lilianweng.\\n   github.io/posts/2023-06-23-agent/.\\nJ. Weston, S. Chopra, and A. Bordes. Memory networks. arXiv preprint arXiv:1410.3916, 2014.\\nA. N. Whitehead and B. Russell. Principia mathematica to* 56, volume 2. Cambridge University Press, 1997.\\nD. E. Wilkins. Practical planning: extending the classical AI planning paradigm. Elsevier, 2014.\\nT. Winograd. Understanding natural language. Cognitive psychology, 3(1):1–191, 1972.\\nL. Wong, G. Grand, A. K. Lew, N. D. Goodman, V. K. Mansinghka, J. Andreas, and J. B. Tenenbaum.\\n   From word models to world models: Translating from natural language to the probabilistic language of\\n   thought. arXiv preprint arXiv:2306.12672, 2023.\\nR. E. Wray, J. R. Kirk, J. E. Laird, et al. Language models as a knowledge source for cognitive agents. arXiv\\n   preprint arXiv:2109.08270, 2021.\\nQ. Wu, G. Bansal, J. Zhang, Y. Wu, S. Zhang, E. Zhu, B. Li, L. Jiang, X. Zhang, and C. Wang. Autogen: En-\\n   abling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155,\\n   2023.\\nT. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and C. J. Cai. Promptchainer: Chaining\\n   large language model prompts through visual programming. In CHI Conference on Human Factors in\\n   Computing Systems Extended Abstracts, pages 1–10, 2022a.\\nT. Wu, M. Terry, and C. J. Cai. AI chains: Transparent and controllable human-AI interaction by chaining\\n   large language model prompts. In Proceedings of the 2022 CHI Conference on Human Factors in Computing\\n   Systems, pages 1–22, 2022b.\\n\\n                                                                   30', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='ca2527a2-6594-4b63-9206-bc7d26a111af', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nZ. Xi, W. Chen, X. Guo, W. He, Y. Ding, B. Hong, M. Zhang, J. Wang, S. Jin, E. Zhou, et al. The rise and\\n   potential of large language model based agents: A survey. arXiv preprint arXiv:2309.07864, 2023.\\nY. Xia, M. Shenoy, N. Jazdi, and M. Weyrich. Towards autonomous system: flexible modular production\\n   system enhanced with large language model agents. arXiv preprint arXiv:2304.14721, 2023.\\nY. Xie, T. Xie, M. Lin, W. Wei, C. Li, B. Kong, L. Chen, C. Zhuo, B. Hu, and Z. Li. Olagpt: Empowering\\n   llms with human-like problem-solving abilities. arXiv preprint arXiv:2305.16334, 2023.\\nB. Xu, X. Liu, H. Shen, Z. Han, Y. Li, M. Yue, Z. Peng, Y. Liu, Z. Yao, and D. Xu. Gentopia: A collaborative\\n   platform for tool-augmented llms. arXiv preprint arXiv:2308.04030, 2023a.\\nB. Xu, Z. Peng, B. Lei, S. Mukherjee, Y. Liu, and D. Xu. Rewoo: Decoupling reasoning from observations\\n   for efficient augmented language models. arXiv preprint arXiv:2305.18323, 2023b.\\nB. Xu, A. Yang, J. Lin, Q. Wang, C. Zhou, Y. Zhang, and Z. Mao. ExpertPrompting: Instructing Large\\n   Language Models to be Distinguished Experts. arXiv preprint arXiv:2305.14688, 2023c.\\nJ. Yang, A. Prabhakar, K. Narasimhan, and S. Yao. Intercode: Standardizing and benchmarking interactive\\n   coding with execution feedback. arXiv preprint arXiv:2306.14898, 2023.\\nS. Yao and K. Narasimhan. Language agents in the digital world: Opportunities and risks. princeton-\\n   nlp.github.io, Jul 2023. URL https://princeton-nlp.github.io/language-agent-impact/.\\nS. Yao, R. Rao, M. Hausknecht, and K. Narasimhan. Keep CALM and explore: Language models for action\\n   generation in text-based games. arXiv preprint arXiv:2010.02903, 2020.\\nS. Yao, H. Chen, J. Yang, and K. Narasimhan. Webshop: Towards scalable real-world web interaction with\\n   grounded language agents. Advances in Neural Information Processing Systems, 35:20744–20757, 2022a.\\nS. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y. Cao. React: Synergizing reasoning and\\n   acting in language models. arXiv preprint arXiv:2210.03629, 2022b.\\nS. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts: Deliberate\\n   problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023.\\nE. Zelikman, Y. Wu, J. Mu, and N. Goodman. STaR: Bootstrapping reasoning with reasoning. Advances in\\n   Neural Information Processing Systems, 35:15476–15488, 2022.\\nA. Zeng, M. Attarian, B. Ichter, K. Choromanski, A. Wong, S. Welker, F. Tombari, A. Purohit, M. Ryoo,\\n   V. Sindhwani, et al. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv\\n   preprint arXiv:2204.00598, 2022.\\nC. Zhang, L. Wong, G. Grand, and J. Tenenbaum. Grounded physical language understanding with\\n   probabilistic programs and simulated worlds. In Proceedings of the Annual Meeting of the Cognitive Science\\n   Society, volume 45, 2023a.\\nT. Zhang, F. Liu, J. Wong, P. Abbeel, and J. E. Gonzalez. The wisdom of hindsight makes language models\\n   better instruction followers. arXiv preprint arXiv:2302.05206, 2023b.\\nY. Zhang, S. Sun, M. Galley, Y.-C. Chen, C. Brockett, X. Gao, J. Gao, J. Liu, and W. B. Dolan. Dialogpt:\\n   Large-scale generative pre-training for conversational response generation. In Proceedings of the 58th\\n   Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 270–278,\\n   2020.\\nW. J. Zhao, R. Richie, and S. Bhatia. Process and content in decisions from memory. Psychological Review,\\n   129(1):73, 2022.\\nV. Zhong, A. W. Hanjie, S. Wang, K. Narasimhan, and L. Zettlemoyer. SILG: The Multi-domain Symbolic\\n   Interactive Language Grounding Benchmark. Advances in Neural Information Processing Systems, 34:\\n   21505–21519, 2021.\\n\\n                                                                   31', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}'),\n",
       " Document(id_='875aba78-cbf0-4519-95de-05fc7c299b5a', embedding=None, metadata={'file_path': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_name': 'Cognitive_Architectures_for_Language_Agents.pdf', 'file_type': 'application/pdf', 'file_size': 2785564, 'creation_date': '2025-01-08', 'last_modified_date': '2025-01-08'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={}, metadata_template='{key}: {value}', metadata_separator='\\n', text_resource=MediaResource(embeddings=None, data=None, text='Published in Transactions on Machine Learning Research (02/2024)\\n\\nC. Y. Zhou, D. Talmi, N. Daw, and M. G. Mattar. Episodic retrieval for model-based evaluation in sequential\\n  decision tasks, 2023a.\\nH. Zhou, M. Huang, T. Zhang, X. Zhu, and B. Liu. Emotional chatting machine: Emotional conversation\\n  generation with internal and external memory. In Proceedings of the AAAI Conference on Artificial\\n  Intelligence, volume 32, 2018.\\nS. Zhou, U. Alon, F. F. Xu, Z. Jiang, and G. Neubig. Docprompting: Generating code by retrieving the docs.\\n  In The Eleventh International Conference on Learning Representations, 2022a.\\nS. Zhou, F. F. Xu, H. Zhu, X. Zhou, R. Lo, A. Sridhar, X. Cheng, Y. Bisk, D. Fried, U. Alon, et al. WebArena:\\n  A Realistic Web Environment for Building Autonomous Agents. arXiv preprint arXiv:2307.13854, 2023b.\\nY. Zhou, A. I. Muresanu, Z. Han, K. Paster, S. Pitis, H. Chan, and J. Ba. Large language models are\\n  human-level prompt engineers. arXiv preprint arXiv:2211.01910, 2022b.\\n\\n                                                                   32', path=None, url=None, mimetype=None), image_resource=None, audio_resource=None, video_resource=None, text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents =data_for_parse.load_data()\n",
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=64,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_list = []\n",
    "page_number = 0\n",
    "last_doc = None\n",
    "for doc in documents:\n",
    "    if last_doc is None or last_doc != doc.metadata[\"file_name\"]:\n",
    "        page_number = 1\n",
    "        last_doc = doc.metadata[\"file_name\"]\n",
    "    else:\n",
    "        page_number += 1\n",
    "\n",
    "    texts = text_splitter.split_text(doc.text)\n",
    "    for text in texts:\n",
    "        item = {}\n",
    "        item[\"id_\"] = doc.id_\n",
    "        item[\"text\"] = text\n",
    "        item[\"metadata_file_name\"] = doc.metadata[\"file_name\"]\n",
    "        item[\"metadata_creation_date\"] = doc.metadata[\"creation_date\"]\n",
    "        item[\"metadata_pagenumber\"] = page_number\n",
    "        documents_list.append(item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunks to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_</th>\n",
       "      <th>text</th>\n",
       "      <th>metadata_file_name</th>\n",
       "      <th>metadata_creation_date</th>\n",
       "      <th>metadata_pagenumber</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d8e107bc-267a-4918-9831-64028f5ff03f</td>\n",
       "      <td>ChatLLM Network: More brains, More intelligenc...</td>\n",
       "      <td>ChatLLM_Network.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d8e107bc-267a-4918-9831-64028f5ff03f</td>\n",
       "      <td>Yirui Zhang\\n                                 ...</td>\n",
       "      <td>ChatLLM_Network.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d8e107bc-267a-4918-9831-64028f5ff03f</td>\n",
       "      <td>Abstract\\n                                    ...</td>\n",
       "      <td>ChatLLM_Network.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d8e107bc-267a-4918-9831-64028f5ff03f</td>\n",
       "      <td>ChatLLM network that allows multiple dialogue-...</td>\n",
       "      <td>ChatLLM_Network.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d8e107bc-267a-4918-9831-64028f5ff03f</td>\n",
       "      <td>network attains significant improvements in pr...</td>\n",
       "      <td>ChatLLM_Network.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306</th>\n",
       "      <td>ca2527a2-6594-4b63-9206-bc7d26a111af</td>\n",
       "      <td>S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. ...</td>\n",
       "      <td>Cognitive_Architectures_for_Language_Agents.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>307</th>\n",
       "      <td>ca2527a2-6594-4b63-9206-bc7d26a111af</td>\n",
       "      <td>Society, volume 45, 2023a.\\nT. Zhang, F. Liu, ...</td>\n",
       "      <td>Cognitive_Architectures_for_Language_Agents.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>308</th>\n",
       "      <td>ca2527a2-6594-4b63-9206-bc7d26a111af</td>\n",
       "      <td>31</td>\n",
       "      <td>Cognitive_Architectures_for_Language_Agents.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309</th>\n",
       "      <td>875aba78-cbf0-4519-95de-05fc7c299b5a</td>\n",
       "      <td>Published in Transactions on Machine Learning ...</td>\n",
       "      <td>Cognitive_Architectures_for_Language_Agents.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>875aba78-cbf0-4519-95de-05fc7c299b5a</td>\n",
       "      <td>32</td>\n",
       "      <td>Cognitive_Architectures_for_Language_Agents.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>311 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      id_  \\\n",
       "0    d8e107bc-267a-4918-9831-64028f5ff03f   \n",
       "1    d8e107bc-267a-4918-9831-64028f5ff03f   \n",
       "2    d8e107bc-267a-4918-9831-64028f5ff03f   \n",
       "3    d8e107bc-267a-4918-9831-64028f5ff03f   \n",
       "4    d8e107bc-267a-4918-9831-64028f5ff03f   \n",
       "..                                    ...   \n",
       "306  ca2527a2-6594-4b63-9206-bc7d26a111af   \n",
       "307  ca2527a2-6594-4b63-9206-bc7d26a111af   \n",
       "308  ca2527a2-6594-4b63-9206-bc7d26a111af   \n",
       "309  875aba78-cbf0-4519-95de-05fc7c299b5a   \n",
       "310  875aba78-cbf0-4519-95de-05fc7c299b5a   \n",
       "\n",
       "                                                  text  \\\n",
       "0    ChatLLM Network: More brains, More intelligenc...   \n",
       "1    Yirui Zhang\\n                                 ...   \n",
       "2    Abstract\\n                                    ...   \n",
       "3    ChatLLM network that allows multiple dialogue-...   \n",
       "4    network attains significant improvements in pr...   \n",
       "..                                                 ...   \n",
       "306  S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. ...   \n",
       "307  Society, volume 45, 2023a.\\nT. Zhang, F. Liu, ...   \n",
       "308                                                 31   \n",
       "309  Published in Transactions on Machine Learning ...   \n",
       "310                                                 32   \n",
       "\n",
       "                                  metadata_file_name metadata_creation_date  \\\n",
       "0                                ChatLLM_Network.pdf             2025-01-08   \n",
       "1                                ChatLLM_Network.pdf             2025-01-08   \n",
       "2                                ChatLLM_Network.pdf             2025-01-08   \n",
       "3                                ChatLLM_Network.pdf             2025-01-08   \n",
       "4                                ChatLLM_Network.pdf             2025-01-08   \n",
       "..                                               ...                    ...   \n",
       "306  Cognitive_Architectures_for_Language_Agents.pdf             2025-01-08   \n",
       "307  Cognitive_Architectures_for_Language_Agents.pdf             2025-01-08   \n",
       "308  Cognitive_Architectures_for_Language_Agents.pdf             2025-01-08   \n",
       "309  Cognitive_Architectures_for_Language_Agents.pdf             2025-01-08   \n",
       "310  Cognitive_Architectures_for_Language_Agents.pdf             2025-01-08   \n",
       "\n",
       "     metadata_pagenumber  \n",
       "0                      1  \n",
       "1                      1  \n",
       "2                      1  \n",
       "3                      1  \n",
       "4                      1  \n",
       "..                   ...  \n",
       "306                   31  \n",
       "307                   31  \n",
       "308                   31  \n",
       "309                   32  \n",
       "310                   32  \n",
       "\n",
       "[311 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(documents_list)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LanceDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lancedb\n",
      "  Using cached lancedb-0.17.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting deprecation (from lancedb)\n",
      "  Using cached deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "INFO: pip is looking at multiple versions of lancedb to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting lancedb\n",
      "  Using cached lancedb-0.17.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (4.7 kB)\n",
      "Collecting pylance==0.20.0 (from lancedb)\n",
      "  Using cached pylance-0.20.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from lancedb) (4.67.1)\n",
      "Requirement already satisfied: pydantic>=1.10 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from lancedb) (2.10.1)\n",
      "Requirement already satisfied: packaging in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from lancedb) (23.2)\n",
      "Collecting overrides>=0.7 (from lancedb)\n",
      "  Using cached overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting pyarrow>=14 (from pylance==0.20.0->lancedb)\n",
      "  Using cached pyarrow-18.1.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: numpy>=1.22 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from pylance==0.20.0->lancedb) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting torch>=1.11.0 (from sentence_transformers)\n",
      "  Using cached torch-2.5.1-cp311-none-macosx_11_0_arm64.whl.metadata (28 kB)\n",
      "Collecting scikit-learn (from sentence_transformers)\n",
      "  Using cached scikit_learn-1.6.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting scipy (from sentence_transformers)\n",
      "  Downloading scipy-1.15.0-cp311-cp311-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers)\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting Pillow (from sentence_transformers)\n",
      "  Downloading pillow-11.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.1 kB)\n",
      "Collecting filelock (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence_transformers)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from pydantic>=1.10->lancedb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from pydantic>=1.10->lancedb) (2.27.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting networkx (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.5)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence_transformers)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch>=1.11.0->sentence_transformers)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Downloading safetensors-0.5.1-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence_transformers)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.12.14)\n",
      "Using cached lancedb-0.17.0-cp39-abi3-macosx_11_0_arm64.whl (24.8 MB)\n",
      "Using cached pylance-0.20.0-cp39-abi3-macosx_11_0_arm64.whl (29.3 MB)\n",
      "Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Using cached overrides-7.7.0-py3-none-any.whl (17 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached torch-2.5.1-cp311-none-macosx_11_0_arm64.whl (63.9 MB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Using cached transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Using cached deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
      "Downloading pillow-11.1.0-cp311-cp311-macosx_11_0_arm64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m914.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scikit_learn-1.6.0-cp311-cp311-macosx_12_0_arm64.whl (11.1 MB)\n",
      "Downloading scipy-1.15.0-cp311-cp311-macosx_14_0_arm64.whl (24.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.8/24.8 MB\u001b[0m \u001b[31m868.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached pyarrow-18.1.0-cp311-cp311-macosx_12_0_arm64.whl (29.6 MB)\n",
      "Downloading safetensors-0.5.1-cp38-abi3-macosx_11_0_arm64.whl (408 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Using cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Installing collected packages: pytz, mpmath, tzdata, threadpoolctl, sympy, scipy, safetensors, pyarrow, Pillow, overrides, networkx, joblib, fsspec, filelock, deprecation, torch, scikit-learn, pylance, pandas, huggingface-hub, tokenizers, lancedb, transformers, sentence_transformers\n",
      "Successfully installed Pillow-11.1.0 deprecation-2.1.0 filelock-3.16.1 fsspec-2024.12.0 huggingface-hub-0.27.1 joblib-1.4.2 lancedb-0.17.0 mpmath-1.3.0 networkx-3.4.2 overrides-7.7.0 pandas-2.2.3 pyarrow-18.1.0 pylance-0.20.0 pytz-2024.2 safetensors-0.5.1 scikit-learn-1.6.0 scipy-1.15.0 sentence_transformers-3.3.1 sympy-1.13.1 threadpoolctl-3.5.0 tokenizers-0.21.0 torch-2.5.1 transformers-4.47.1 tzdata-2024.2\n"
     ]
    }
   ],
   "source": [
    "!pip install lancedb pandas sentence_transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miladmohammadi/Documents/NLP 6/NLP_CA6_Solutions/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import lancedb\n",
    "db = lancedb.connect(\".lancedb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the embedding function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lancedb.embeddings import get_registry\n",
    "embedding_model = get_registry().get(\"sentence-transformers\").create(name=\"BAAI/bge-small-en-v1.5\", device=\"mps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the data model or schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You should put HF_TOKEN in the Notebook enviroment variables\n",
    "from lancedb.pydantic import LanceModel, Vector\n",
    "\n",
    "class ChunksOfData(LanceModel):\n",
    "    text: str = embedding_model.SourceField()\n",
    "    metadata_file_name: str\n",
    "    metadata_creation_date: str\n",
    "    metadata_pagenumber: int\n",
    "    vector: Vector(embedding_model.ndims()) = embedding_model.VectorField()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create table and add data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_dict_batches(df: pd.DataFrame, batch_size: int = 128):\n",
    "    \"\"\"\n",
    "    Yields data from a DataFrame in batches of dictionaries.\n",
    "    Each batch is a list of dict, suitable for LanceDB ingestion.\n",
    "    \"\"\"\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = start_idx + batch_size\n",
    "        # Convert the batch of rows to a list of dict\n",
    "        batch_dicts = df.iloc[start_idx:end_idx].to_dict(orient=\"records\")\n",
    "        yield batch_dicts\n",
    "\n",
    "tbl = db.create_table(\n",
    "    \"embedded_chunks3\",\n",
    "    data=df_to_dict_batches(df, batch_size=10),\n",
    "    schema=ChunksOfData,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Querying your table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>metadata_file_name</th>\n",
       "      <th>metadata_creation_date</th>\n",
       "      <th>metadata_pagenumber</th>\n",
       "      <th>vector</th>\n",
       "      <th>_distance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>making process to choose actions. We use CoALA...</td>\n",
       "      <td>Cognitive_Architectures_for_Language_Agents.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.053947296, 0.03000264, -0.013586269, -0.02...</td>\n",
       "      <td>0.569758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Published in Transactions on Machine Learning ...</td>\n",
       "      <td>Cognitive_Architectures_for_Language_Agents.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>12</td>\n",
       "      <td>[-0.09369078, 0.025953628, 0.015643774, -0.011...</td>\n",
       "      <td>0.637318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Agent design: thinking beyond simple reasoning...</td>\n",
       "      <td>Cognitive_Architectures_for_Language_Agents.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>15</td>\n",
       "      <td>[-0.07036838, 0.036759418, -0.0069561503, -0.0...</td>\n",
       "      <td>0.643972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4\\n        Cognitive Architectures for Languag...</td>\n",
       "      <td>Cognitive_Architectures_for_Language_Agents.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>8</td>\n",
       "      <td>[-0.07170645, -0.004507213, -0.022157056, 0.01...</td>\n",
       "      <td>0.646239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Decision Procedure                            ...</td>\n",
       "      <td>Cognitive_Architectures_for_Language_Agents.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>8</td>\n",
       "      <td>[-0.06418777, 0.029837094, -0.022896353, -0.01...</td>\n",
       "      <td>0.665226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  making process to choose actions. We use CoALA...   \n",
       "1  Published in Transactions on Machine Learning ...   \n",
       "2  Agent design: thinking beyond simple reasoning...   \n",
       "3  4\\n        Cognitive Architectures for Languag...   \n",
       "4  Decision Procedure                            ...   \n",
       "\n",
       "                                metadata_file_name metadata_creation_date  \\\n",
       "0  Cognitive_Architectures_for_Language_Agents.pdf             2025-01-08   \n",
       "1  Cognitive_Architectures_for_Language_Agents.pdf             2025-01-08   \n",
       "2  Cognitive_Architectures_for_Language_Agents.pdf             2025-01-08   \n",
       "3  Cognitive_Architectures_for_Language_Agents.pdf             2025-01-08   \n",
       "4  Cognitive_Architectures_for_Language_Agents.pdf             2025-01-08   \n",
       "\n",
       "   metadata_pagenumber                                             vector  \\\n",
       "0                    1  [-0.053947296, 0.03000264, -0.013586269, -0.02...   \n",
       "1                   12  [-0.09369078, 0.025953628, 0.015643774, -0.011...   \n",
       "2                   15  [-0.07036838, 0.036759418, -0.0069561503, -0.0...   \n",
       "3                    8  [-0.07170645, -0.004507213, -0.022157056, 0.01...   \n",
       "4                    8  [-0.06418777, 0.029837094, -0.022896353, -0.01...   \n",
       "\n",
       "   _distance  \n",
       "0   0.569758  \n",
       "1   0.637318  \n",
       "2   0.643972  \n",
       "3   0.646239  \n",
       "4   0.665226  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What does CoALA stands for?\"\n",
    "#actual = table.search(query).limit(5).to_pydantic(Words)[0]\n",
    "res= tbl.search(query).limit(5).to_pandas()\n",
    "res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>metadata_file_name</th>\n",
       "      <th>metadata_creation_date</th>\n",
       "      <th>metadata_pagenumber</th>\n",
       "      <th>vector</th>\n",
       "      <th>_relevance_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Agent design: thinking beyond simple reasoning...</td>\n",
       "      <td>Cognitive_Architectures_for_Language_Agents.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>15</td>\n",
       "      <td>[-0.07036838, 0.036759418, -0.0069561503, -0.0...</td>\n",
       "      <td>0.031498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4\\n        Cognitive Architectures for Languag...</td>\n",
       "      <td>Cognitive_Architectures_for_Language_Agents.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>8</td>\n",
       "      <td>[-0.07170645, -0.004507213, -0.022157056, 0.01...</td>\n",
       "      <td>0.031498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>making process to choose actions. We use CoALA...</td>\n",
       "      <td>Cognitive_Architectures_for_Language_Agents.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>1</td>\n",
       "      <td>[-0.053947296, 0.03000264, -0.013586269, -0.02...</td>\n",
       "      <td>0.016393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>just choose their preferred framing, as long a...</td>\n",
       "      <td>Cognitive_Architectures_for_Language_Agents.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>18</td>\n",
       "      <td>[-0.04917023, -0.039094172, 0.012346595, 0.007...</td>\n",
       "      <td>0.016393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Published in Transactions on Machine Learning ...</td>\n",
       "      <td>Cognitive_Architectures_for_Language_Agents.pdf</td>\n",
       "      <td>2025-01-08</td>\n",
       "      <td>12</td>\n",
       "      <td>[-0.09369078, 0.025953628, 0.015643774, -0.011...</td>\n",
       "      <td>0.016129</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Agent design: thinking beyond simple reasoning...   \n",
       "1  4\\n        Cognitive Architectures for Languag...   \n",
       "2  making process to choose actions. We use CoALA...   \n",
       "3  just choose their preferred framing, as long a...   \n",
       "4  Published in Transactions on Machine Learning ...   \n",
       "\n",
       "                                metadata_file_name metadata_creation_date  \\\n",
       "0  Cognitive_Architectures_for_Language_Agents.pdf             2025-01-08   \n",
       "1  Cognitive_Architectures_for_Language_Agents.pdf             2025-01-08   \n",
       "2  Cognitive_Architectures_for_Language_Agents.pdf             2025-01-08   \n",
       "3  Cognitive_Architectures_for_Language_Agents.pdf             2025-01-08   \n",
       "4  Cognitive_Architectures_for_Language_Agents.pdf             2025-01-08   \n",
       "\n",
       "   metadata_pagenumber                                             vector  \\\n",
       "0                   15  [-0.07036838, 0.036759418, -0.0069561503, -0.0...   \n",
       "1                    8  [-0.07170645, -0.004507213, -0.022157056, 0.01...   \n",
       "2                    1  [-0.053947296, 0.03000264, -0.013586269, -0.02...   \n",
       "3                   18  [-0.04917023, -0.039094172, 0.012346595, 0.007...   \n",
       "4                   12  [-0.09369078, 0.025953628, 0.015643774, -0.011...   \n",
       "\n",
       "   _relevance_score  \n",
       "0          0.031498  \n",
       "1          0.031498  \n",
       "2          0.016393  \n",
       "3          0.016393  \n",
       "4          0.016129  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What does CoALA stands for?\"\n",
    "tbl.create_fts_index('text', use_tantivy=False)\n",
    "tbl.search(query, query_type=\"hybrid\").limit(5).to_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "AVALAI_BASE_URL = \"https://api.avalai.ir/v1\"\n",
    "GPT_MODEL_NAME = \"gpt-4o-mini\"\n",
    "\n",
    "gpt4o_chat = ChatOpenAI(model=GPT_MODEL_NAME,\n",
    "                        base_url=AVALAI_BASE_URL,\n",
    "                        api_key=os.environ[\"AVALAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Agent design: thinking beyond simple reasoning. CoALA defines agents over three distinct concepts: (i)\\ninternal memory, (ii) a set of possible internal and external actions, and (iii) a decision making procedure over\\nthose actions. Using CoALA to develop an application-specific agent consists of specifying implementations\\nfor each of these components in turn. We assume that the agent’s environment and external action space are\\ngiven, and show how CoALA can be used to determine an appropriate high-level architecture. For example,\\nwe can imagine designing a personalized retail assistant (Yao et al., 2022a) that helps users find relevant items\\nbased on their queries and purchasing history. In this case, the external actions would consist of dialogue or\\nreturning search results to the user.',\n",
       "  'metadata_file_name': 'Cognitive_Architectures_for_Language_Agents.pdf',\n",
       "  'metadata_creation_date': '2025-01-08',\n",
       "  'metadata_pagenumber': 15,\n",
       "  'vector': [-0.07036837935447693,\n",
       "   0.03675941750407219,\n",
       "   -0.006956150289624929,\n",
       "   -0.039191365242004395,\n",
       "   -0.06220829486846924,\n",
       "   0.002021811669692397,\n",
       "   0.020763637498021126,\n",
       "   0.007920507341623306,\n",
       "   -0.0014996349345892668,\n",
       "   0.024335062131285667,\n",
       "   0.02044363133609295,\n",
       "   0.01241031289100647,\n",
       "   -0.014844960533082485,\n",
       "   0.010066363029181957,\n",
       "   0.023019062355160713,\n",
       "   0.016695499420166016,\n",
       "   -0.042740482836961746,\n",
       "   0.025613881647586823,\n",
       "   0.043840110301971436,\n",
       "   -0.018555451184511185,\n",
       "   0.041020311415195465,\n",
       "   -0.013675875961780548,\n",
       "   -0.021025341004133224,\n",
       "   -0.006588329095393419,\n",
       "   -0.07469949126243591,\n",
       "   0.0026696573477238417,\n",
       "   -0.05803203955292702,\n",
       "   -0.019479386508464813,\n",
       "   -0.031497772783041,\n",
       "   -0.16156290471553802,\n",
       "   0.04046015813946724,\n",
       "   0.023490862920880318,\n",
       "   0.05618240311741829,\n",
       "   -0.004351015202701092,\n",
       "   -0.06763970106840134,\n",
       "   0.022547297179698944,\n",
       "   -0.01887040212750435,\n",
       "   0.00829094648361206,\n",
       "   -0.024275073781609535,\n",
       "   0.024361923336982727,\n",
       "   -0.002964481245726347,\n",
       "   0.029782677069306374,\n",
       "   0.013527179136872292,\n",
       "   0.012335684150457382,\n",
       "   -0.04465799033641815,\n",
       "   -0.06353648006916046,\n",
       "   0.026361830532550812,\n",
       "   0.00300611462444067,\n",
       "   -0.11433874815702438,\n",
       "   -0.014144131913781166,\n",
       "   0.028869928792119026,\n",
       "   -0.010142805054783821,\n",
       "   -0.02969568409025669,\n",
       "   0.03359297662973404,\n",
       "   0.01599200814962387,\n",
       "   0.05579632893204689,\n",
       "   0.07239387929439545,\n",
       "   0.050716549158096313,\n",
       "   0.023725422099232674,\n",
       "   0.00042910303454846144,\n",
       "   0.0970284640789032,\n",
       "   0.041367169469594955,\n",
       "   -0.09893158078193665,\n",
       "   0.0847952663898468,\n",
       "   0.030076883733272552,\n",
       "   0.051268335431814194,\n",
       "   -0.037228040397167206,\n",
       "   -0.04681747779250145,\n",
       "   0.03699161857366562,\n",
       "   0.0698886513710022,\n",
       "   -0.007942896336317062,\n",
       "   0.002303364221006632,\n",
       "   0.025643829256296158,\n",
       "   0.047577884048223495,\n",
       "   0.03212320804595947,\n",
       "   0.06668303161859512,\n",
       "   -0.04308832809329033,\n",
       "   -0.02315647341310978,\n",
       "   0.055443622171878815,\n",
       "   -0.07875270396471024,\n",
       "   -0.03883527219295502,\n",
       "   0.005020700395107269,\n",
       "   0.00987066887319088,\n",
       "   0.011631354689598083,\n",
       "   -0.050135333091020584,\n",
       "   -0.035685859620571136,\n",
       "   -0.016098126769065857,\n",
       "   -0.0005369039135985076,\n",
       "   0.025045763701200485,\n",
       "   0.005168048664927483,\n",
       "   0.007761682383716106,\n",
       "   0.013705024495720863,\n",
       "   -0.022617846727371216,\n",
       "   -0.006812948267906904,\n",
       "   0.01170553918927908,\n",
       "   -0.022649018093943596,\n",
       "   -0.04377048835158348,\n",
       "   0.020776361227035522,\n",
       "   -0.05385088920593262,\n",
       "   0.41303202509880066,\n",
       "   0.00293117412365973,\n",
       "   0.004132147412747145,\n",
       "   0.01323871873319149,\n",
       "   -0.027287432923913002,\n",
       "   -0.02565506100654602,\n",
       "   0.01505575142800808,\n",
       "   -0.03179626911878586,\n",
       "   -0.05081585794687271,\n",
       "   -0.04063740000128746,\n",
       "   0.03216080367565155,\n",
       "   0.008589920587837696,\n",
       "   -0.018336838111281395,\n",
       "   0.008805293589830399,\n",
       "   0.005210379604250193,\n",
       "   -0.002657909644767642,\n",
       "   -0.05966515839099884,\n",
       "   -0.0021003950387239456,\n",
       "   0.014531928114593029,\n",
       "   0.04652633145451546,\n",
       "   -0.04220176860690117,\n",
       "   -0.01841837726533413,\n",
       "   0.014596579596400261,\n",
       "   0.04490438476204872,\n",
       "   -0.028871675953269005,\n",
       "   -0.02384648658335209,\n",
       "   -0.02147156186401844,\n",
       "   0.012855490669608116,\n",
       "   0.0631428062915802,\n",
       "   -0.0942031517624855,\n",
       "   0.028108755126595497,\n",
       "   0.06663785129785538,\n",
       "   0.020984428003430367,\n",
       "   0.002537553897127509,\n",
       "   0.06136513873934746,\n",
       "   0.0630129873752594,\n",
       "   0.021608348935842514,\n",
       "   0.026657579466700554,\n",
       "   0.07236558943986893,\n",
       "   -0.04258446395397186,\n",
       "   0.03490332514047623,\n",
       "   -0.04424527287483215,\n",
       "   0.08141931146383286,\n",
       "   0.020124778151512146,\n",
       "   -0.020612601190805435,\n",
       "   -0.10634423792362213,\n",
       "   0.03855545446276665,\n",
       "   0.06253687292337418,\n",
       "   -0.01661926507949829,\n",
       "   -0.02643602527678013,\n",
       "   0.02389669418334961,\n",
       "   -0.04521770030260086,\n",
       "   0.07675500214099884,\n",
       "   0.02411889098584652,\n",
       "   -0.01312835793942213,\n",
       "   -0.028668051585555077,\n",
       "   0.021989116445183754,\n",
       "   0.05986309051513672,\n",
       "   -0.033090539276599884,\n",
       "   -0.03466157242655754,\n",
       "   0.0023750117979943752,\n",
       "   -0.025546520948410034,\n",
       "   -0.03500410169363022,\n",
       "   -0.03695712238550186,\n",
       "   0.06861748546361923,\n",
       "   -0.020113669335842133,\n",
       "   -0.10051268339157104,\n",
       "   -0.003031717613339424,\n",
       "   0.016383621841669083,\n",
       "   0.02372165583074093,\n",
       "   0.03302061930298805,\n",
       "   0.011324322782456875,\n",
       "   0.03170717880129814,\n",
       "   -0.05214358866214752,\n",
       "   -0.04665616154670715,\n",
       "   -0.016484640538692474,\n",
       "   0.014799915254116058,\n",
       "   -0.048239655792713165,\n",
       "   0.025985101237893105,\n",
       "   0.028394360095262527,\n",
       "   0.007216502446681261,\n",
       "   0.026688382029533386,\n",
       "   -2.9649422685906757e-06,\n",
       "   0.034762777388095856,\n",
       "   -0.006959769409149885,\n",
       "   -0.013638363219797611,\n",
       "   -0.04949319735169411,\n",
       "   -0.02337532490491867,\n",
       "   -0.008262568153440952,\n",
       "   0.009031446650624275,\n",
       "   0.02646813541650772,\n",
       "   -0.07155469059944153,\n",
       "   0.020648641511797905,\n",
       "   -0.002430205000564456,\n",
       "   0.030040036886930466,\n",
       "   -0.04593002796173096,\n",
       "   -0.018767278641462326,\n",
       "   0.01892460510134697,\n",
       "   -0.015778755769133568,\n",
       "   -0.05667445436120033,\n",
       "   0.010844727978110313,\n",
       "   0.024390147998929024,\n",
       "   0.05821668356657028,\n",
       "   -0.09656793624162674,\n",
       "   -0.020464027300477028,\n",
       "   -0.0007529273279942572,\n",
       "   -0.04126352816820145,\n",
       "   0.02872427925467491,\n",
       "   -0.030338823795318604,\n",
       "   0.015092375688254833,\n",
       "   -0.03873100504279137,\n",
       "   -0.046378590166568756,\n",
       "   0.037388503551483154,\n",
       "   -0.007042290642857552,\n",
       "   0.03810390084981918,\n",
       "   -0.016860220581293106,\n",
       "   -0.016688283532857895,\n",
       "   0.037528470158576965,\n",
       "   0.048206306993961334,\n",
       "   0.03707576543092728,\n",
       "   -0.036715131253004074,\n",
       "   0.0023554502986371517,\n",
       "   -0.003891783533617854,\n",
       "   0.007822556421160698,\n",
       "   -0.3196271061897278,\n",
       "   -0.0552433617413044,\n",
       "   -0.026476573199033737,\n",
       "   -0.04159972071647644,\n",
       "   -0.0018374669598415494,\n",
       "   -0.010348713956773281,\n",
       "   0.010973001830279827,\n",
       "   0.01598215661942959,\n",
       "   -0.03708207607269287,\n",
       "   0.013079301454126835,\n",
       "   0.08387227356433868,\n",
       "   -0.04877233877778053,\n",
       "   -0.01318910252302885,\n",
       "   0.009580210782587528,\n",
       "   -0.002283926587551832,\n",
       "   0.04556576535105705,\n",
       "   -0.016085820272564888,\n",
       "   0.06106771528720856,\n",
       "   -0.10506882518529892,\n",
       "   0.03893386945128441,\n",
       "   0.024026205763220787,\n",
       "   -0.003988505341112614,\n",
       "   0.037474390119314194,\n",
       "   -0.09659336507320404,\n",
       "   -0.03619372099637985,\n",
       "   0.03064446523785591,\n",
       "   0.15822292864322662,\n",
       "   -0.032142117619514465,\n",
       "   0.041499048471450806,\n",
       "   0.013638278469443321,\n",
       "   0.054967839270830154,\n",
       "   0.02688000537455082,\n",
       "   -0.07075610011816025,\n",
       "   -0.061662741005420685,\n",
       "   0.024396954104304314,\n",
       "   -0.006310548633337021,\n",
       "   0.016760604456067085,\n",
       "   0.004547987598925829,\n",
       "   0.030643608421087265,\n",
       "   -0.05568023398518562,\n",
       "   0.01596425101161003,\n",
       "   0.024331605061888695,\n",
       "   -0.018985792994499207,\n",
       "   -0.0217446181923151,\n",
       "   -0.02392425946891308,\n",
       "   -0.022055000066757202,\n",
       "   -0.014613603241741657,\n",
       "   0.026471666991710663,\n",
       "   0.006489288993179798,\n",
       "   -0.04123691841959953,\n",
       "   0.014812187291681767,\n",
       "   0.026012955233454704,\n",
       "   -0.037958547472953796,\n",
       "   0.027420910075306892,\n",
       "   -0.0177202969789505,\n",
       "   -0.05246185138821602,\n",
       "   -0.03026176057755947,\n",
       "   0.016989678144454956,\n",
       "   -0.0592406764626503,\n",
       "   0.05197175592184067,\n",
       "   -0.019439488649368286,\n",
       "   -0.017904557287693024,\n",
       "   0.033614933490753174,\n",
       "   -0.00416558375582099,\n",
       "   0.04268725588917732,\n",
       "   -0.07238339632749557,\n",
       "   0.0710512176156044,\n",
       "   -0.03806346282362938,\n",
       "   0.003648874582722783,\n",
       "   -0.0017268883530050516,\n",
       "   -0.02353307232260704,\n",
       "   0.07806143909692764,\n",
       "   -0.005765270907431841,\n",
       "   -0.031078888103365898,\n",
       "   0.045323122292757034,\n",
       "   -0.06523721665143967,\n",
       "   0.03125418350100517,\n",
       "   -0.023166241124272346,\n",
       "   0.0049026040360331535,\n",
       "   0.05760210007429123,\n",
       "   -0.036883026361465454,\n",
       "   -0.08557499945163727,\n",
       "   0.03570855036377907,\n",
       "   0.011093012988567352,\n",
       "   0.022584883496165276,\n",
       "   -0.0016643968410789967,\n",
       "   0.014035072177648544,\n",
       "   -0.10748051851987839,\n",
       "   0.05048058554530144,\n",
       "   0.007139553315937519,\n",
       "   -0.015840858221054077,\n",
       "   0.014819648116827011,\n",
       "   -0.06048574298620224,\n",
       "   -0.039001449942588806,\n",
       "   -0.007429447956383228,\n",
       "   -0.04354802519083023,\n",
       "   -0.25547370314598083,\n",
       "   0.03295687586069107,\n",
       "   -0.02188267931342125,\n",
       "   0.0545835979282856,\n",
       "   0.022241249680519104,\n",
       "   -0.009443122893571854,\n",
       "   -0.0476178303360939,\n",
       "   -0.016965890303254128,\n",
       "   0.00032969730091281235,\n",
       "   -0.006510904524475336,\n",
       "   0.006808340083807707,\n",
       "   0.03582373261451721,\n",
       "   0.003167829941958189,\n",
       "   0.02894587628543377,\n",
       "   -0.018491843715310097,\n",
       "   -0.01755315624177456,\n",
       "   0.10720624029636383,\n",
       "   -0.0012430442729964852,\n",
       "   0.06279131770133972,\n",
       "   -0.008026502095162868,\n",
       "   0.011581875383853912,\n",
       "   0.025544892996549606,\n",
       "   0.16943059861660004,\n",
       "   -0.015026229433715343,\n",
       "   0.05306386202573776,\n",
       "   0.04258965700864792,\n",
       "   0.02881731279194355,\n",
       "   -0.0637795701622963,\n",
       "   -0.0442207008600235,\n",
       "   0.03186224028468132,\n",
       "   0.039727747440338135,\n",
       "   -0.03673359751701355,\n",
       "   0.12753871083259583,\n",
       "   -0.012603129260241985,\n",
       "   0.05321185290813446,\n",
       "   -0.001206467393785715,\n",
       "   0.02881520614027977,\n",
       "   0.0032418151386082172,\n",
       "   0.025991668924689293,\n",
       "   0.005524451844394207,\n",
       "   -0.04134342819452286,\n",
       "   0.004399202764034271,\n",
       "   0.03442683443427086,\n",
       "   0.007329560350626707,\n",
       "   0.031035367399454117,\n",
       "   0.04851958528161049,\n",
       "   -0.04718228429555893,\n",
       "   -0.03068939968943596,\n",
       "   -0.06789234280586243,\n",
       "   0.0279809832572937,\n",
       "   0.006459192838519812,\n",
       "   0.008517269976437092,\n",
       "   -0.03161809220910072,\n",
       "   -0.02994481474161148,\n",
       "   0.0651659294962883,\n",
       "   -0.020472615957260132,\n",
       "   -0.004700607154518366,\n",
       "   0.010085886344313622,\n",
       "   -0.0033598621375858784,\n",
       "   -0.0002776718174573034,\n",
       "   0.01989675499498844,\n",
       "   0.035043664276599884,\n",
       "   0.061216261237859726,\n",
       "   -0.014995824545621872,\n",
       "   0.01805153861641884],\n",
       "  '_relevance_score': 0.0314980149269104},\n",
       " {'text': '4\\n        Cognitive Architectures for Language Agents (CoALA): A Conceptual Framework\\nWe present Cognitive Architectures for Language Agents (CoALA) as a framework to organize existing                                                                                                                                Reasoning\\n language agents and guide the development of new ones. CoALA positions the LLM as the core component\\n of a larger cognitive architecture (Figure 4). Under CoALA, a language agent stores information in memory\\n modules (Section 4.1), and acts in an action space structured into external and internal parts (Figure 5):',\n",
       "  'metadata_file_name': 'Cognitive_Architectures_for_Language_Agents.pdf',\n",
       "  'metadata_creation_date': '2025-01-08',\n",
       "  'metadata_pagenumber': 8,\n",
       "  'vector': [-0.07170645147562027,\n",
       "   -0.004507212899625301,\n",
       "   -0.022157056257128716,\n",
       "   0.016637742519378662,\n",
       "   -0.054436326026916504,\n",
       "   0.052462223917245865,\n",
       "   0.004478981718420982,\n",
       "   -0.01972648687660694,\n",
       "   0.046788353472948074,\n",
       "   -0.024918271228671074,\n",
       "   0.023859592154622078,\n",
       "   -0.004508594051003456,\n",
       "   0.013129452243447304,\n",
       "   0.040348298847675323,\n",
       "   0.06054432690143585,\n",
       "   0.02821381762623787,\n",
       "   -0.06960436701774597,\n",
       "   0.03875057399272919,\n",
       "   0.031121762469410896,\n",
       "   -0.02607620321214199,\n",
       "   0.05528179183602333,\n",
       "   -0.026079364120960236,\n",
       "   0.022906506434082985,\n",
       "   0.01790473237633705,\n",
       "   -0.08271096646785736,\n",
       "   0.010132557712495327,\n",
       "   -0.07475558668375015,\n",
       "   -0.0045415121130645275,\n",
       "   -0.014709874987602234,\n",
       "   -0.19376222789287567,\n",
       "   0.014018849469721317,\n",
       "   0.011891955509781837,\n",
       "   0.06082747131586075,\n",
       "   -0.051944706588983536,\n",
       "   -0.055293478071689606,\n",
       "   0.011184146627783775,\n",
       "   -0.021845847368240356,\n",
       "   0.011748217046260834,\n",
       "   -0.036533456295728683,\n",
       "   0.022240206599235535,\n",
       "   0.01697932370007038,\n",
       "   0.056422699242830276,\n",
       "   0.038303278386592865,\n",
       "   -0.021874457597732544,\n",
       "   -0.007184676826000214,\n",
       "   -0.06860174983739853,\n",
       "   -0.03644558787345886,\n",
       "   0.019524989649653435,\n",
       "   -0.10588379204273224,\n",
       "   -0.05943826958537102,\n",
       "   0.02057173103094101,\n",
       "   -0.025232167914509773,\n",
       "   -0.015557423233985901,\n",
       "   0.04496968910098076,\n",
       "   0.007208235561847687,\n",
       "   0.06709311157464981,\n",
       "   0.07431069016456604,\n",
       "   0.032826926559209824,\n",
       "   0.03353302180767059,\n",
       "   0.024243587628006935,\n",
       "   0.08833638578653336,\n",
       "   0.05310961231589317,\n",
       "   -0.12774509191513062,\n",
       "   0.08630474656820297,\n",
       "   0.0017065586289390922,\n",
       "   0.037528641521930695,\n",
       "   -0.02582615427672863,\n",
       "   -0.032227013260126114,\n",
       "   0.03611621633172035,\n",
       "   0.05433005839586258,\n",
       "   -0.006938934791833162,\n",
       "   0.02192341722548008,\n",
       "   0.07414664328098297,\n",
       "   0.002911446848884225,\n",
       "   0.0332692414522171,\n",
       "   0.05129626765847206,\n",
       "   -0.013279289938509464,\n",
       "   -0.01967371441423893,\n",
       "   0.07162433862686157,\n",
       "   -0.06958766281604767,\n",
       "   -0.019457412883639336,\n",
       "   -0.0020728257950395346,\n",
       "   0.009808238595724106,\n",
       "   -0.01684022694826126,\n",
       "   -0.015743626281619072,\n",
       "   -0.02201620116829872,\n",
       "   -0.07658933848142624,\n",
       "   -0.013846470974385738,\n",
       "   -0.035024337470531464,\n",
       "   0.004487929400056601,\n",
       "   -0.015800053253769875,\n",
       "   0.005075804423540831,\n",
       "   0.000654098519589752,\n",
       "   0.03445044159889221,\n",
       "   0.01171873789280653,\n",
       "   -0.03392232581973076,\n",
       "   -0.04965526983141899,\n",
       "   0.027371566742658615,\n",
       "   -0.04157980531454086,\n",
       "   0.39387163519859314,\n",
       "   -0.006501862779259682,\n",
       "   -0.018106352537870407,\n",
       "   0.02434835582971573,\n",
       "   -0.012103310786187649,\n",
       "   -0.013487700372934341,\n",
       "   0.01292412169277668,\n",
       "   -0.03907947987318039,\n",
       "   -0.04928490146994591,\n",
       "   -0.03394526615738869,\n",
       "   0.020393066108226776,\n",
       "   -0.005525631830096245,\n",
       "   -0.023951776325702667,\n",
       "   0.02883336879312992,\n",
       "   0.009540862403810024,\n",
       "   0.029782041907310486,\n",
       "   -0.07230240106582642,\n",
       "   0.033242665231227875,\n",
       "   0.011996300891041756,\n",
       "   0.02284114994108677,\n",
       "   -0.03050803951919079,\n",
       "   -0.025363042950630188,\n",
       "   0.008493348956108093,\n",
       "   0.04289453476667404,\n",
       "   -0.01907225511968136,\n",
       "   -0.0005490295588970184,\n",
       "   -0.026646653190255165,\n",
       "   0.00870595220476389,\n",
       "   0.0750657245516777,\n",
       "   -0.012913186103105545,\n",
       "   0.03270242363214493,\n",
       "   0.03803957998752594,\n",
       "   0.04550148546695709,\n",
       "   0.013795881532132626,\n",
       "   -0.0025189288426190615,\n",
       "   0.052356116473674774,\n",
       "   0.04230644553899765,\n",
       "   0.057785872370004654,\n",
       "   -0.011670571751892567,\n",
       "   -0.04861373081803322,\n",
       "   0.02465069480240345,\n",
       "   -0.01795796863734722,\n",
       "   0.01506879087537527,\n",
       "   0.04811648651957512,\n",
       "   -0.021485211327672005,\n",
       "   -0.0784136950969696,\n",
       "   0.07914867997169495,\n",
       "   0.016166426241397858,\n",
       "   -0.017998743802309036,\n",
       "   -0.01429263036698103,\n",
       "   0.05787953361868858,\n",
       "   -0.033241935074329376,\n",
       "   0.08001784235239029,\n",
       "   0.03862595558166504,\n",
       "   -0.001431186799891293,\n",
       "   -0.03474859893321991,\n",
       "   0.02498956024646759,\n",
       "   0.003332289168611169,\n",
       "   -0.004198127891868353,\n",
       "   -0.035389143973588943,\n",
       "   -0.009910809807479382,\n",
       "   -0.03762517124414444,\n",
       "   -0.017712585628032684,\n",
       "   -0.035932593047618866,\n",
       "   0.09679923206567764,\n",
       "   -0.02219787985086441,\n",
       "   -0.09838852286338806,\n",
       "   -0.033615246415138245,\n",
       "   -0.0013288718182593584,\n",
       "   0.043955493718385696,\n",
       "   -0.00910219457000494,\n",
       "   0.03035392425954342,\n",
       "   0.046488288789987564,\n",
       "   0.007134441286325455,\n",
       "   -0.04782136157155037,\n",
       "   -0.03709406778216362,\n",
       "   0.029986513778567314,\n",
       "   -0.0746876522898674,\n",
       "   0.0033366777934134007,\n",
       "   -0.004387159366160631,\n",
       "   0.015224459581077099,\n",
       "   0.038411371409893036,\n",
       "   -0.02099943719804287,\n",
       "   0.024631408974528313,\n",
       "   -0.0011824779212474823,\n",
       "   0.002711346372961998,\n",
       "   -0.02546393685042858,\n",
       "   -0.04345367103815079,\n",
       "   -0.05557512864470482,\n",
       "   0.04038149118423462,\n",
       "   -0.02886052615940571,\n",
       "   -0.0769803524017334,\n",
       "   0.048771344125270844,\n",
       "   0.0209890678524971,\n",
       "   0.02039986662566662,\n",
       "   -0.04656507819890976,\n",
       "   -0.04867779091000557,\n",
       "   0.012923053465783596,\n",
       "   -0.03843224048614502,\n",
       "   -0.07534269243478775,\n",
       "   0.03158814087510109,\n",
       "   0.004977942910045385,\n",
       "   0.07550915330648422,\n",
       "   -0.07315223664045334,\n",
       "   -0.01515901368111372,\n",
       "   -0.0013315813848748803,\n",
       "   -0.03471720218658447,\n",
       "   0.020227011293172836,\n",
       "   -0.008420041762292385,\n",
       "   0.03499383106827736,\n",
       "   -0.011056104674935341,\n",
       "   -0.041015684604644775,\n",
       "   0.007267070468515158,\n",
       "   0.01681295782327652,\n",
       "   0.04173896089196205,\n",
       "   -0.023084575310349464,\n",
       "   -0.01741977408528328,\n",
       "   0.025397147983312607,\n",
       "   -0.004424418322741985,\n",
       "   0.0008753486326895654,\n",
       "   -0.019625207409262657,\n",
       "   -0.012623334303498268,\n",
       "   -0.01612183451652527,\n",
       "   -0.04380615055561066,\n",
       "   -0.3132856786251068,\n",
       "   -0.040780626237392426,\n",
       "   0.009657444432377815,\n",
       "   0.023565789684653282,\n",
       "   -0.006360574159771204,\n",
       "   -0.023682719096541405,\n",
       "   0.00661431672051549,\n",
       "   0.03170764446258545,\n",
       "   0.013257883489131927,\n",
       "   0.036985110491514206,\n",
       "   0.035348884761333466,\n",
       "   -0.042566120624542236,\n",
       "   -0.024456430226564407,\n",
       "   -0.026465099304914474,\n",
       "   -0.021988870576024055,\n",
       "   0.0428500771522522,\n",
       "   0.020925354212522507,\n",
       "   0.05599203333258629,\n",
       "   -0.05364234372973442,\n",
       "   0.06829633563756943,\n",
       "   0.039708469063043594,\n",
       "   -0.018105654045939445,\n",
       "   0.01469809003174305,\n",
       "   -0.07727432996034622,\n",
       "   0.0027490630745887756,\n",
       "   0.004090787377208471,\n",
       "   0.15544505417346954,\n",
       "   -0.035273581743240356,\n",
       "   0.06652206927537918,\n",
       "   0.008149584755301476,\n",
       "   0.02218768373131752,\n",
       "   0.06387705355882645,\n",
       "   -0.05368800833821297,\n",
       "   -0.045523617416620255,\n",
       "   0.056603241711854935,\n",
       "   -0.008695128373801708,\n",
       "   -0.002371452748775482,\n",
       "   -0.03673563525080681,\n",
       "   0.05408318340778351,\n",
       "   -0.08657975494861603,\n",
       "   0.00840459018945694,\n",
       "   0.01765170879662037,\n",
       "   -0.008001874200999737,\n",
       "   -0.05990869179368019,\n",
       "   -0.033014073967933655,\n",
       "   0.015476895496249199,\n",
       "   0.00029247108614072204,\n",
       "   0.004551578313112259,\n",
       "   0.005235849414020777,\n",
       "   -0.017944682389497757,\n",
       "   0.027035312727093697,\n",
       "   0.0034631378948688507,\n",
       "   -0.00029046463896520436,\n",
       "   0.06611891090869904,\n",
       "   -0.053358256816864014,\n",
       "   -0.033700015395879745,\n",
       "   -0.032425083220005035,\n",
       "   -0.007871733047068119,\n",
       "   -0.062424711883068085,\n",
       "   0.03829120099544525,\n",
       "   0.017428221181035042,\n",
       "   0.00533760990947485,\n",
       "   0.015262428671121597,\n",
       "   0.011476286686956882,\n",
       "   0.04230494424700737,\n",
       "   -0.04462239518761635,\n",
       "   0.04339179769158363,\n",
       "   -0.039634086191654205,\n",
       "   0.010526373982429504,\n",
       "   -0.0008026188006624579,\n",
       "   -0.040605366230010986,\n",
       "   0.08048532903194427,\n",
       "   -0.02067234180867672,\n",
       "   -0.015280789695680141,\n",
       "   0.04412243142724037,\n",
       "   0.004918275400996208,\n",
       "   -0.002143730176612735,\n",
       "   -0.032000359147787094,\n",
       "   0.012857749126851559,\n",
       "   0.047350917011499405,\n",
       "   0.0404190868139267,\n",
       "   -0.05529623106122017,\n",
       "   0.03242978826165199,\n",
       "   -0.010228457860648632,\n",
       "   0.023497050628066063,\n",
       "   0.0094406483694911,\n",
       "   0.03384009748697281,\n",
       "   -0.06919785588979721,\n",
       "   0.047144532203674316,\n",
       "   0.03655751049518585,\n",
       "   -0.0039746761322021484,\n",
       "   0.045642539858818054,\n",
       "   -0.026482248678803444,\n",
       "   -0.03525667265057564,\n",
       "   4.3015133996959776e-05,\n",
       "   -0.01864679716527462,\n",
       "   -0.2801424264907837,\n",
       "   0.03461909294128418,\n",
       "   -0.012932801619172096,\n",
       "   0.041926778852939606,\n",
       "   -0.020673442631959915,\n",
       "   0.014299454167485237,\n",
       "   -0.02737969346344471,\n",
       "   -0.017919357866048813,\n",
       "   -0.008086427114903927,\n",
       "   -0.02518395334482193,\n",
       "   0.0012471141526475549,\n",
       "   0.05188412219285965,\n",
       "   0.02581694722175598,\n",
       "   0.02031644433736801,\n",
       "   -0.0546470582485199,\n",
       "   -0.03178107738494873,\n",
       "   0.08486175537109375,\n",
       "   -0.023275457322597504,\n",
       "   0.05726194381713867,\n",
       "   -0.001421833410859108,\n",
       "   -0.02347698248922825,\n",
       "   0.0632958635687828,\n",
       "   0.18404948711395264,\n",
       "   0.010812843218445778,\n",
       "   0.03551100194454193,\n",
       "   0.015789136290550232,\n",
       "   0.035542264580726624,\n",
       "   -0.049248602241277695,\n",
       "   -0.03637239336967468,\n",
       "   0.02059618942439556,\n",
       "   -0.0003425096219871193,\n",
       "   -0.026187928393483162,\n",
       "   0.15427760779857635,\n",
       "   0.0006984254578128457,\n",
       "   -0.015398960560560226,\n",
       "   0.014552063308656216,\n",
       "   -0.0006440969882532954,\n",
       "   -0.029852665960788727,\n",
       "   0.06056157499551773,\n",
       "   0.0018400652334094048,\n",
       "   -0.02467731013894081,\n",
       "   0.023811398074030876,\n",
       "   -0.018380416557192802,\n",
       "   -0.01840824820101261,\n",
       "   0.013930712826550007,\n",
       "   0.03330439701676369,\n",
       "   0.013024494051933289,\n",
       "   -0.09380931407213211,\n",
       "   -0.07474682480096817,\n",
       "   0.011348231695592403,\n",
       "   0.02434684894979,\n",
       "   -0.026310978457331657,\n",
       "   0.0003077246365137398,\n",
       "   -0.013986201956868172,\n",
       "   0.042647335678339005,\n",
       "   -0.033456120640039444,\n",
       "   -0.007630941923707724,\n",
       "   -0.024864420294761658,\n",
       "   -0.006278143264353275,\n",
       "   -0.04193856567144394,\n",
       "   -0.030176280066370964,\n",
       "   -0.023905105888843536,\n",
       "   0.05767843499779701,\n",
       "   0.043349795043468475,\n",
       "   -0.020780233666300774],\n",
       "  '_relevance_score': 0.0314980149269104},\n",
       " {'text': 'making process to choose actions. We use CoALA to retrospectively survey and organize\\n                            a large body of recent work, and prospectively identify actionable directions towards more\\n                            capable agents. Taken together, CoALA contextualizes today’s language agents within the\\n                            broader history of AI and outlines a path towards language-based general intelligence.',\n",
       "  'metadata_file_name': 'Cognitive_Architectures_for_Language_Agents.pdf',\n",
       "  'metadata_creation_date': '2025-01-08',\n",
       "  'metadata_pagenumber': 1,\n",
       "  'vector': [-0.05394729599356651,\n",
       "   0.030002640560269356,\n",
       "   -0.01358626876026392,\n",
       "   -0.02900652401149273,\n",
       "   -0.05327117070555687,\n",
       "   0.04386672377586365,\n",
       "   0.018599245697259903,\n",
       "   -0.021821195259690285,\n",
       "   0.029787961393594742,\n",
       "   -0.002875783247873187,\n",
       "   -0.012511156499385834,\n",
       "   0.011424174532294273,\n",
       "   -0.06575390696525574,\n",
       "   0.033788736909627914,\n",
       "   0.01826859451830387,\n",
       "   0.05028984695672989,\n",
       "   -0.05002906173467636,\n",
       "   0.03184584528207779,\n",
       "   -0.022980500012636185,\n",
       "   -0.03523321449756622,\n",
       "   0.008894362486898899,\n",
       "   -0.010299225337803364,\n",
       "   0.041305653750896454,\n",
       "   0.0004286930779926479,\n",
       "   -0.06691087037324905,\n",
       "   0.02039889246225357,\n",
       "   -0.05858173221349716,\n",
       "   -0.019857021048665047,\n",
       "   -0.021273571997880936,\n",
       "   -0.17886973917484283,\n",
       "   0.04933886229991913,\n",
       "   0.027058755978941917,\n",
       "   0.08642733097076416,\n",
       "   -0.04620048403739929,\n",
       "   -0.06285374611616135,\n",
       "   0.05308069661259651,\n",
       "   -0.011512961238622665,\n",
       "   0.02982521615922451,\n",
       "   -0.05642880126833916,\n",
       "   0.02351946197450161,\n",
       "   -0.009654783643782139,\n",
       "   0.031204337254166603,\n",
       "   -0.004521077033132315,\n",
       "   -0.026945408433675766,\n",
       "   -0.010690713301301003,\n",
       "   -0.04933813586831093,\n",
       "   0.013121102936565876,\n",
       "   0.015008967369794846,\n",
       "   -0.09336095303297043,\n",
       "   0.004713067784905434,\n",
       "   0.03029872104525566,\n",
       "   -0.02786005102097988,\n",
       "   -0.004158325493335724,\n",
       "   0.0093575743958354,\n",
       "   0.0420675203204155,\n",
       "   0.05720991641283035,\n",
       "   0.09609830379486084,\n",
       "   0.030236130580306053,\n",
       "   0.06221339851617813,\n",
       "   0.006406852509826422,\n",
       "   0.04857690632343292,\n",
       "   0.052288834005594254,\n",
       "   -0.13545221090316772,\n",
       "   0.061087947338819504,\n",
       "   -0.011330120265483856,\n",
       "   0.05141744762659073,\n",
       "   -0.007343425415456295,\n",
       "   -0.04524607211351395,\n",
       "   0.01833309978246689,\n",
       "   0.044993143528699875,\n",
       "   -0.025369102135300636,\n",
       "   0.025394542142748833,\n",
       "   0.061601459980010986,\n",
       "   0.03634617477655411,\n",
       "   -0.0008713324787095189,\n",
       "   0.05402189865708351,\n",
       "   -0.012912903912365437,\n",
       "   -0.06627970188856125,\n",
       "   0.06845586001873016,\n",
       "   -0.07059567421674728,\n",
       "   -0.009118328802287579,\n",
       "   0.005566880106925964,\n",
       "   -0.01624024659395218,\n",
       "   0.002243442228063941,\n",
       "   -0.044681817293167114,\n",
       "   -0.0424482598900795,\n",
       "   -0.044351767748594284,\n",
       "   0.019879663363099098,\n",
       "   0.027554849162697792,\n",
       "   0.004687897395342588,\n",
       "   0.012319299392402172,\n",
       "   -0.0028507267124950886,\n",
       "   -0.016624055802822113,\n",
       "   0.07119318097829819,\n",
       "   0.006005965173244476,\n",
       "   -0.009677797555923462,\n",
       "   -0.04375245049595833,\n",
       "   0.01157162245362997,\n",
       "   -0.012912058271467686,\n",
       "   0.39160147309303284,\n",
       "   -0.025437718257308006,\n",
       "   -0.0009586657397449017,\n",
       "   -0.004293058067560196,\n",
       "   0.005390905309468508,\n",
       "   -0.028892941772937775,\n",
       "   0.004154533613473177,\n",
       "   -0.05406158044934273,\n",
       "   -0.041898660361766815,\n",
       "   -0.02650536596775055,\n",
       "   0.07219402492046356,\n",
       "   0.028147637844085693,\n",
       "   -0.019870512187480927,\n",
       "   0.01760745793581009,\n",
       "   -0.01193871907889843,\n",
       "   -0.004612905904650688,\n",
       "   -0.0728016123175621,\n",
       "   0.03833841159939766,\n",
       "   0.006558107677847147,\n",
       "   0.043441031128168106,\n",
       "   -0.024927785620093346,\n",
       "   -0.017702259123325348,\n",
       "   -0.0057075112126767635,\n",
       "   0.05570271983742714,\n",
       "   -0.04307899996638298,\n",
       "   -0.039732255041599274,\n",
       "   0.005473616532981396,\n",
       "   0.0243095550686121,\n",
       "   0.08310713618993759,\n",
       "   -0.004063669592142105,\n",
       "   0.0043319822289049625,\n",
       "   0.04990985244512558,\n",
       "   0.028749912977218628,\n",
       "   -0.02010415494441986,\n",
       "   0.016699129715561867,\n",
       "   0.027316320687532425,\n",
       "   0.020206362009048462,\n",
       "   0.04175499454140663,\n",
       "   0.057939134538173676,\n",
       "   -0.0523565411567688,\n",
       "   0.04867256432771683,\n",
       "   -0.04024703428149223,\n",
       "   0.04396742954850197,\n",
       "   0.00866388063877821,\n",
       "   -0.017994916066527367,\n",
       "   -0.07451913505792618,\n",
       "   0.0747128427028656,\n",
       "   -0.011400862596929073,\n",
       "   -0.04079199582338333,\n",
       "   -0.007992097176611423,\n",
       "   0.006504095159471035,\n",
       "   -0.003965010866522789,\n",
       "   0.04578916355967522,\n",
       "   0.030584553256630898,\n",
       "   -0.0393405519425869,\n",
       "   -0.023727329447865486,\n",
       "   0.039857201278209686,\n",
       "   0.022646689787507057,\n",
       "   -0.044810689985752106,\n",
       "   -0.028582345694303513,\n",
       "   0.0006526146316900849,\n",
       "   -0.04151475057005882,\n",
       "   -0.050171539187431335,\n",
       "   -0.04047127068042755,\n",
       "   0.1006147488951683,\n",
       "   0.01732858456671238,\n",
       "   -0.11429397016763687,\n",
       "   -0.02305552363395691,\n",
       "   0.007476298604160547,\n",
       "   0.01628801040351391,\n",
       "   0.05689629539847374,\n",
       "   0.022448545321822166,\n",
       "   0.07450707256793976,\n",
       "   -0.010456444695591927,\n",
       "   -0.007100176531821489,\n",
       "   -0.03187616541981697,\n",
       "   0.02546732686460018,\n",
       "   -0.032989878207445145,\n",
       "   0.009429866448044777,\n",
       "   0.02195826545357704,\n",
       "   0.052912719547748566,\n",
       "   0.05129202455282211,\n",
       "   -0.01269273366779089,\n",
       "   0.05181432515382767,\n",
       "   -0.016089029610157013,\n",
       "   0.01146555133163929,\n",
       "   -0.03517530485987663,\n",
       "   -0.017738372087478638,\n",
       "   -0.04360666871070862,\n",
       "   0.01956181600689888,\n",
       "   -0.006225330289453268,\n",
       "   -0.08309964835643768,\n",
       "   0.05771610513329506,\n",
       "   0.010135226882994175,\n",
       "   0.024365313351154327,\n",
       "   -0.004672948736697435,\n",
       "   -0.01078018732368946,\n",
       "   -0.008538912050426006,\n",
       "   -0.03452421724796295,\n",
       "   -0.0783490240573883,\n",
       "   0.03250216692686081,\n",
       "   -0.04880961403250694,\n",
       "   0.04467076063156128,\n",
       "   -0.0708308219909668,\n",
       "   -0.019312415271997452,\n",
       "   0.009368875995278358,\n",
       "   -0.07534089684486389,\n",
       "   0.015249871648848057,\n",
       "   -0.017157023772597313,\n",
       "   0.051404062658548355,\n",
       "   -0.01168756652623415,\n",
       "   -0.03553205728530884,\n",
       "   0.021805109456181526,\n",
       "   0.026941930875182152,\n",
       "   0.058223456144332886,\n",
       "   -0.0512370802462101,\n",
       "   -0.014603972434997559,\n",
       "   0.0012275761691853404,\n",
       "   -0.01095688994973898,\n",
       "   -0.02492356300354004,\n",
       "   -0.00723233399912715,\n",
       "   -0.016700945794582367,\n",
       "   -0.013490129262208939,\n",
       "   -0.025136848911643028,\n",
       "   -0.3345274329185486,\n",
       "   -0.07069365680217743,\n",
       "   0.013973223976790905,\n",
       "   -0.002885684836655855,\n",
       "   0.011793160811066628,\n",
       "   -0.06620368361473083,\n",
       "   -0.006780729163438082,\n",
       "   0.002130950568243861,\n",
       "   0.010816175490617752,\n",
       "   0.07291585206985474,\n",
       "   0.054127149283885956,\n",
       "   -0.035625018179416656,\n",
       "   -0.008984396234154701,\n",
       "   0.000751699844840914,\n",
       "   0.016085196286439896,\n",
       "   0.02974061295390129,\n",
       "   -0.014350071549415588,\n",
       "   0.070644311606884,\n",
       "   -0.06741445511579514,\n",
       "   0.052917614579200745,\n",
       "   0.05413307249546051,\n",
       "   -0.044515591114759445,\n",
       "   0.0062502347864210606,\n",
       "   -0.09105268865823746,\n",
       "   -0.0439554825425148,\n",
       "   -0.008462919853627682,\n",
       "   0.15202578902244568,\n",
       "   -0.03186310827732086,\n",
       "   0.04426424205303192,\n",
       "   0.011519069783389568,\n",
       "   0.032656777650117874,\n",
       "   0.06102573871612549,\n",
       "   -0.049524154514074326,\n",
       "   -0.05941198766231537,\n",
       "   0.04924849793314934,\n",
       "   0.006428555119782686,\n",
       "   0.03542428836226463,\n",
       "   0.008479967713356018,\n",
       "   0.021322667598724365,\n",
       "   -0.06455278396606445,\n",
       "   0.027914827689528465,\n",
       "   -0.0010063649388030171,\n",
       "   -0.007966368459165096,\n",
       "   -0.05404340475797653,\n",
       "   0.001435312326066196,\n",
       "   0.008531895466148853,\n",
       "   0.0032894506584852934,\n",
       "   -0.03928128629922867,\n",
       "   -0.017618350684642792,\n",
       "   -0.04654363542795181,\n",
       "   0.034746263176202774,\n",
       "   0.05943401902914047,\n",
       "   -0.037191033363342285,\n",
       "   0.04951329901814461,\n",
       "   -0.10688032954931259,\n",
       "   -0.004915831610560417,\n",
       "   -0.036500055342912674,\n",
       "   0.026966223493218422,\n",
       "   -0.04413231834769249,\n",
       "   0.029473822563886642,\n",
       "   -0.0037652107421308756,\n",
       "   0.015790941193699837,\n",
       "   -0.005008837673813105,\n",
       "   0.018805215135216713,\n",
       "   0.014314229600131512,\n",
       "   -0.04259999096393585,\n",
       "   0.042615827172994614,\n",
       "   -0.021377673372626305,\n",
       "   0.035286545753479004,\n",
       "   0.0314343199133873,\n",
       "   -0.02317284792661667,\n",
       "   0.07087846100330353,\n",
       "   -0.021553168073296547,\n",
       "   -0.0040386333130300045,\n",
       "   0.04295286163687706,\n",
       "   0.00038387966924346983,\n",
       "   -0.004342842847108841,\n",
       "   -0.03708760440349579,\n",
       "   0.003886185586452484,\n",
       "   0.035173334181308746,\n",
       "   0.044506244361400604,\n",
       "   -0.06224174052476883,\n",
       "   0.01542347576469183,\n",
       "   0.01845642738044262,\n",
       "   0.0212972704321146,\n",
       "   0.022075025364756584,\n",
       "   0.007748667150735855,\n",
       "   -0.09103596955537796,\n",
       "   0.056980833411216736,\n",
       "   0.016812818124890327,\n",
       "   -0.019231434911489487,\n",
       "   0.005453511141240597,\n",
       "   -0.06538092344999313,\n",
       "   -0.08141845464706421,\n",
       "   0.01233407761901617,\n",
       "   -0.01474402192980051,\n",
       "   -0.22115935385227203,\n",
       "   0.0672222450375557,\n",
       "   0.016623767092823982,\n",
       "   0.042385656386613846,\n",
       "   -0.02105282060801983,\n",
       "   -0.020235152915120125,\n",
       "   -0.003459023078903556,\n",
       "   -0.012428101152181625,\n",
       "   0.004868682008236647,\n",
       "   0.013204908929765224,\n",
       "   0.052499670535326004,\n",
       "   0.060757432132959366,\n",
       "   0.004988504573702812,\n",
       "   -0.0004880620981566608,\n",
       "   -0.033039942383766174,\n",
       "   -0.030833061784505844,\n",
       "   0.123908132314682,\n",
       "   -0.016802841797471046,\n",
       "   0.035419341176748276,\n",
       "   -0.0031891674734652042,\n",
       "   0.018013596534729004,\n",
       "   0.03988245129585266,\n",
       "   0.15679098665714264,\n",
       "   0.0066954344511032104,\n",
       "   0.055160246789455414,\n",
       "   0.00043811980867758393,\n",
       "   0.01769241876900196,\n",
       "   -0.08006754517555237,\n",
       "   -0.04713493213057518,\n",
       "   -0.0010041469940915704,\n",
       "   0.03521290794014931,\n",
       "   -0.03305087611079216,\n",
       "   0.11719419062137604,\n",
       "   -0.030453136190772057,\n",
       "   0.030392279848456383,\n",
       "   0.018382485955953598,\n",
       "   0.02263294905424118,\n",
       "   -0.006697284989058971,\n",
       "   -0.006069275084882975,\n",
       "   0.027892833575606346,\n",
       "   -0.05509866401553154,\n",
       "   0.048747509717941284,\n",
       "   0.000934113806579262,\n",
       "   -0.05280854180455208,\n",
       "   0.02869327925145626,\n",
       "   0.03235302120447159,\n",
       "   -0.034411534667015076,\n",
       "   -0.06012794002890587,\n",
       "   -0.06896082311868668,\n",
       "   0.03024384379386902,\n",
       "   0.011379316449165344,\n",
       "   0.028236711397767067,\n",
       "   -0.028990311548113823,\n",
       "   -0.005419114138931036,\n",
       "   0.053560756146907806,\n",
       "   0.008594591170549393,\n",
       "   -0.0005539535195566714,\n",
       "   0.01477746106684208,\n",
       "   -0.016216471791267395,\n",
       "   -0.0592540018260479,\n",
       "   -0.0008053381461650133,\n",
       "   -0.0550214983522892,\n",
       "   0.04064732417464256,\n",
       "   0.005595277529209852,\n",
       "   -0.02530382014811039],\n",
       "  '_relevance_score': 0.016393441706895828},\n",
       " {'text': 'just choose their preferred framing, as long as it is consistent and useful for their own work.\\nPhysical vs. digital: what differences beget attention? While animals only live once in the physical\\nworld, digital environments (e.g., the Internet) often allow sequential (via resets) and parallel trials. This\\nmeans digital agents can more boldly explore (e.g., open a million webpages) and self-clone for parallel task\\nsolving (e.g., a million web agents try different web paths), which may result in decision-making procedures\\ndifferent from current ones inspired by human cognition (Griffiths, 2020).\\nLearning vs. acting: how should agents continuously and autonomously learn? In the CoALA\\nframework, learning is a result action of a decision-making cycle just like grounding: the agent deliberately\\nchooses to commit information to long-term memory. This is in contrast to most agents, which simply fix a\\nlearning schedule and only use decison making for external actions. Biological agents, however, do not have',\n",
       "  'metadata_file_name': 'Cognitive_Architectures_for_Language_Agents.pdf',\n",
       "  'metadata_creation_date': '2025-01-08',\n",
       "  'metadata_pagenumber': 18,\n",
       "  'vector': [-0.04917022958397865,\n",
       "   -0.03909417241811752,\n",
       "   0.012346594594419003,\n",
       "   0.007315000984817743,\n",
       "   -0.0018587749218568206,\n",
       "   -0.0044872998259961605,\n",
       "   -0.0010329752694815397,\n",
       "   0.02183983102440834,\n",
       "   0.05129013955593109,\n",
       "   -0.006816827692091465,\n",
       "   0.024600576609373093,\n",
       "   0.0017598761478438973,\n",
       "   -0.003014378249645233,\n",
       "   0.04752271994948387,\n",
       "   0.012063553556799889,\n",
       "   0.001680482062511146,\n",
       "   -0.09487718343734741,\n",
       "   -0.02569708041846752,\n",
       "   0.03272517770528793,\n",
       "   0.022837821394205093,\n",
       "   0.007697828579694033,\n",
       "   0.01768333651125431,\n",
       "   0.027743928134441376,\n",
       "   -0.053422633558511734,\n",
       "   -8.454039198113605e-05,\n",
       "   -0.01644260063767433,\n",
       "   -0.02852538786828518,\n",
       "   -0.03886689245700836,\n",
       "   0.011748994700610638,\n",
       "   -0.1692059338092804,\n",
       "   0.05449072644114494,\n",
       "   0.04346460849046707,\n",
       "   0.02470426820218563,\n",
       "   0.01721811853349209,\n",
       "   -0.03307740017771721,\n",
       "   -0.03786725178360939,\n",
       "   0.008218050934374332,\n",
       "   -0.012547152116894722,\n",
       "   -0.015753725543618202,\n",
       "   0.005559144541621208,\n",
       "   -0.019869962707161903,\n",
       "   0.005150255281478167,\n",
       "   0.0147701445966959,\n",
       "   0.0005282777128741145,\n",
       "   -0.03880582004785538,\n",
       "   0.02932078018784523,\n",
       "   0.006074809469282627,\n",
       "   -0.0339866578578949,\n",
       "   -0.02086867205798626,\n",
       "   -0.02990758791565895,\n",
       "   0.01248655840754509,\n",
       "   -0.024649329483509064,\n",
       "   0.02286680042743683,\n",
       "   0.006585417781025171,\n",
       "   0.015146841295063496,\n",
       "   0.025562845170497894,\n",
       "   0.03835349529981613,\n",
       "   0.13780002295970917,\n",
       "   0.03150409460067749,\n",
       "   -0.02544219419360161,\n",
       "   0.04777195677161217,\n",
       "   0.026915566995739937,\n",
       "   -0.10057651996612549,\n",
       "   0.09069622308015823,\n",
       "   0.03416213020682335,\n",
       "   0.05877050757408142,\n",
       "   -0.04138387367129326,\n",
       "   0.0031036026775836945,\n",
       "   -0.022168386727571487,\n",
       "   0.044641025364398956,\n",
       "   -0.024620791897177696,\n",
       "   0.03144749999046326,\n",
       "   -0.01602454110980034,\n",
       "   0.0361955389380455,\n",
       "   0.05008504167199135,\n",
       "   0.009258580394089222,\n",
       "   -0.02838076464831829,\n",
       "   -0.02742113545536995,\n",
       "   0.030881082639098167,\n",
       "   -0.042667828500270844,\n",
       "   -0.0025340234860777855,\n",
       "   -0.009680700488388538,\n",
       "   -0.01021153200417757,\n",
       "   -0.046236954629421234,\n",
       "   -0.07840652018785477,\n",
       "   -0.03895511478185654,\n",
       "   0.0036703867372125387,\n",
       "   -0.052825927734375,\n",
       "   0.03171304613351822,\n",
       "   0.02723766304552555,\n",
       "   -0.025790635496377945,\n",
       "   0.04466971009969711,\n",
       "   -0.03982026129961014,\n",
       "   0.013509564101696014,\n",
       "   -0.0019541229121387005,\n",
       "   -0.0033433150965720415,\n",
       "   0.010471339337527752,\n",
       "   0.02047882229089737,\n",
       "   -0.012129689566791058,\n",
       "   0.3850632607936859,\n",
       "   0.006797172594815493,\n",
       "   0.01196075975894928,\n",
       "   0.004659307189285755,\n",
       "   0.02631411701440811,\n",
       "   0.02642839029431343,\n",
       "   0.003973348066210747,\n",
       "   -0.022068606689572334,\n",
       "   -0.031929582357406616,\n",
       "   -0.031212501227855682,\n",
       "   0.05928879231214523,\n",
       "   0.04625244811177254,\n",
       "   -0.003399585373699665,\n",
       "   0.0009263062383979559,\n",
       "   0.0015368115855380893,\n",
       "   -0.026486167684197426,\n",
       "   -0.0031340999994426966,\n",
       "   -0.025960613042116165,\n",
       "   0.0319451205432415,\n",
       "   0.05143490433692932,\n",
       "   -0.015548969618976116,\n",
       "   -0.040672410279512405,\n",
       "   0.005112271755933762,\n",
       "   0.07360781729221344,\n",
       "   -0.02132578007876873,\n",
       "   -0.023191874846816063,\n",
       "   -0.038001514971256256,\n",
       "   0.01582186482846737,\n",
       "   0.07343264669179916,\n",
       "   -0.048376452177762985,\n",
       "   -0.016523147001862526,\n",
       "   0.04989643394947052,\n",
       "   -0.05212210863828659,\n",
       "   -0.011534647084772587,\n",
       "   0.014506704173982143,\n",
       "   0.02217285707592964,\n",
       "   0.03118337318301201,\n",
       "   0.06707021594047546,\n",
       "   0.03380383551120758,\n",
       "   -0.016808174550533295,\n",
       "   0.018909402191638947,\n",
       "   -0.0424724780023098,\n",
       "   0.0908745676279068,\n",
       "   0.028683830052614212,\n",
       "   -0.026233000680804253,\n",
       "   -0.09330867975950241,\n",
       "   0.08307250589132309,\n",
       "   0.06574519723653793,\n",
       "   0.01610536128282547,\n",
       "   -0.049714021384716034,\n",
       "   -0.026290688663721085,\n",
       "   -0.014157692901790142,\n",
       "   0.03795541822910309,\n",
       "   0.04391076788306236,\n",
       "   -0.024088390171527863,\n",
       "   -0.04610178992152214,\n",
       "   0.0811692327260971,\n",
       "   -0.031990766525268555,\n",
       "   -0.04241599887609482,\n",
       "   -0.030598845332860947,\n",
       "   -0.048414964228868484,\n",
       "   -0.02782890945672989,\n",
       "   0.005997927859425545,\n",
       "   -0.020965689793229103,\n",
       "   0.11014869809150696,\n",
       "   0.0429147332906723,\n",
       "   -0.015325410291552544,\n",
       "   -0.021986374631524086,\n",
       "   -0.024434925988316536,\n",
       "   -0.0022819966543465853,\n",
       "   0.012414112687110901,\n",
       "   0.005888636223971844,\n",
       "   0.03617142140865326,\n",
       "   -0.025723477825522423,\n",
       "   -0.03781649470329285,\n",
       "   -0.062233299016952515,\n",
       "   -0.025828206911683083,\n",
       "   -0.0947342962026596,\n",
       "   -0.04353762045502663,\n",
       "   0.014836993999779224,\n",
       "   0.017300358042120934,\n",
       "   0.061811719089746475,\n",
       "   -0.03375628963112831,\n",
       "   0.02975030243396759,\n",
       "   0.010310770012438297,\n",
       "   0.005481273867189884,\n",
       "   -0.01306850928813219,\n",
       "   -0.07730786502361298,\n",
       "   -0.017094500362873077,\n",
       "   0.066051185131073,\n",
       "   0.02582189440727234,\n",
       "   0.018955180421471596,\n",
       "   0.022661086171865463,\n",
       "   0.038802627474069595,\n",
       "   0.007608334068208933,\n",
       "   -0.03557976335287094,\n",
       "   -0.010642179287970066,\n",
       "   0.008898740634322166,\n",
       "   -0.04412376880645752,\n",
       "   -0.06409990787506104,\n",
       "   0.04771094024181366,\n",
       "   -0.0707780048251152,\n",
       "   0.045894838869571686,\n",
       "   -0.04734562337398529,\n",
       "   -0.03208469599485397,\n",
       "   -0.0015626768581569195,\n",
       "   0.009738165885210037,\n",
       "   0.040738411247730255,\n",
       "   -0.08764337748289108,\n",
       "   0.05745358765125275,\n",
       "   -0.011153808794915676,\n",
       "   -0.08499107509851456,\n",
       "   0.019918186590075493,\n",
       "   -0.03367343917489052,\n",
       "   0.014538048766553402,\n",
       "   -0.052001889795064926,\n",
       "   0.012761124409735203,\n",
       "   0.05654899403452873,\n",
       "   0.006361982319504023,\n",
       "   0.0065068453550338745,\n",
       "   0.050034843385219574,\n",
       "   -0.030049722641706467,\n",
       "   0.014781992882490158,\n",
       "   0.037314239889383316,\n",
       "   -0.2982318699359894,\n",
       "   -0.07875946909189224,\n",
       "   -0.01902022957801819,\n",
       "   -0.0445702038705349,\n",
       "   0.04538574814796448,\n",
       "   -0.030911581590771675,\n",
       "   0.002668406581506133,\n",
       "   -0.017497902736067772,\n",
       "   -0.03905852511525154,\n",
       "   0.05606852099299431,\n",
       "   0.06518367677927017,\n",
       "   -0.04763631522655487,\n",
       "   -0.015765594318509102,\n",
       "   -0.05504249036312103,\n",
       "   0.008366407826542854,\n",
       "   -0.020252415910363197,\n",
       "   0.036617882549762726,\n",
       "   0.03172290697693825,\n",
       "   -0.0676703229546547,\n",
       "   0.032435331493616104,\n",
       "   0.0057464102283120155,\n",
       "   0.013716643676161766,\n",
       "   0.03435671702027321,\n",
       "   -0.12538157403469086,\n",
       "   0.028132176026701927,\n",
       "   0.02919715829193592,\n",
       "   0.13599783182144165,\n",
       "   -0.007490355055779219,\n",
       "   0.059542927891016006,\n",
       "   0.03573932498693466,\n",
       "   0.010388416238129139,\n",
       "   0.06180611252784729,\n",
       "   -0.0650399848818779,\n",
       "   -0.08591834455728531,\n",
       "   -0.0007002410711720586,\n",
       "   -0.07649480551481247,\n",
       "   0.02872231975197792,\n",
       "   -0.03719223663210869,\n",
       "   -0.014836764894425869,\n",
       "   -0.10140249133110046,\n",
       "   0.06583667546510696,\n",
       "   -0.0014427832793444395,\n",
       "   -0.01688516139984131,\n",
       "   -0.05380786210298538,\n",
       "   -0.05831855162978172,\n",
       "   0.040378693491220474,\n",
       "   -0.03599119931459427,\n",
       "   0.053116437047719955,\n",
       "   -0.004844324197620153,\n",
       "   -0.05792772397398949,\n",
       "   -0.04157884418964386,\n",
       "   0.011480090208351612,\n",
       "   -0.03324739262461662,\n",
       "   0.025593619793653488,\n",
       "   -0.0752304196357727,\n",
       "   0.015114483423531055,\n",
       "   -0.054366037249565125,\n",
       "   0.037566181272268295,\n",
       "   -0.037350237369537354,\n",
       "   0.030446214601397514,\n",
       "   0.02878449857234955,\n",
       "   -0.03458140790462494,\n",
       "   -0.00011400212679291144,\n",
       "   -0.03512883186340332,\n",
       "   0.06044076010584831,\n",
       "   -0.02654472179710865,\n",
       "   0.03405901417136192,\n",
       "   -0.04543027654290199,\n",
       "   0.0284894946962595,\n",
       "   0.046062592417001724,\n",
       "   0.0077845314517617226,\n",
       "   0.12375719100236893,\n",
       "   0.00546256685629487,\n",
       "   -0.044678375124931335,\n",
       "   0.04062389209866524,\n",
       "   -0.036990635097026825,\n",
       "   0.03815171867609024,\n",
       "   0.0058545274659991264,\n",
       "   0.0020539748948067427,\n",
       "   0.06135398894548416,\n",
       "   -0.006421795580536127,\n",
       "   -0.08391077816486359,\n",
       "   0.02660045027732849,\n",
       "   0.051595840603113174,\n",
       "   0.06672809273004532,\n",
       "   -0.04591338708996773,\n",
       "   0.05227910354733467,\n",
       "   -0.05285963788628578,\n",
       "   0.04130890592932701,\n",
       "   0.022424526512622833,\n",
       "   -0.01687387190759182,\n",
       "   0.016869664192199707,\n",
       "   -0.07225988060235977,\n",
       "   -0.02476241998374462,\n",
       "   -0.05392972752451897,\n",
       "   0.016775717958807945,\n",
       "   -0.23315614461898804,\n",
       "   0.0459558442234993,\n",
       "   0.03360435739159584,\n",
       "   0.08353893458843231,\n",
       "   -0.02011527679860592,\n",
       "   -0.011786486953496933,\n",
       "   0.005560661666095257,\n",
       "   -0.03666721284389496,\n",
       "   0.030396556481719017,\n",
       "   -0.0036512967199087143,\n",
       "   -0.017896370962262154,\n",
       "   0.028247695416212082,\n",
       "   0.010876484215259552,\n",
       "   0.038519274443387985,\n",
       "   -0.01925157569348812,\n",
       "   0.059627171605825424,\n",
       "   0.1251249462366104,\n",
       "   0.01180869247764349,\n",
       "   -0.00015352241462096572,\n",
       "   -0.053334739059209824,\n",
       "   0.006946575362235308,\n",
       "   0.003192394506186247,\n",
       "   0.16703654825687408,\n",
       "   -0.03814830258488655,\n",
       "   0.08064405620098114,\n",
       "   -0.027122771367430687,\n",
       "   -0.02122795209288597,\n",
       "   -0.006494678556919098,\n",
       "   0.021872691810131073,\n",
       "   -0.016108179464936256,\n",
       "   0.006808134727180004,\n",
       "   -0.006122446618974209,\n",
       "   0.04833618178963661,\n",
       "   -0.03848065063357353,\n",
       "   0.013209208846092224,\n",
       "   0.0010645299917086959,\n",
       "   0.0034135887399315834,\n",
       "   0.0007457398460246623,\n",
       "   0.018146367743611336,\n",
       "   -0.003037633839994669,\n",
       "   -0.04023539274930954,\n",
       "   -0.009678333066403866,\n",
       "   0.02039168030023575,\n",
       "   -0.04423202574253082,\n",
       "   0.06686721742153168,\n",
       "   0.04495212808251381,\n",
       "   -0.011261451058089733,\n",
       "   -0.07006372511386871,\n",
       "   -0.11581577360630035,\n",
       "   0.01183925848454237,\n",
       "   -0.03256628289818764,\n",
       "   -0.03009709157049656,\n",
       "   0.0002524489245843142,\n",
       "   -0.022225718945264816,\n",
       "   0.021975230425596237,\n",
       "   0.016169873997569084,\n",
       "   0.04947257041931152,\n",
       "   -0.009584799408912659,\n",
       "   -0.019079141318798065,\n",
       "   -0.006789581850171089,\n",
       "   0.041924234479665756,\n",
       "   0.0359082855284214,\n",
       "   0.054689571261405945,\n",
       "   0.014595922082662582,\n",
       "   0.019564516842365265],\n",
       "  '_relevance_score': 0.016393441706895828},\n",
       " {'text': 'Published in Transactions on Machine Learning Research (02/2024)\\n\\nUpdating agent code (procedural memory). CoALA allows agents to update their source code, thus\\nmodifying the implementation of various procedures. These can be broken down as follows:',\n",
       "  'metadata_file_name': 'Cognitive_Architectures_for_Language_Agents.pdf',\n",
       "  'metadata_creation_date': '2025-01-08',\n",
       "  'metadata_pagenumber': 12,\n",
       "  'vector': [-0.09369078278541565,\n",
       "   0.02595362812280655,\n",
       "   0.0156437736004591,\n",
       "   -0.011263088323175907,\n",
       "   -0.00825512409210205,\n",
       "   0.010134057141840458,\n",
       "   -0.0031882389448583126,\n",
       "   -0.031273216009140015,\n",
       "   -0.0031269460450857878,\n",
       "   0.04040508717298508,\n",
       "   0.02957119792699814,\n",
       "   0.01588629186153412,\n",
       "   -0.014159604907035828,\n",
       "   0.0039577181451022625,\n",
       "   -0.007983383722603321,\n",
       "   0.04804263263940811,\n",
       "   -0.050872791558504105,\n",
       "   0.03955749049782753,\n",
       "   0.023011330515146255,\n",
       "   -0.03867907449603081,\n",
       "   0.0008683170308358967,\n",
       "   0.004658987279981375,\n",
       "   0.015336621552705765,\n",
       "   0.02604888379573822,\n",
       "   -0.0774032324552536,\n",
       "   0.004941900260746479,\n",
       "   -0.09839558601379395,\n",
       "   -0.043238911777734756,\n",
       "   -0.05929312855005264,\n",
       "   -0.17833304405212402,\n",
       "   0.05374960973858833,\n",
       "   0.04917524382472038,\n",
       "   0.031120071187615395,\n",
       "   -0.02044127881526947,\n",
       "   -0.022302189841866493,\n",
       "   0.0052608102560043335,\n",
       "   -0.003509459551423788,\n",
       "   0.005225432571023703,\n",
       "   -0.0491287000477314,\n",
       "   0.040616001933813095,\n",
       "   0.017717566341161728,\n",
       "   0.04999557510018349,\n",
       "   -0.06244995817542076,\n",
       "   -0.032923344522714615,\n",
       "   -0.030216936022043228,\n",
       "   -0.015411222353577614,\n",
       "   0.07096591591835022,\n",
       "   0.017038671299815178,\n",
       "   -0.03449699282646179,\n",
       "   -0.011477611027657986,\n",
       "   0.04946903884410858,\n",
       "   0.013777057640254498,\n",
       "   -0.02685219794511795,\n",
       "   0.013216296210885048,\n",
       "   0.02953309379518032,\n",
       "   -0.0028341489378362894,\n",
       "   0.05329488217830658,\n",
       "   0.1032882109284401,\n",
       "   0.06635531038045883,\n",
       "   0.028700612485408783,\n",
       "   0.08323545753955841,\n",
       "   0.08320316672325134,\n",
       "   -0.12910382449626923,\n",
       "   0.08922982215881348,\n",
       "   0.049925707280635834,\n",
       "   0.02572476863861084,\n",
       "   -0.041976138949394226,\n",
       "   -0.1040668934583664,\n",
       "   0.04098391905426979,\n",
       "   0.08329208195209503,\n",
       "   -0.042604707181453705,\n",
       "   0.02613588236272335,\n",
       "   0.04951627552509308,\n",
       "   0.015502380207180977,\n",
       "   0.0445634126663208,\n",
       "   0.06982629746198654,\n",
       "   -0.0044084638357162476,\n",
       "   0.0034539750777184963,\n",
       "   0.059380903840065,\n",
       "   -0.036665987223386765,\n",
       "   -0.032424554228782654,\n",
       "   -0.02246570587158203,\n",
       "   -0.0416078045964241,\n",
       "   -0.03599919006228447,\n",
       "   -0.01590711437165737,\n",
       "   -0.056463710963726044,\n",
       "   -0.030298881232738495,\n",
       "   0.011663304641842842,\n",
       "   0.0355830155313015,\n",
       "   -0.050806768238544464,\n",
       "   0.006797685753554106,\n",
       "   0.000270548538537696,\n",
       "   0.03444913774728775,\n",
       "   0.02751518040895462,\n",
       "   -0.023057280108332634,\n",
       "   0.007008324842900038,\n",
       "   -0.03323713690042496,\n",
       "   0.047654420137405396,\n",
       "   -0.018149567767977715,\n",
       "   0.36158689856529236,\n",
       "   0.007754584774374962,\n",
       "   -0.008289327844977379,\n",
       "   -0.05134864151477814,\n",
       "   0.006084781605750322,\n",
       "   -0.02627723291516304,\n",
       "   -0.038648463785648346,\n",
       "   0.008947072550654411,\n",
       "   -0.000869041308760643,\n",
       "   -0.047459736466407776,\n",
       "   0.008931891992688179,\n",
       "   0.02091175876557827,\n",
       "   0.009877930395305157,\n",
       "   0.07967915385961533,\n",
       "   0.06531345844268799,\n",
       "   -0.0418258011341095,\n",
       "   -0.01129300706088543,\n",
       "   0.007769628893584013,\n",
       "   0.020176872611045837,\n",
       "   -0.0039696283638477325,\n",
       "   0.01427880022674799,\n",
       "   -0.01724820025265217,\n",
       "   0.013221885077655315,\n",
       "   0.03574565052986145,\n",
       "   -0.019023237749934196,\n",
       "   -0.03091653622686863,\n",
       "   0.04711330682039261,\n",
       "   -0.020696889609098434,\n",
       "   0.08824072778224945,\n",
       "   -0.033742934465408325,\n",
       "   -0.012001264840364456,\n",
       "   0.04086463525891304,\n",
       "   0.05407724902033806,\n",
       "   -0.04380824416875839,\n",
       "   0.018119608983397484,\n",
       "   0.03295733034610748,\n",
       "   -0.016137879341840744,\n",
       "   0.054224852472543716,\n",
       "   0.04840917885303497,\n",
       "   -0.046652793884277344,\n",
       "   -0.007722401525825262,\n",
       "   -0.07761400192975998,\n",
       "   0.05714431777596474,\n",
       "   0.011874297633767128,\n",
       "   -0.008911799639463425,\n",
       "   -0.11008325964212418,\n",
       "   0.05260322988033295,\n",
       "   0.007628979627043009,\n",
       "   -0.007246207445859909,\n",
       "   -0.019929740577936172,\n",
       "   -0.018105819821357727,\n",
       "   -0.042258281260728836,\n",
       "   0.058155275881290436,\n",
       "   0.02829321101307869,\n",
       "   -0.024584006518125534,\n",
       "   -0.032489318400621414,\n",
       "   0.027611929923295975,\n",
       "   0.039318569004535675,\n",
       "   -0.009737303480505943,\n",
       "   -0.035007622092962265,\n",
       "   0.016869697719812393,\n",
       "   -0.021217573434114456,\n",
       "   -0.04148665815591812,\n",
       "   -0.009086621925234795,\n",
       "   0.08281361311674118,\n",
       "   0.0026316617149859667,\n",
       "   -0.05766717717051506,\n",
       "   -0.00290084769949317,\n",
       "   -0.023186517879366875,\n",
       "   0.06355512887239456,\n",
       "   0.02215343713760376,\n",
       "   0.016131769865751266,\n",
       "   0.06480493396520615,\n",
       "   -0.03410225734114647,\n",
       "   -0.11699572950601578,\n",
       "   -0.01142048742622137,\n",
       "   0.002477155765518546,\n",
       "   0.0042222850024700165,\n",
       "   -0.015264004468917847,\n",
       "   0.06209569051861763,\n",
       "   -0.008298876695334911,\n",
       "   0.02646525762975216,\n",
       "   0.009019923396408558,\n",
       "   -0.022071626037359238,\n",
       "   0.00023278986918739974,\n",
       "   0.014789427630603313,\n",
       "   -0.026413146406412125,\n",
       "   -0.06747401505708694,\n",
       "   -0.036007825285196304,\n",
       "   0.014090297743678093,\n",
       "   0.00123696681112051,\n",
       "   -0.04963265731930733,\n",
       "   0.008909483440220356,\n",
       "   0.006635196506977081,\n",
       "   0.015283632092177868,\n",
       "   0.00555801996961236,\n",
       "   -0.029990416020154953,\n",
       "   -0.02907332219183445,\n",
       "   -0.016405105590820312,\n",
       "   -0.0462379977107048,\n",
       "   -0.004815188702195883,\n",
       "   -0.04508495703339577,\n",
       "   0.09979522228240967,\n",
       "   -0.07095584273338318,\n",
       "   -0.03366117551922798,\n",
       "   0.01258918922394514,\n",
       "   -0.03586967661976814,\n",
       "   0.03671656548976898,\n",
       "   -0.04778311774134636,\n",
       "   -0.0196773000061512,\n",
       "   -0.014460748992860317,\n",
       "   -0.02470916509628296,\n",
       "   0.002073157811537385,\n",
       "   0.007528566289693117,\n",
       "   0.053631968796253204,\n",
       "   -0.07547371834516525,\n",
       "   0.03156110271811485,\n",
       "   -0.009538741782307625,\n",
       "   0.018913239240646362,\n",
       "   0.020739508792757988,\n",
       "   -0.03241419792175293,\n",
       "   0.010935857892036438,\n",
       "   -0.003908128943294287,\n",
       "   0.037636127322912216,\n",
       "   -0.32224589586257935,\n",
       "   -0.02872513048350811,\n",
       "   -0.022732427343726158,\n",
       "   -0.024937111884355545,\n",
       "   -0.017773758620023727,\n",
       "   -0.052373502403497696,\n",
       "   0.005551276728510857,\n",
       "   0.013999824412167072,\n",
       "   -0.012154614552855492,\n",
       "   -0.003329397877678275,\n",
       "   0.05576113611459732,\n",
       "   -0.020648915320634842,\n",
       "   0.008820786140859127,\n",
       "   -0.03031572885811329,\n",
       "   0.0027611327823251486,\n",
       "   0.017982371151447296,\n",
       "   0.035816021263599396,\n",
       "   0.010574945248663425,\n",
       "   -0.05622477829456329,\n",
       "   0.03532814234495163,\n",
       "   -0.0043061464093625546,\n",
       "   -0.013187402859330177,\n",
       "   0.028026940301060677,\n",
       "   -0.10784781724214554,\n",
       "   0.0023715607821941376,\n",
       "   0.016164369881153107,\n",
       "   0.1348951905965805,\n",
       "   -0.06533674895763397,\n",
       "   0.02722429856657982,\n",
       "   0.009958330541849136,\n",
       "   -0.0016093651065602899,\n",
       "   0.05177871882915497,\n",
       "   -0.042542316019535065,\n",
       "   -0.08117152750492096,\n",
       "   0.06703843921422958,\n",
       "   -0.00724606541916728,\n",
       "   -0.013392377644777298,\n",
       "   0.010318285785615444,\n",
       "   0.03764111176133156,\n",
       "   -0.03375012427568436,\n",
       "   0.027345750480890274,\n",
       "   -0.0007975270855240524,\n",
       "   -0.013970941305160522,\n",
       "   -0.07135355472564697,\n",
       "   -0.01002935878932476,\n",
       "   -0.008392241783440113,\n",
       "   -0.04287612438201904,\n",
       "   -0.02267962135374546,\n",
       "   0.028650354593992233,\n",
       "   -0.0401211753487587,\n",
       "   0.002935267984867096,\n",
       "   0.03385023772716522,\n",
       "   -0.005755827762186527,\n",
       "   0.05390654876828194,\n",
       "   -0.02381393499672413,\n",
       "   -0.056756097823381424,\n",
       "   -0.01208104845136404,\n",
       "   0.022634731605648994,\n",
       "   -0.03833961859345436,\n",
       "   0.037569183856248856,\n",
       "   0.02010369300842285,\n",
       "   -0.024936217814683914,\n",
       "   0.04422760382294655,\n",
       "   0.031461477279663086,\n",
       "   0.0681212916970253,\n",
       "   -0.05621551349759102,\n",
       "   0.10108448565006256,\n",
       "   -0.056492771953344345,\n",
       "   0.05912056937813759,\n",
       "   0.049129419028759,\n",
       "   -0.028018740937113762,\n",
       "   0.0957876592874527,\n",
       "   -0.010796821676194668,\n",
       "   -0.04476109892129898,\n",
       "   0.04571501910686493,\n",
       "   -0.041004523634910583,\n",
       "   -0.008849142119288445,\n",
       "   -0.009588501416146755,\n",
       "   -0.010523628443479538,\n",
       "   0.06407415121793747,\n",
       "   0.0053841909393668175,\n",
       "   -0.046858467161655426,\n",
       "   0.023377373814582825,\n",
       "   0.06695239990949631,\n",
       "   0.028133079409599304,\n",
       "   0.04184351861476898,\n",
       "   -0.003966469783335924,\n",
       "   -0.018572542816400528,\n",
       "   0.015167783945798874,\n",
       "   0.06003334000706673,\n",
       "   -0.016619432717561722,\n",
       "   0.04252958670258522,\n",
       "   -0.057511694729328156,\n",
       "   -0.06946434080600739,\n",
       "   -0.01194110419601202,\n",
       "   -0.009276192635297775,\n",
       "   -0.2740669548511505,\n",
       "   0.04457899183034897,\n",
       "   -0.019228624179959297,\n",
       "   0.021621957421302795,\n",
       "   -0.025345461443066597,\n",
       "   -0.03290756419301033,\n",
       "   -0.009563982486724854,\n",
       "   0.01692494936287403,\n",
       "   -0.01176405232399702,\n",
       "   -0.03696507215499878,\n",
       "   -0.025213459506630898,\n",
       "   0.05129783973097801,\n",
       "   -0.0311160646378994,\n",
       "   0.03235694020986557,\n",
       "   -0.0009735900675877929,\n",
       "   -0.015031205490231514,\n",
       "   0.10019771754741669,\n",
       "   -0.005293418187648058,\n",
       "   0.018484368920326233,\n",
       "   -0.03904334083199501,\n",
       "   -0.026380224153399467,\n",
       "   -0.002737957751378417,\n",
       "   0.1872059851884842,\n",
       "   0.016351714730262756,\n",
       "   0.05447525531053543,\n",
       "   0.023103337734937668,\n",
       "   0.029392139986157417,\n",
       "   -0.022025562822818756,\n",
       "   0.00332862907089293,\n",
       "   0.014353870414197445,\n",
       "   0.01450665108859539,\n",
       "   0.02778550237417221,\n",
       "   0.10558114945888519,\n",
       "   -0.01641300693154335,\n",
       "   0.041657671332359314,\n",
       "   0.014192157424986362,\n",
       "   -0.023243576288223267,\n",
       "   -0.011865552514791489,\n",
       "   0.008798322640359402,\n",
       "   0.003543733386322856,\n",
       "   -0.0576556995511055,\n",
       "   0.03373315557837486,\n",
       "   -0.05253339558839798,\n",
       "   0.005736998748034239,\n",
       "   0.040228039026260376,\n",
       "   0.06630590558052063,\n",
       "   -0.04082918539643288,\n",
       "   -0.0609707310795784,\n",
       "   -0.06722871959209442,\n",
       "   0.020227186381816864,\n",
       "   -0.019007684662938118,\n",
       "   0.022105582058429718,\n",
       "   0.013932283967733383,\n",
       "   -0.029298584908246994,\n",
       "   0.03444023057818413,\n",
       "   -0.005192140117287636,\n",
       "   -0.04472028836607933,\n",
       "   0.007781637366861105,\n",
       "   0.02345765195786953,\n",
       "   0.007994777522981167,\n",
       "   0.02199384942650795,\n",
       "   -0.037780314683914185,\n",
       "   0.005738920532166958,\n",
       "   -0.005407123360782862,\n",
       "   -0.0473155751824379],\n",
       "  '_relevance_score': 0.016129031777381897}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What does CoALA stands for?\"\n",
    "context_list = tbl.search(query, query_type=\"hybrid\").limit(5).to_list()\n",
    "context_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent design: thinking beyond simple reasoning. CoALA defines agents over three distinct concepts: (i)\n",
      "internal memory, (ii) a set of possible internal and external actions, and (iii) a decision making procedure over\n",
      "those actions. Using CoALA to develop an application-specific agent consists of specifying implementations\n",
      "for each of these components in turn. We assume that the agent’s environment and external action space are\n",
      "given, and show how CoALA can be used to determine an appropriate high-level architecture. For example,\n",
      "we can imagine designing a personalized retail assistant (Yao et al., 2022a) that helps users find relevant items\n",
      "based on their queries and purchasing history. In this case, the external actions would consist of dialogue or\n",
      "returning search results to the user.\n",
      "\n",
      "4\n",
      "        Cognitive Architectures for Language Agents (CoALA): A Conceptual Framework\n",
      "We present Cognitive Architectures for Language Agents (CoALA) as a framework to organize existing                                                                                                                                Reasoning\n",
      " language agents and guide the development of new ones. CoALA positions the LLM as the core component\n",
      " of a larger cognitive architecture (Figure 4). Under CoALA, a language agent stores information in memory\n",
      " modules (Section 4.1), and acts in an action space structured into external and internal parts (Figure 5):\n",
      "\n",
      "making process to choose actions. We use CoALA to retrospectively survey and organize\n",
      "                            a large body of recent work, and prospectively identify actionable directions towards more\n",
      "                            capable agents. Taken together, CoALA contextualizes today’s language agents within the\n",
      "                            broader history of AI and outlines a path towards language-based general intelligence.\n",
      "\n",
      "just choose their preferred framing, as long as it is consistent and useful for their own work.\n",
      "Physical vs. digital: what differences beget attention? While animals only live once in the physical\n",
      "world, digital environments (e.g., the Internet) often allow sequential (via resets) and parallel trials. This\n",
      "means digital agents can more boldly explore (e.g., open a million webpages) and self-clone for parallel task\n",
      "solving (e.g., a million web agents try different web paths), which may result in decision-making procedures\n",
      "different from current ones inspired by human cognition (Griffiths, 2020).\n",
      "Learning vs. acting: how should agents continuously and autonomously learn? In the CoALA\n",
      "framework, learning is a result action of a decision-making cycle just like grounding: the agent deliberately\n",
      "chooses to commit information to long-term memory. This is in contrast to most agents, which simply fix a\n",
      "learning schedule and only use decison making for external actions. Biological agents, however, do not have\n",
      "\n",
      "Published in Transactions on Machine Learning Research (02/2024)\n",
      "\n",
      "Updating agent code (procedural memory). CoALA allows agents to update their source code, thus\n",
      "modifying the implementation of various procedures. These can be broken down as follows:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context = ''.join([f\"{c['text']}\\n\\n\" for c in context_list])\n",
    "\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:\n",
      "What does CoALA stands for?\n",
      "Context:\n",
      "Agent design: thinking beyond simple reasoning. CoALA defines agents over three distinct concepts: (i)\n",
      "internal memory, (ii) a set of possible internal and external actions, and (iii) a decision making procedure over\n",
      "those actions. Using CoALA to develop an application-specific agent consists of specifying implementations\n",
      "for each of these components in turn. We assume that the agent’s environment and external action space are\n",
      "given, and show how CoALA can be used to determine an appropriate high-level architecture. For example,\n",
      "we can imagine designing a personalized retail assistant (Yao et al., 2022a) that helps users find relevant items\n",
      "based on their queries and purchasing history. In this case, the external actions would consist of dialogue or\n",
      "returning search results to the user.\n",
      "\n",
      "4\n",
      "        Cognitive Architectures for Language Agents (CoALA): A Conceptual Framework\n",
      "We present Cognitive Architectures for Language Agents (CoALA) as a framework to organize existing                                                                                                                                Reasoning\n",
      " language agents and guide the development of new ones. CoALA positions the LLM as the core component\n",
      " of a larger cognitive architecture (Figure 4). Under CoALA, a language agent stores information in memory\n",
      " modules (Section 4.1), and acts in an action space structured into external and internal parts (Figure 5):\n",
      "\n",
      "making process to choose actions. We use CoALA to retrospectively survey and organize\n",
      "                            a large body of recent work, and prospectively identify actionable directions towards more\n",
      "                            capable agents. Taken together, CoALA contextualizes today’s language agents within the\n",
      "                            broader history of AI and outlines a path towards language-based general intelligence.\n",
      "\n",
      "just choose their preferred framing, as long as it is consistent and useful for their own work.\n",
      "Physical vs. digital: what differences beget attention? While animals only live once in the physical\n",
      "world, digital environments (e.g., the Internet) often allow sequential (via resets) and parallel trials. This\n",
      "means digital agents can more boldly explore (e.g., open a million webpages) and self-clone for parallel task\n",
      "solving (e.g., a million web agents try different web paths), which may result in decision-making procedures\n",
      "different from current ones inspired by human cognition (Griffiths, 2020).\n",
      "Learning vs. acting: how should agents continuously and autonomously learn? In the CoALA\n",
      "framework, learning is a result action of a decision-making cycle just like grounding: the agent deliberately\n",
      "chooses to commit information to long-term memory. This is in contrast to most agents, which simply fix a\n",
      "learning schedule and only use decison making for external actions. Biological agents, however, do not have\n",
      "\n",
      "Published in Transactions on Machine Learning Research (02/2024)\n",
      "\n",
      "Updating agent code (procedural memory). CoALA allows agents to update their source code, thus\n",
      "modifying the implementation of various procedures. These can be broken down as follows:\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "system_prompt = \"Answer user query based on the given context.\"\n",
    "user_prompt = f\"Question:\\n{query}\\nContext:\\n{context}\"\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(system_prompt),\n",
    "    HumanMessage(user_prompt),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "CoALA stands for **Cognitive Architectures for Language Agents**.\n"
     ]
    }
   ],
   "source": [
    "response = gpt4o_chat.invoke(messages)\n",
    "response.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "CoALA stands for \"Coalition for Algorithmic Accountability.\" It is an initiative focused on promoting transparency and accountability in algorithmic decision-making processes, particularly in the context of technology and data usage. The coalition typically involves various stakeholders, including civil society organizations, researchers, and policy advocates, working together to address the ethical implications of algorithms and their impact on society. If you have a specific context in mind or if there are other meanings for CoALA, please let me know!\n"
     ]
    }
   ],
   "source": [
    "response = gpt4o_chat.invoke(\"What does CoALA stands for?\")\n",
    "response.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
